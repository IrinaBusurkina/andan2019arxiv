"names","day","month","year","abstract","abstract ","link","tag","title","emails"
"Ahmed Osman	Wojciech Samek","1","2","2018","We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-of-the-art ensemble model. We also experiment with replacing attention mechanisms in other state-of-the-art models with our implementation and show increased accuracy. In both cases, our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the VQA dataset.","We propose an architecture for VQA which utilize recurrent layer to generate visual and textual attention . The memory characteristic of the propose recurrent attention unit offer a rich joint embedding of visual and textual feature and enable the model to reason relation between several part of the image and question . Our single model outperform the first place winner on the VQA 1.0 dataset , performs within margin to the current state-of-the-art ensemble model . We also experiment with replace attention mechanism in other state-of-the-art model with our implementation and show increase accuracy . In both case , our recurrent attention mechanism improve performance in task require sequential or relational reasoning on the VQA dataset . ","http://arxiv.org/pdf/1802.00209v1","cs.AI	cs.CL	cs.CV	cs.NE	stat.ML","Dual Recurrent Attention Units for Visual Question Answering","ahmed.osman@hhi.fraunhofer.de	wojciech.samek@hhi.fraunhofer.de"
"Ji Young Lee	Franck Dernoncourt","12","3","2016","Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts. Our model achieves state-of-the-art results on three different datasets for dialog act prediction.","Recent approach base on artificial neural network ( ANNs ) have show promising result for short-text classification . However , many short text occur in sequence ( e.g. , sentence in a document or utterance in a dialog ) , and most existing ANN-based system do not leverage the precede short text when classify a subsequent one . In this work , we present a model base on recurrent neural network and convolutional neural network that incorporate the precede short text . Our model achieve state-of-the-art result on three different datasets for dialog act prediction . ","http://arxiv.org/pdf/1603.03827v1","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML","Sequential Short-Text Classification with Recurrent and Convolutional   Neural Networks","jjylee@mit.edu	francky@mit.edu"
"Iulian Vlad Serban	Tim Klinger	Gerald Tesauro	Kartik Talamadupula	Bowen Zhou	Yoshua Bengio	Aaron Courville","2","6","2016","We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture a wealth of high-level discourse semantics. Such procedure allows training the multiresolution recurrent neural network by maximizing the exact joint log-likelihood over both sequences. In contrast to the standard log- likelihood objective w.r.t. natural language tokens (word perplexity), optimizing the joint log-likelihood biases the model towards modeling high-level abstractions. We apply the proposed model to the task of dialogue response generation in two challenging domains: the Ubuntu technical support domain, and Twitter conversations. On Ubuntu, the model outperforms competing approaches by a substantial margin, achieving state-of-the-art results according to both automatic evaluation metrics and a human evaluation study. On Twitter, the model appears to generate more relevant and on-topic responses according to automatic evaluation metrics. Finally, our experiments demonstrate that the proposed model is more adept at overcoming the sparsity of natural language and is better able to capture long-term structure.","We introduce the multiresolution recurrent neural network , which extend the sequence-to-sequence framework to model natural language generation a two parallel discrete stochastic process : a sequence of high-level coarse token , and a sequence of natural language token . There be many way to estimate or learn the high-level coarse token , but we argue that a simple extraction procedure be sufficient to capture a wealth of high-level discourse semantics . Such procedure allow train the multiresolution recurrent neural network by maximize the exact joint log-likelihood over both sequence . In contrast to the standard log- likelihood objective w.r.t . natural language token ( word perplexity ) , optimize the joint log-likelihood bias the model towards model high-level abstraction . We apply the propose model to the task of dialogue response generation in two challenging domain : the Ubuntu technical support domain , and Twitter conversation . On Ubuntu , the model outperform compete approach by a substantial margin , achieve state-of-the-art result accord to both automatic evaluation metric and a human evaluation study . On Twitter , the model appear to generate more relevant and on-topic response accord to automatic evaluation metric . Finally , our experiment demonstrate that the propose model be more adept at overcome the sparsity of natural language and be well able to capture long-term structure . ","http://arxiv.org/pdf/1606.00776v2","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML	I.5.1; I.2.7","Multiresolution Recurrent Neural Networks: An Application to Dialogue   Response Generation",""
"Sebastian Ruder	Joachim Bingel	Isabelle Augenstein	Anders SÃ¸gaard","23","5","2017","Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.","Multi-task learning be motivate by the observation that humans bring to bear what they know about related problem when solve new one . Similarly , deep neural network can profit from related task by share parameter with other network . However , human do not consciously decide to transfer knowledge between task . In Natural Language Processing ( NLP ) , it be hard to predict if sharing will lead to improvement , particularly if task be only loosely relate . To overcome this , we introduce Sluice Networks , a general framework for multi-task learning where trainable parameter control the amount of share . Our framework generalize previous proposal in enable sharing of all combination of subspace , layer , and skip connection . We perform experiment on three task pair , and across seven different domain , use data from OntoNotes 5.0 , and achieve up to 15 % average error reduction over common approach to multi-task learning . We show that a ) label entropy be predictive of gain in sluice network , confirm finding for hard parameter sharing and b ) while sluice network easily fit noise , they be robust across domain in practice . ","http://arxiv.org/pdf/1705.08142v2","stat.ML	cs.AI	cs.CL	cs.LG	cs.NE","Learning what to share between loosely related tasks","sebastian@ruder.io,	bingel|soegaard}@di.ku.dk	augenstein@di.ku.dk"
"Iulian V. Serban	Chinnadhurai Sankar	Mathieu Germain	Saizheng Zhang	Zhouhan Lin	Sandeep Subramanian	Taesup Kim	Michael Pieper	Sarath Chandar	Nan Rosemary Ke	Sai Rajeshwar	Alexandre de Brebisson	Jose M. R. Sotelo	Dendi Suhubdy	Vincent Michalski	Alexandre Nguyen	Joelle Pineau	Yoshua Bengio","7","9","2017","We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.","We present MILABOT : a deep reinforcement learn chatbot develop by the Montreal Institute for Learning Algorithms ( MILA ) for the Amazon Alexa Prize competition . MILABOT be capable of converse with human on popular small talk topic through both speech and text . The system consist of an ensemble of natural language generation and retrieval model , include template-based model , bag-of-words model , sequence-to-sequence neural network and latent variable neural network model . By apply reinforcement learn to crowdsourced data and real-world user interaction , the system have be train to select an appropriate response from the model in it ensemble . The system have be evaluate through A/B test with real-world user , where it perform significantly good than many compete system . Due to it machine learning architecture , the system be likely to improve with additional data . ","http://arxiv.org/pdf/1709.02349v2","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML	I.5.1; I.2.7","A Deep Reinforcement Learning Chatbot",""
"Kelvin Guu	Tatsunori B. Hashimoto	Yonatan Oren	Percy Liang","26","9","2017","We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit vector that captures interpretable semantics such as sentence similarity and sentence-level analogies.","We propose a new generative model of sentence that first sample a prototype sentence from the training corpus and then edit it into a new sentence . Compared to traditional model that generate from scratch either left-to-right or by first sample a latent sentence vector , our prototype-then-edit model improve perplexity on language modeling and generate high quality output accord to human evaluation . Furthermore , the model give rise to a latent edit vector that capture interpretable semantics such a sentence similarity and sentence-level analogy . ","http://arxiv.org/pdf/1709.08878v1","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML","Generating Sentences by Editing Prototypes","kguu@stanford.edu	thashim@stanford.edu	yonatano@stanford.edu	pliang@cs.stanford.edu"
"Iulian V. Serban	Chinnadhurai Sankar	Mathieu Germain	Saizheng Zhang	Zhouhan Lin	Sandeep Subramanian	Taesup Kim	Michael Pieper	Sarath Chandar	Nan Rosemary Ke	Sai Rajeswar	Alexandre de Brebisson	Jose M. R. Sotelo	Dendi Suhubdy	Vincent Michalski	Alexandre Nguyen	Joelle Pineau	Yoshua Bengio","20","1","2018","We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than other systems. The results highlight the potential of coupling ensemble systems with deep reinforcement learning as a fruitful path for developing real-world, open-domain conversational agents.","We present MILABOT : a deep reinforcement learn chatbot develop by the Montreal Institute for Learning Algorithms ( MILA ) for the Amazon Alexa Prize competition . MILABOT be capable of converse with human on popular small talk topic through both speech and text . The system consist of an ensemble of natural language generation and retrieval model , include neural network and template-based model . By apply reinforcement learn to crowdsourced data and real-world user interaction , the system have be train to select an appropriate response from the model in it ensemble . The system have be evaluate through A/B test with real-world user , where it perform significantly good than other system . The result highlight the potential of couple ensemble system with deep reinforcement learning a a fruitful path for develop real-world , open-domain conversational agent . ","http://arxiv.org/pdf/1801.06700v1","cs.CL	cs.AI	cs.LG	cs.NE	stat.ML	I.5.1; I.2.7","A Deep Reinforcement Learning Chatbot (Short Version)",""
"Darko Brodic	Alessia Amelio	Zoran N. Milivojevic	Milena Jevtic","21","9","2016","The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.","The paper introduce a new method for discrimination of document give in different script . The document be map into a uniformly cod text of numerical value . It be derive from the position of the letter in the text line , base on their typographical characteristic . Each code be consider a a gray level . Accordingly , the coded text determine a 1-D image , on which texture analysis by run-length statistic and local binary pattern be perform . It define feature vector represent the script content of the document . A modified clustering approach employ on document feature vector group document write in the same script . Experimentation perform on two custom orient database of historical document in old Cyrillic , angular and round Glagolitic as well a Antiqua and Fraktur script demonstrate the superiority of the propose method with respect to well-known method in the state-of-the-art . ","http://arxiv.org/pdf/1609.06492v1","cs.CV	cs.AI	cs.CL	cs.LG	cs.NE	97R40, 62H35, 68U15, 68T50,","Document Image Coding and Clustering for Script Discrimination","dbrodic@tf.bor.ac.rs	mjevtic@tf.bor.ac.rs	aamelio@dimes.unical.it	zoran.milivojevic@vtsnis.edu.rs"
"Mateusz Malinowski	Mario Fritz","4","10","2016","Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.","Together with the development of more accurate method in Computer Vision and Natural Language Understanding , holistic architecture that answer on question about the content of real-world image have emerge . In this tutorial , we build a neural-based approach to answer question about image . We base our tutorial on two datasets : ( mostly on ) DAQUAR , and ( a bit on ) VQA . With small tweak the model that we present here can achieve a competitive performance on both datasets , in fact , they be among the best method that use a combination of LSTM with a global , full frame CNN representation of an image . We hope that after read this tutorial , the reader will be able to use Deep Learning framework , such a Keras and introduce Kraino , to build various architecture that will lead to a further performance improvement on this challenging task . ","http://arxiv.org/pdf/1610.01076v1","cs.CV	cs.AI	cs.CL	cs.LG	cs.NE","Tutorial on Answering Questions about Images with Deep Learning","mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Tony Beltramelli","22","5","2017","Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and web-based technologies).","Transforming a graphical user interface screenshot create by a designer into computer code be a typical task conduct by a developer in order to build customized software , websites , and mobile application . In this paper , we show that deep learn method can be leverage to train a model end-to-end to automatically generate code from a single input image with over 77 % of accuracy for three different platform ( i.e . iOS , Android and web-based technology ) . ","http://arxiv.org/pdf/1705.07962v2","cs.LG	cs.AI	cs.CL	cs.CV	cs.NE	68T45	I.2.1; I.2.10; I.2.2; I.2.6","pix2code: Generating Code from a Graphical User Interface Screenshot","tony@uizard.io"
"Fred Richardson	Douglas Reynolds	Najim Dehak","3","4","2015","Learned feature representations and sub-phoneme posteriors from Deep Neural Networks (DNNs) have been used separately to produce significant performance gains for speaker and language recognition tasks. In this work we show how these gains are possible using a single DNN for both speaker and language recognition. The unified DNN approach is shown to yield substantial performance improvements on the the 2013 Domain Adaptation Challenge speaker recognition task (55% reduction in EER for the out-of-domain condition) and on the NIST 2011 Language Recognition Evaluation (48% reduction in EER for the 30s test condition).","Learned feature representation and sub-phoneme posterior from Deep Neural Networks ( DNNs ) have be use separately to produce significant performance gain for speaker and language recognition task . In this work we show how these gain be possible use a single DNN for both speaker and language recognition . The unified DNN approach be show to yield substantial performance improvement on the the 2013 Domain Adaptation Challenge speaker recognition task ( 55 % reduction in EER for the out-of-domain condition ) and on the NIST 2011 Language Recognition Evaluation ( 48 % reduction in EER for the 30 test condition ) . ","http://arxiv.org/pdf/1504.00923v1","cs.CL	cs.CV	cs.LG	cs.NE	stat.ML","A Unified Deep Neural Network for Speaker and Language Recognition",""
"Hieu Pham	Melody Y. Guan	Barret Zoph	Quoc V. Le	Jeff Dean","9","2","2018","We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.","We propose Efficient Neural Architecture Search ( ENAS ) , a fast and inexpensive approach for automatic model design . In ENAS , a controller learn to discover neural network architecture by search for an optimal subgraph within a large computational graph . The controller be train with policy gradient to select a subgraph that maximize the expected reward on the validation set . Meanwhile the model correspond to the select subgraph be train to minimize a canonical cross entropy loss . Thanks to parameter share between child model , ENAS be fast : it deliver strong empirical performance use much few GPU-hours than all exist automatic model design approach , and notably , 1000x less expensive than standard Neural Architecture Search . On the Penn Treebank dataset , ENAS discover a novel architecture that achieve a test perplexity of 55.8 , establish a new state-of-the-art among all method without post-training processing . On the CIFAR-10 dataset , ENAS design novel architecture that achieve a test error of 2.89 % , which be on par with NASNet ( Zoph et al. , 2018 ) , whose test error be 2.65 % . ","http://arxiv.org/pdf/1802.03268v2","cs.LG	cs.CL	cs.CV	cs.NE	stat.ML","Efficient Neural Architecture Search via Parameter Sharing",""
"Brenden M. Lake	Tomer D. Ullman	Joshua B. Tenenbaum	Samuel J. Gershman","1","4","2016","Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.","Recent progress in artificial intelligence ( AI ) have renew interest in build system that learn and think like people . Many advance have come from use deep neural network train end-to-end in task such a object recognition , video game , and board game , achieve performance that equal or even beat human in some respect . Despite their biological inspiration and performance achievement , these system differ from human intelligence in crucial way . We review progress in cognitive science suggest that truly human-like learning and think machine will have to reach beyond current engineering trend in both what they learn , and how they learn it . Specifically , we argue that these machine should ( a ) build causal model of the world that support explanation and understanding , rather than merely solve pattern recognition problem ; ( b ) ground learn in intuitive theory of physic and psychology , to support and enrich the knowledge that be learn ; and ( c ) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new task and situation . We suggest concrete challenge and promise route towards these goal that can combine the strength of recent neural network advance with more structured cognitive model . ","http://arxiv.org/pdf/1604.00289v3","cs.AI	cs.CV	cs.LG	cs.NE	stat.ML","Building Machines That Learn and Think Like People",""
"Hao Wang	Dit-Yan Yeung","6","4","2016","While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a general introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this survey, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.","While perception task such a visual object recognition and text understanding play an important role in human intelligence , the subsequent task that involve inference , reason and plan require an even high level of intelligence . The past few year have see major advance in many perception task use deep learning model . For higher-level inference , however , probabilistic graphical model with their Bayesian nature be still more powerful and flexible . To achieve integrated intelligence that involve both perception and inference , it be naturally desirable to tightly integrate deep learning and Bayesian model within a principled probabilistic framework , which we call Bayesian deep learning . In this unified framework , the perception of text or image use deep learning can boost the performance of higher-level inference and in return , the feedback from the inference process be able to enhance the perception of text or image . This survey provide a general introduction to Bayesian deep learning and review it recent application on recommender system , topic model , and control . In this survey , we also discuss the relationship and difference between Bayesian deep learning and other related topic like Bayesian treatment of neural network . ","http://arxiv.org/pdf/1604.01662v2","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","Towards Bayesian Deep Learning: A Survey","hwangaz@cse.ust.hk	dyyeung@cse.ust.hk"
"Tejas D. Kulkarni	Karthik R. Narasimhan	Ardavan Saeedi	Joshua B. Tenenbaum","20","4","2016","Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.","Learning goal-directed behavior in environment with sparse feedback be a major challenge for reinforcement learning algorithm . The primary difficulty arises due to insufficient exploration , result in an agent be unable to learn robust value function . Intrinsically motivated agent can explore new behavior for it own sake rather than to directly solve problem . Such intrinsic behavior could eventually help the agent solve task pose by the environment . We present hierarchical-DQN ( h-DQN ) , a framework to integrate hierarchical value function , operate at different temporal scale , with intrinsically motivate deep reinforcement learning . A top-level value function learn a policy over intrinsic goal , and a lower-level function learn a policy over atomic action to satisfy the give goal . h-DQN allows for flexible goal specification , such a function over entity and relation . This provide an efficient space for exploration in complicated environment . We demonstrate the strength of our approach on two problem with very sparse , delayed feedback : ( 1 ) a complex discrete stochastic decision process , and ( 2 ) the classic ATARI game ` Montezuma 's Revenge ' . ","http://arxiv.org/pdf/1604.06057v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Hierarchical Deep Reinforcement Learning: Integrating Temporal   Abstraction and Intrinsic Motivation",""
"Deepak Pathak	Ross Girshick	Piotr DollÃ¡r	Trevor Darrell	Bharath Hariharan","19","12","2016","This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence that motion plays a key role in the development of the human visual system, we hope that this straightforward approach to unsupervised learning will be more effective than cleverly designed 'pretext' tasks studied in the literature. Indeed, our extensive experiments show that this is the case. When used for transfer learning on object detection, our representation significantly outperforms previous unsupervised approaches across multiple settings, especially when training data for the target task is scarce.","This paper present a novel yet intuitive approach to unsupervised feature learning . Inspired by the human visual system , we explore whether low-level motion-based grouping cue can be use to learn an effective visual representation . Specifically , we use unsupervised motion-based segmentation on video to obtain segment , which we use a 'pseudo ground truth ' to train a convolutional network to segment object from a single frame . Given the extensive evidence that motion play a key role in the development of the human visual system , we hope that this straightforward approach to unsupervised learning will be more effective than cleverly design 'pretext ' task study in the literature . Indeed , our extensive experiment show that this be the case . When use for transfer learning on object detection , our representation significantly outperform previous unsupervised approach across multiple setting , especially when train data for the target task be scarce . ","http://arxiv.org/pdf/1612.06370v2","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Learning Features by Watching Objects Move",""
"Muhammad Ghifary	W. Bastiaan Kleijn	Mengjie Zhang","21","9","2014","We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF features and raw image pixels of a particular image data set. We also show that our proposed model, preceded by the denoising auto-encoder pretraining, achieves better performance than recent benchmark models on the same data sets. This work represents the first study of MMD measure in the context of neural networks.","We propose a simple neural network model to deal with the domain adaptation problem in object recognition . Our model incorporate the Maximum Mean Discrepancy ( MMD ) measure a a regularization in the supervised learning to reduce the distribution mismatch between the source and target domain in the latent space . From experiment , we demonstrate that the MMD regularization be an effective tool to provide good domain adaptation model on both SURF feature and raw image pixel of a particular image data set . We also show that our propose model , precede by the denoising auto-encoder pretraining , achieve good performance than recent benchmark model on the same data set . This work represent the first study of MMD measure in the context of neural network . ","http://arxiv.org/pdf/1409.6041v1","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Domain Adaptive Neural Networks for Object Recognition","muhammad.ghifary@ecs.vuw.ac.nz	bastiaan.kleijn@ecs.vuw.ac.nz	mengjie.zhang@ecs.vuw.ac.nz"
"Lionel Pigou	AÃ¤ron van den Oord	Sander Dieleman	Mieke Van Herreweghe	Joni Dambre","5","6","2015","Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.","Recent study have demonstrate the power of recurrent neural network for machine translation , image captioning and speech recognition . For the task of capture temporal structure in video , however , there still remain numerous open research question . Current research suggest use a simple temporal feature pool strategy to take into account the temporal aspect of video . We demonstrate that this method be not sufficient for gesture recognition , where temporal information be more discriminative compare to general video classification task . We explore deep architecture for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporate temporal convolution and bidirectional recurrence . Our main contribution be twofold ; first , we show that recurrence be crucial for this task ; second , we show that add temporal convolution lead to significant improvement . We evaluate the different approach on the Montalbano gesture recognition dataset , where we achieve state-of-the-art result . ","http://arxiv.org/pdf/1506.01911v3","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Beyond Temporal Pooling: Recurrence and Temporal Convolutions for   Gesture Recognition in Video",""
"Rakesh Achanta	Trevor Hastie","20","9","2015","In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph level. Telugu script is a complex alphasyllabary and the language is agglutinative, making the problem hard. In this paper we apply the latest advances in neural networks to achieve state-of-the-art error rates. We also review convolutional neural networks in great detail and expound the statistical justification behind the many tricks needed to make Deep Learning work.","In this paper , we address the task of Optical Character Recognition ( OCR ) for the Telugu script . We present an end-to-end framework that segment the text image , classify the character and extract line use a language model . The segmentation be base on mathematical morphology . The classification module , which be the most challenging task of the three , be a deep convolutional neural network . The language be model a a third degree markov chain at the glyph level . Telugu script be a complex alphasyllabary and the language be agglutinative , make the problem hard . In this paper we apply the late advance in neural network to achieve state-of-the-art error rate . We also review convolutional neural network in great detail and expound the statistical justification behind the many trick need to make Deep Learning work . ","http://arxiv.org/pdf/1509.05962v2","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","Telugu OCR Framework using Deep Learning",""
"Jeff Donahue	Philipp KrÃ¤henbÃ¼hl	Trevor Darrell","31","5","2016","The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.","The ability of the Generative Adversarial Networks ( GANs ) framework to learn generative model map from simple latent distribution to arbitrarily complex data distribution have be demonstrate empirically , with compelling result show that the latent space of such generator capture semantic variation in the data distribution . Intuitively , model train to predict these semantic latent representation give data may serve a useful feature representation for auxiliary problem where semantics be relevant . However , in their existing form , GANs have no mean of learn the inverse mapping -- project data back into the latent space . We propose Bidirectional Generative Adversarial Networks ( BiGANs ) a a mean of learn this inverse mapping , and demonstrate that the result learned feature representation be useful for auxiliary supervised discrimination task , competitive with contemporary approach to unsupervised and self-supervised feature learning . ","http://arxiv.org/pdf/1605.09782v7","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Adversarial Feature Learning","jdonahue@cs.berkeley.edu	philkr@utexas.edu	trevor@eecs.berkeley.edu"
"Zachary C. Lipton","10","6","2016","Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.","Supervised machine learn model boast remarkable predictive capability . But can you trust your model ? Will it work in deployment ? What else can it tell you about the world ? We want model to be not only good , but interpretable . And yet the task of interpretation appear underspecified . Papers provide diverse and sometimes non-overlapping motivation for interpretability , and offer myriad notion of what attribute render model interpretable . Despite this ambiguity , many paper proclaim interpretability axiomatically , absent further explanation . In this paper , we seek to refine the discourse on interpretability . First , we examine the motivation underlie interest in interpretability , find them to be diverse and occasionally discordant . Then , we address model property and technique think to confer interpretability , identify transparency to human and post-hoc explanation a compete notion . Throughout , we discuss the feasibility and desirability of different notion , and question the oft-made assertion that linear model be interpretable and that deep neural network be not . ","http://arxiv.org/pdf/1606.03490v3","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","The Mythos of Model Interpretability",""
"Sahil Garg	Irina Rish	Guillermo Cecchi	Aurelie Lozano","22","1","2017","In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the neuronal-birth is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.","In this paper , we focus on online representation learn in non-stationary environment which may require continuous adaptation of model architecture . We propose a novel online dictionary-learning ( sparse-coding ) framework which incorporate the addition and deletion of hidden unit ( dictionary element ) , and be inspire by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus , know to be associate with improved cognitive function and adaptation to new environment . In the online learn setting , where new input instance arrive sequentially in batch , the neuronal-birth be implement by add new unit with random initial weight ( random dictionary element ) ; the number of new unit be determine by the current performance ( representation error ) of the dictionary , high error cause an increase in the birth rate . Neuronal-death be implement by impose l1/l2-regularization ( group sparsity ) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternate minimization scheme , which iterate between the code and dictionary update . Finally , hidden unit connectivity adaptation be facilitate by introduce sparsity in dictionary element . Our empirical evaluation on several real-life datasets ( image and language ) as well a on synthetic data demonstrate that the propose approach can considerably outperform the state-of-art fixed-size ( nonadaptive ) online sparse coding of Mairal et al . ( 2009 ) in the presence of nonstationary data . Moreover , we identify certain property of the data ( e.g. , sparse input with nearly non-overlapping support ) and of the model ( e.g. , dictionary sparsity ) associate with such improvement . ","http://arxiv.org/pdf/1701.06106v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a   Changing World","sahilgar@usc.edu	rish@us.ibm.com	gcecchi@us.ibm.com	aclozano@us.ibm.com"
"Weifeng Ge	Yizhou Yu","28","2","2017","Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.   Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model.","Deep neural network require a large amount of labeled training data during supervised learning . However , collect and label so much data might be infeasible in many case . In this paper , we introduce a source-target selective joint fine-tuning scheme for improve the performance of deep learning task with insufficient training data . In this scheme , a target learn task with insufficient training data be carry out simultaneously with another source learn task with abundant training data . However , the source learn task do not use all exist training data . Our core idea be to identify and use a subset of training image from the original source learn task whose low-level characteristic be similar to those from the target learn task , and jointly fine-tune share convolutional layer for both task . Specifically , we compute descriptor from linear or nonlinear filter bank response on training image from both task , and use such descriptor to search for a desired subset of training sample for the source learn task . Experiments demonstrate that our selective joint fine-tuning scheme achieve state-of-the-art performance on multiple visual classification task with insufficient training data for deep learning . Such task include Caltech 256 , MIT Indoor 67 , Oxford Flowers 102 and Stanford Dogs 120 . In comparison to fine-tuning without a source domain , the propose method can improve the classification accuracy by 2 % - 10 % use a single model . ","http://arxiv.org/pdf/1702.08690v2","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Borrowing Treasures from the Wealthy: Deep Transfer Learning through   Selective Joint Fine-tuning",""
"Tanmay Gupta	Kevin Shih	Saurabh Singh	Derek Hoiem","2","4","2017","An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing each to use the same word-region embeddings. We show this leads to greater inductive transfer from recognition to VQA than standard multitask learning. Visual recognition also improves, especially for categories that have relatively few recognition training labels but appear often in the VQA setting. Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of interpretable, flexible, and trainable core representations.","An important goal of computer vision be to build system that learn visual representation over time that can be apply to many task . In this paper , we investigate a vision-language embedding a a core representation and show that it lead to well cross-task transfer than standard multi-task learning . In particular , the task of visual recognition be align to the task of visual question answering by force each to use the same word-region embeddings . We show this lead to great inductive transfer from recognition to VQA than standard multitask learning . Visual recognition also improve , especially for category that have relatively few recognition training label but appear often in the VQA setting . Thus , our paper take a small step towards create more general vision system by show the benefit of interpretable , flexible , and trainable core representation . ","http://arxiv.org/pdf/1704.00260v2","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks","tgupta6@illinois.edu	kjshih2@illinois.edu	dhoiem@illinois.edu	saurabhsingh@google.com"
"Jan Hendrik Metzen	Mummadi Chaithanya Kumar	Thomas Brox	Volker Fischer","19","4","2017","While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.","While deep learning be remarkably successful on perceptual task , it be also show to be vulnerable to adversarial perturbation of the input . These perturbation denote noise add to the input that be generate specifically to fool the system while be quasi-imperceptible for human . More severely , there even exist universal perturbation that be input-agnostic but fool the network on the majority of input . While recent work have focus on image classification , this work propose attack against semantic image segmentation : we present an approach for generate ( universal ) adversarial perturbation that make the network yield a desired target segmentation a output . We show empirically that there exist barely perceptible universal noise pattern which result in nearly the same predicted segmentation for arbitrary input . Furthermore , we also show the existence of universal noise which remove a target class ( e.g. , all pedestrian ) from the segmentation while leave the segmentation mostly unchanged otherwise . ","http://arxiv.org/pdf/1704.05712v3","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","Universal Adversarial Perturbations Against Semantic Image Segmentation","janhendrik.metzen@de.bosch.com	chaithu0536@gmail.com	brox@cs.uni-freiburg.de	volker.fischer@de.bosch.com"
"Quynh Nguyen	Matthias Hein","26","4","2017","While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.","While the optimization problem behind deep neural network be highly non-convex , it be frequently observe in practice that train deep network seem possible without get stick in suboptimal point . It have be argue that this be the case a all local minimum be close to be globally optimal . We show that this be ( almost ) true , in fact almost all local minimum be globally optimal , for a fully connect network with squared loss and analytic activation function give that the number of hidden unit of one layer of the network be large than the number of train point and the network structure from this layer on be pyramidal . ","http://arxiv.org/pdf/1704.08045v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","The loss surface of deep and wide neural networks",""
"Chris Donahue	Zachary C. Lipton	Akshay Balsubramani	Julian McAuley","22","5","2017","We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as lighting and pose. Our algorithm features a pairwise training scheme in which each sample from the generator consists of two images with a common identity code. Corresponding samples from the real dataset consist of two distinct photographs of the same subject. In order to fool the discriminator, the generator must produce pairs that are photorealistic, distinct, and appear to depict the same individual. We augment both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate pairwise training. Experiments with human judges and an off-the-shelf face verification system demonstrate our algorithm's ability to generate convincing, identity-matched photographs.","We propose a new algorithm for train generative adversarial network that jointly learn latent code for both identity ( e.g . individual human ) and observation ( e.g . specific photograph ) . By fix the identity portion of the latent code , we can generate diverse image of the same subject , and by fix the observation portion , we can traverse the manifold of subject while maintain contingent aspect such a lighting and pose . Our algorithm feature a pairwise training scheme in which each sample from the generator consist of two image with a common identity code . Corresponding sample from the real dataset consist of two distinct photograph of the same subject . In order to fool the discriminator , the generator must produce pair that be photorealistic , distinct , and appear to depict the same individual . We augment both the DCGAN and BEGAN approach with Siamese discriminator to facilitate pairwise training . Experiments with human judge and an off-the-shelf face verification system demonstrate our algorithm 's ability to generate convincing , identity-matched photograph . ","http://arxiv.org/pdf/1705.07904v3","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Semantically Decomposing the Latent Spaces of Generative Adversarial   Networks","cdonahue@ucsd.edu	zlipton@cmu.edu	abalsubr@stanford.edu	jmcauley@eng.ucsd.edu"
"Mahesh Chandra Mukkamala	Matthias Hein","17","6","2017","Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.","Adaptive gradient method have become recently very popular , in particular a they have be show to be useful in the training of deep neural network . In this paper we have analyze RMSProp , originally propose for the training of deep neural network , in the context of online convex optimization and show $ \sqrt { T } $ -type regret bound . Moreover , we propose two variant SC-Adagrad and SC-RMSProp for which we show logarithmic regret bound for strongly convex function . Finally , we demonstrate in the experiment that these new variant outperform other adaptive gradient technique or stochastic gradient descent in the optimization of strongly convex function as well a in training of deep neural network . ","http://arxiv.org/pdf/1706.05507v2","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML","Variants of RMSProp and Adagrad with Logarithmic Regret Bounds",""
"Chunyuan Li	Hao Liu	Changyou Chen	Yunchen Pu	Liqun Chen	Ricardo Henao	Lawrence Carin","5","9","2017","We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.","We investigate the non-identifiability issue associate with bidirectional adversarial training for joint distribution matching . Within a framework of conditional entropy , we propose both adversarial and non-adversarial approach to learn desirable match joint distribution for unsupervised and supervised task . We unify a broad family of adversarial model a joint distribution match problem . Our approach stabilize learn of unsupervised bidirectional adversarial learning method . Further , we introduce an extension for semi-supervised learning task . Theoretical result be validate in synthetic data and real-world application . ","http://arxiv.org/pdf/1709.01215v2","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE","ALICE: Towards Understanding Adversarial Learning for Joint Distribution   Matching","cl319@duke.edu"
"Mateusz Buda	Atsuto Maki	Maciej A. Mazurowski","15","10","2017","In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that totally eliminates the imbalance, whereas undersampling can perform better when the imbalance is only removed to some extent; (iv) as opposed to some classical machine learning models, oversampling does not necessarily cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.","In this study , we systematically investigate the impact of class imbalance on classification performance of convolutional neural network ( CNNs ) and compare frequently use method to address the issue . Class imbalance be a common problem that have be comprehensively study in classical machine learning , yet very limited systematic research be available in the context of deep learning . In our study , we use three benchmark datasets of increase complexity , MNIST , CIFAR-10 and ImageNet , to investigate the effect of imbalance on classification and perform an extensive comparison of several method to address the issue : oversampling , undersampling , two-phase training , and thresholding that compensate for prior class probability . Our main evaluation metric be area under the receiver operate characteristic curve ( ROC AUC ) adjust to multi-class task since overall accuracy metric be associate with notable difficulty in the context of imbalanced data . Based on result from our experiment we conclude that ( i ) the effect of class imbalance on classification performance be detrimental ; ( ii ) the method of address class imbalance that emerge a dominant in almost all analyzed scenario be oversampling ; ( iii ) oversampling should be apply to the level that totally eliminate the imbalance , whereas undersampling can perform good when the imbalance be only remove to some extent ; ( iv ) a oppose to some classical machine learning model , oversampling do not necessarily cause overfitting of CNNs ; ( v ) thresholding should be apply to compensate for prior class probability when overall number of properly classify case be of interest . ","http://arxiv.org/pdf/1710.05381v1","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","A systematic study of the class imbalance problem in convolutional   neural networks","buda	atsuto}@kth.se	maciej.mazurowski@duke.edu"
"Jan KukaÄka	Vladimir Golkov	Daniel Cremers","29","10","2017","Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; instead, we present an overview of how the methods can be sorted into meaningful categories and sub-categories. This helps revealing links and fundamental similarities between them. Finally, we include practical recommendations both for users and for developers of new regularization methods.","Regularization be one of the crucial ingredient of deep learning , yet the term regularization have various definition , and regularization method be often study separately from each other . In our work we present a systematic , unifying taxonomy to categorize exist method . We distinguish method that affect data , network architecture , error term , regularization term , and optimization procedure . We do not provide all detail about the listed method ; instead , we present an overview of how the method can be sort into meaningful category and sub-categories . This help reveal link and fundamental similarity between them . Finally , we include practical recommendation both for user and for developer of new regularization method . ","http://arxiv.org/pdf/1710.10686v1","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML	62M45	I.2.6; I.5","Regularization for Deep Learning: A Taxonomy","jan.kukacka@tum.de	vladimir.golkov@tum.de	cremers@tum.de"
"Elie Aljalbout	Vladimir Golkov	Yawar Siddiqui	Daniel Cremers","23","1","2018","Clustering is a fundamental machine learning method. The quality of its results is dependent on the data distribution. For this reason, deep neural networks can be used for learning better representations of the data. In this paper, we propose a systematic taxonomy for clustering with deep learning, in addition to a review of methods from the field. Based on our taxonomy, creating new methods is more straightforward. We also propose a new approach which is built on the taxonomy and surpasses some of the limitations of some previous work. Our experimental evaluation on image datasets shows that the method approaches state-of-the-art clustering quality, and performs better in some cases.","Clustering be a fundamental machine learn method . The quality of it result be dependent on the data distribution . For this reason , deep neural network can be use for learn good representation of the data . In this paper , we propose a systematic taxonomy for cluster with deep learning , in addition to a review of method from the field . Based on our taxonomy , create new method be more straightforward . We also propose a new approach which be build on the taxonomy and surpass some of the limitation of some previous work . Our experimental evaluation on image datasets show that the method approach state-of-the-art clustering quality , and performs well in some case . ","http://arxiv.org/pdf/1801.07648v1","cs.LG	cs.AI	cs.CV	cs.NE	stat.ML	62H30, 62M45, 91C20	H.3.3; I.2.6; I.5; I.5.3; I.5.4","Clustering with Deep Learning: Taxonomy and New Methods","firstname.lastname@tum.de	cremers@tum.de"
"Armand Zampieri	Guillaume Charpiat	Yuliya Tarabalka","27","2","2018","We tackle here the problem of multimodal image non-rigid registration, which is of prime importance in remote sensing and medical imaging. The difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent. By analyzing these methods, we note the significance of the notion of scale. We design easy-to-train, fully-convolutional neural networks able to learn scale-specific features. Once chained appropriately, they perform global registration in linear time, getting rid of gradient descent schemes by predicting directly the deformation.We show their performance in terms of quality and speed through various tasks of remote sensing multimodal image alignment. In particular, we are able to register correctly cadastral maps of buildings as well as road polylines onto RGB images, and outperform current keypoint matching methods.","We tackle here the problem of multimodal image non-rigid registration , which be of prime importance in remote sensing and medical imaging . The difficulty encounter by classical registration approach include feature design and slow optimization by gradient descent . By analyze these method , we note the significance of the notion of scale . We design easy-to-train , fully-convolutional neural network able to learn scale-specific feature . Once chain appropriately , they perform global registration in linear time , get rid of gradient descent scheme by predict directly the deformation.We show their performance in term of quality and speed through various task of remote sense multimodal image alignment . In particular , we be able to register correctly cadastral map of building as well a road polylines onto RGB image , and outperform current keypoint matching method . ","http://arxiv.org/pdf/1802.09816v1","cs.CV	cs.AI	cs.LG	cs.NE	stat.ML","Coarse to fine non-rigid registration: a chain of scale-specific neural   networks for multimodal image alignment with application to remote sensing","guillaume.charpiat@inria.fr	yuliya.tarabalka@inria.fr"
"Li Yao	Atousa Torabi	Kyunghyun Cho	Nicolas Ballas	Christopher Pal	Hugo Larochelle	Aaron Courville","27","2","2015","Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.","Recent progress in use recurrent neural network ( RNNs ) for image description have motivate the exploration of their application for video description . However , while image be static , work with videos require model their dynamic temporal structure and then properly integrate that information into a natural language description . In this context , we propose an approach that successfully take into account both the local and global temporal structure of video to produce description . First , our approach incorporate a spatial temporal 3-D convolutional neural network ( 3-D CNN ) representation of the short temporal dynamic . The 3-D CNN representation be train on video action recognition task , so a to produce a representation that be tune to human motion and behavior . Second we propose a temporal attention mechanism that allow to go beyond local temporal modeling and learn to automatically select the most relevant temporal segment give the text-generating RNN . Our approach exceed the current state-of-art for both BLEU and METEOR metric on the Youtube2Text dataset . We also present result on a new , large and more challenging dataset of paired video and natural language description . ","http://arxiv.org/pdf/1502.08029v5","stat.ML	cs.AI	cs.CL	cs.CV	cs.LG","Describing Videos by Exploiting Temporal Structure","li.yao@umontreal.ca	atousa.torabi@umontreal.ca	kyunghyun.cho@umontreal.ca	nicolas.ballas@umontreal.ca	christopher.pal@polymtl.ca	hugo.larochelle@usherbrooke.ca	aaron.courville@umontreal.ca"
"Hao Wang	Xingjian Shi	Dit-Yan Yeung","2","11","2016","Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.","Hybrid method that utilize both content and rating information be commonly use in many recommender system . However , most of them use either handcraft feature or the bag-of-words representation a a surrogate for the content information but they be neither effective nor natural enough . To address this problem , we develop a collaborative recurrent autoencoder ( CRAE ) which be a denoising recurrent autoencoder ( DRAE ) that model the generation of content sequence in the collaborative filtering ( CF ) setting . The model generalize recent advance in recurrent deep learning from i.i.d . input to non-i.i.d . ( CF-based ) input and provide a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder . To do this , we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting . The synergy between denoising and CF enable CRAE to make accurate recommendation while learn to fill in the blank in sequence . Experiments on real-world datasets from different domain ( CiteULike and Netflix ) show that , by jointly model the order-aware generation of sequence for the content information and performing CF for the rating , CRAE be able to significantly outperform the state of the art on both the recommendation task base on rating and the sequence generation task base on content information . ","http://arxiv.org/pdf/1611.00454v1","cs.LG	cs.AI	cs.CL	cs.CV	stat.ML","Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in   the Blanks","hwangaz@cse.ust.hk	xshiab@cse.ust.hk	dyyeung@cse.ust.hk"
"Laura Graesser	Abhinav Gupta	Lakshay Sharma	Evelina Bakhturina","3","12","2017","In this project we analysed how much semantic information images carry, and how much value image data can add to sentiment analysis of the text associated with the images. To better understand the contribution from images, we compared models which only made use of image data, models which only made use of text data, and models which combined both data types. We also analysed if this approach could help sentiment classifiers generalize to unknown sentiments.","In this project we analyse how much semantic information image carry , and how much value image data can add to sentiment analysis of the text associate with the image . To well understand the contribution from image , we compare model which only make use of image data , model which only make use of text data , and model which combine both data type . We also analyse if this approach could help sentiment classifier generalize to unknown sentiment . ","http://arxiv.org/pdf/1712.00725v1","cs.CL	cs.AI	cs.CV	cs.LG	stat.ML","Sentiment Classification using Images and Label Embeddings","lhg256@nyu.edu	abhinavg@nyu.edu	ls4170@nyu.edu	eb2992@nyu.edu"
"Hao Wang	Xingjian Shi	Dit-Yan Yeung","2","11","2016","Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family distributions to model the weights and neurons. Different from traditional NN and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As a Bayesian treatment, efficient backpropagation (BP) is performed to learn the natural parameters for the distributions over both the weights and neurons. The output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction. Experiments on real-world datasets show that NPN can achieve state-of-the-art performance.","Neural network ( NN ) have achieve state-of-the-art performance in various application . Unfortunately in application where train data be insufficient , they be often prone to overfitting . One effective way to alleviate this problem be to exploit the Bayesian approach by use Bayesian neural network ( BNN ) . Another shortcoming of NN be the lack of flexibility to customize different distribution for the weight and neuron accord to the data , a be often do in probabilistic graphical model . To address these problem , we propose a class of probabilistic neural network , dub natural-parameter network ( NPN ) , a a novel and lightweight Bayesian treatment of NN . NPN allow the usage of arbitrary exponential-family distribution to model the weight and neuron . Different from traditional NN and BNN , NPN take distribution a input and go through layer of transformation before produce distribution to match the target output distribution . As a Bayesian treatment , efficient backpropagation ( BP ) be perform to learn the natural parameter for the distribution over both the weight and neuron . The output distribution of each layer , a byproduct , may be use a second-order representation for the associated task such a link prediction . Experiments on real-world datasets show that NPN can achieve state-of-the-art performance . ","http://arxiv.org/pdf/1611.00448v1","cs.LG	cs.AI	cs.CL	cs.CV	stat.ML","Natural-Parameter Networks: A Class of Probabilistic Neural Networks","hwangaz@cse.ust.hk	xshiab@cse.ust.hk	dyyeung@cse.ust.hk"
"Misha Denil	Pulkit Agrawal	Tejas D Kulkarni	Tom Erez	Peter Battaglia	Nando de Freitas","6","11","2016","When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that state of art deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.","When encounter novel object , human be able to infer a wide range of physical property such a mass , friction and deformability by interact with them in a goal driven way . This process of active interaction be in the same spirit a a scientist perform experiment to discover hidden fact . Recent advance in artificial intelligence have yield machine that can achieve superhuman performance in Go , Atari , natural language processing , and complex control problem ; however , it be not clear that these system can rival the scientific intuition of even a young child . In this work we introduce a basic set of task that require agent to estimate property such a mass and cohesion of object in an interactive simulated environment where they can manipulate the object and observe the consequence . We find that state of art deep reinforcement learn method can learn to perform the experiment necessary to discover such hidden property . By systematically manipulate the problem difficulty and the cost incur by the agent for perform experiment , we find that agent learn different strategy that balance the cost of gather information against the cost of make mistake in different situation . ","http://arxiv.org/pdf/1611.01843v3","stat.ML	cs.AI	cs.CV	cs.LG	cs.NE	physics.soc-ph","Learning to Perform Physics Experiments via Deep Reinforcement Learning","mdenil@google.com	tkulkarni@google.com	etom@google.com	peterbattaglia@google.com	nandodefreitas@google.com	pulkitag@berkeley.edu"
"Tsung-Hsien Wen	David Vandyke	Nikola Mrksic	Milica Gasic	Lina M. Rojas-Barahona	Pei-Hao Su	Stefan Ultes	Steve Young","15","4","2016","Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-oriented dialogue system along with a new way of collecting dialogue data based on a novel pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue systems easily and without making too many assumptions about the task at hand. The results show that the model can converse with human subjects naturally whilst helping them to accomplish tasks in a restaurant search domain.","Teaching machine to accomplish task by converse naturally with human be challenge . Currently , develop task-oriented dialogue system require create multiple component and typically this involve either a large amount of handcrafting , or acquire costly label datasets to solve a statistical learning problem for each component . In this work we introduce a neural network-based text-in , text-out end-to-end trainable goal-oriented dialogue system along with a new way of collect dialogue data base on a novel pipe-lined Wizard-of-Oz framework . This approach allow u to develop dialogue system easily and without make too many assumption about the task at hand . The result show that the model can converse with human subject naturally whilst help them to accomplish task in a restaurant search domain . ","http://arxiv.org/pdf/1604.04562v3","cs.CL	cs.AI	cs.NE	stat.ML","A Network-based End-to-End Trainable Task-oriented Dialogue System","thw28@cam.ac.uk	djv27@cam.ac.uk	nm480@cam.ac.uk	mg436@cam.ac.uk	lmr46@cam.ac.uk	phs26@cam.ac.uk	su259@cam.ac.uk	sjy11@cam.ac.uk"
"Johannes Welbl	Guillaume Bouchard	Sebastian Riedel","20","4","2016","Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations from it. We investigate the relevance of various bigram types on the fb15k237 dataset and find relative improvements compared to a compositional model.","Embedding-based Knowledge Base Completion model have so far mostly combine distributed representation of individual entity or relation to compute truth score of miss link . Facts can however also be represent use pairwise embeddings , i.e . embeddings for pair of entity and relation . In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablation from it . We investigate the relevance of various bigram type on the fb15k237 dataset and find relative improvement compare to a compositional model . ","http://arxiv.org/pdf/1604.05878v1","cs.CL	cs.AI	cs.NE	stat.ML","A Factorization Machine Framework for Testing Bigram Embeddings in   Knowledgebase Completion","j.welbl@cs.ucl.ac.uk	g.bouchard@cs.ucl.ac.uk	s.riedel@cs.ucl.ac.uk"
"Franck Dernoncourt	Ji Young Lee	Peter Szolovits","15","12","2016","Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the effectiveness of typical ANN models to classify sentences in isolation, with the strength of structured prediction. Our model achieves state-of-the-art results on two different datasets for sequential sentence classification in medical abstracts.","Existing model base on artificial neural network ( ANNs ) for sentence classification often do not incorporate the context in which sentence appear , and classify sentence individually . However , traditional sentence classification approach have be show to greatly benefit from jointly classify subsequent sentence , such a with conditional random field . In this work , we present an ANN architecture that combine the effectiveness of typical ANN model to classify sentence in isolation , with the strength of structured prediction . Our model achieve state-of-the-art result on two different datasets for sequential sentence classification in medical abstract . ","http://arxiv.org/pdf/1612.05251v1","cs.CL	cs.AI	cs.NE	stat.ML","Neural Networks for Joint Sentence Classification in Medical Paper   Abstracts",""
"Franck Dernoncourt	Ji Young Lee	Ozlem Uzuner	Peter Szolovits","10","6","2016","Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify patient notes. Manual de-identification is impractical given the size of EHR databases, the limited number of researchers with access to the non-de-identified notes, and the frequent mistakes of human annotators. A reliable automated de-identification system would consequently be of high value.   Materials and Methods: We introduce the first de-identification system based on artificial neural networks (ANNs), which requires no handcrafted features or rules, unlike existing systems. We compare the performance of the system with state-of-the-art systems on two datasets: the i2b2 2014 de-identification challenge dataset, which is the largest publicly available de-identification dataset, and the MIMIC de-identification dataset, which we assembled and is twice as large as the i2b2 2014 dataset.   Results: Our ANN model outperforms the state-of-the-art systems. It yields an F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with a recall 99.25 and a precision of 99.06.   Conclusion: Our findings support the use of ANNs for de-identification of patient notes, as they show better performance than previously published systems while requiring no feature engineering.","Objective : Patient note in electronic health record ( EHRs ) may contain critical information for medical investigation . However , the vast majority of medical investigator can only access de-identified note , in order to protect the confidentiality of patient . In the United States , the Health Insurance Portability and Accountability Act ( HIPAA ) define 18 type of protected health information ( PHI ) that need to be remove to de-identify patient note . Manual de-identification be impractical give the size of EHR database , the limited number of researcher with access to the non-de-identified note , and the frequent mistake of human annotator . A reliable automate de-identification system would consequently be of high value . Materials and Methods : We introduce the first de-identification system base on artificial neural network ( ANNs ) , which require no handcrafted feature or rule , unlike exist system . We compare the performance of the system with state-of-the-art system on two datasets : the i2b2 2014 de-identification challenge dataset , which be the large publicly available de-identification dataset , and the MIMIC de-identification dataset , which we assemble and be twice as large a the i2b2 2014 dataset . Results : Our ANN model outperform the state-of-the-art system . It yield an F1-score of 97.85 on the i2b2 2014 dataset , with a recall 97.38 and a precision of 97.32 , and an F1-score of 99.23 on the MIMIC de-identification dataset , with a recall 99.25 and a precision of 99.06 . Conclusion : Our finding support the use of ANNs for de-identification of patient note , a they show good performance than previously publish system while require no feature engineering . ","http://arxiv.org/pdf/1606.03475v1","cs.CL	cs.AI	cs.NE	stat.ML","De-identification of Patient Notes with Recurrent Neural Networks","francky@mit.edu	jjylee@mit.edu	psz@mit.edu	ouzuner@albany.edu"
"Tsendsuren Munkhdalai	Hong Yu","20","10","2016","Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.","Hypothesis testing be an important cognitive process that support human reason . In this paper , we introduce a computational hypothesis test approach base on memory augment neural network . Our approach involve a hypothesis test loop that reconsiders and progressively refine a previously form hypothesis in order to generate new hypothesis to test . We apply the propose approach to language comprehension task by use Neural Semantic Encoders ( NSE ) . Our NSE model achieve the state-of-the-art result show an absolute improvement of 1.2 % to 2.6 % accuracy over previous result obtain by single and ensemble system on standard machine comprehension benchmark such a the Children 's Book Test ( CBT ) and Who-Did-What ( WDW ) news article datasets . ","http://arxiv.org/pdf/1610.06454v2","cs.CL	cs.AI	cs.NE	stat.ML","Reasoning with Memory Augmented Neural Networks for Language   Comprehension","tsendsuren.munkhdalai@umassmed.edu	hong.yu@umassmed.edu"
"W. James Murdoch	Arthur Szlam","8","2","2017","Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.","Although deep learning model have prove effective at solve problem in natural language processing , the mechanism by which they come to their conclusion be often unclear . As a result , these model be generally treat a black box , yield no insight of the underlie learn pattern . In this paper we consider Long Short Term Memory network ( LSTMs ) and demonstrate a new approach for track the importance of a give input to the LSTM for a give output . By identify consistently important pattern of word , we be able to distill state of the art LSTMs on sentiment analysis and question answer into a set of representative phrase . This representation be then quantitatively validate by use the extracted phrase to construct a simple , rule-based classifier which approximate the output of the LSTM . ","http://arxiv.org/pdf/1702.02540v2","cs.CL	cs.AI	cs.NE	stat.ML","Automatic Rule Extraction from Long Short Term Memory Networks","aszlam@fb.com	jmurdoch@berkeley.edu"
"Sebastian Gehrmann	Franck Dernoncourt	Yeran Li	Eric T. Carlson	Joy T. Wu	Jonathan Welt	John Foote Jr.	Edward T. Moseley	David W. Grant	Patrick D. Tyler	Leo Anthony Celi","25","3","2017","Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods: We compare convolutional neural networks (CNNs), n-gram models, and approaches based on cTAKES that extract pre-defined medical concepts from clinical notes and use them to predict patient phenotypes. The performance is tested on 10 different phenotyping tasks using 1,610 discharge summaries extracted from the MIMIC-III database.   Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our model having an F1-score up to 37 points higher than alternative approaches. We additionally assess the interpretability of our model by presenting a method that extracts the most salient phrases for a particular prediction.   Conclusion: We show that NLP methods based on deep learning improve the performance of patient phenotyping. Our CNN-based algorithm automatically learns the phrases associated with each patient phenotype. As such, it reduces the annotation complexity for clinical domain experts, who are normally required to develop task-specific annotation rules and identify relevant phrases. Our method performs well in terms of both performance and interpretability, which indicates that deep learning is an effective approach to patient phenotyping based on clinicians' notes.","Objective : We investigate whether deep learning technique for natural language processing ( NLP ) can be use efficiently for patient phenotyping . Patient phenotyping be a classification task for determine whether a patient have a medical condition , and be a crucial part of secondary analysis of healthcare data . We assess the performance of deep learning algorithm and compare them with classical NLP approach . Materials and Methods : We compare convolutional neural network ( CNNs ) , n-gram model , and approach base on cTAKES that extract pre-defined medical concept from clinical note and use them to predict patient phenotype . The performance be test on 10 different phenotyping task use 1,610 discharge summary extract from the MIMIC-III database . Results : CNNs outperform other phenotyping algorithm in all 10 task . The average F1-score of our model be 76 ( PPV of 83 , and sensitivity of 71 ) with our model have an F1-score up to 37 point high than alternative approach . We additionally assess the interpretability of our model by present a method that extract the most salient phrase for a particular prediction . Conclusion : We show that NLP method base on deep learning improve the performance of patient phenotyping . Our CNN-based algorithm automatically learn the phrase associate with each patient phenotype . As such , it reduce the annotation complexity for clinical domain expert , who be normally require to develop task-specific annotation rule and identify relevant phrase . Our method performs well in term of both performance and interpretability , which indicate that deep learning be an effective approach to patient phenotyping base on clinician ' note . ","http://arxiv.org/pdf/1703.08705v1","cs.CL	cs.AI	cs.NE	stat.ML","Comparing Rule-Based and Deep Learning Models for Patient Phenotyping",""
"Ji Young Lee	Franck Dernoncourt	Peter Szolovits","5","4","2017","Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Our model ranked first in the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific articles (subtask C).","Over 50 million scholarly article have be publish : they constitute a unique repository of knowledge . In particular , one may infer from them relation between scientific concept , such a synonym and hyponym . Artificial neural network have be recently explore for relation extraction . In this work , we continue this line of work and present a system base on a convolutional neural network to extract relation . Our model rank first in the SemEval-2017 task 10 ( ScienceIE ) for relation extraction in scientific article ( subtask C ) . ","http://arxiv.org/pdf/1704.01523v1","cs.CL	cs.AI	cs.NE	stat.ML","MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks","jjylee@mit.edu	francky@mit.edu	psz@mit.edu"
"Ji Young Lee	Franck Dernoncourt	Peter Szolovits","17","5","2017","Recent approaches based on artificial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identification, which is an instance of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identification.","Recent approach base on artificial neural network ( ANNs ) have show promising result for named-entity recognition ( NER ) . In order to achieve high performance , ANNs need to be train on a large labeled dataset . However , label might be difficult to obtain for the dataset on which the user want to perform NER : label scarcity be particularly pronounce for patient note de-identification , which be an instance of NER . In this work , we analyze to what extent transfer learning may address this issue . In particular , we demonstrate that transfer an ANN model train on a large labeled dataset to another dataset with a limited number of label improves upon the state-of-the-art result on two different datasets for patient note de-identification . ","http://arxiv.org/pdf/1705.06273v1","cs.CL	cs.AI	cs.NE	stat.ML","Transfer Learning for Named-Entity Recognition with Neural Networks","jjylee@mit.edu	francky@mit.edu	psz@mit.edu"
"Sai Rajeswar	Sandeep Subramanian	Francis Dutil	Christopher Pal	Aaron Courville","31","5","2017","Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.","Generative Adversarial Networks ( GANs ) have gather a lot of attention from the computer vision community , yield impressive result for image generation . Advances in the adversarial generation of natural language from noise however be not commensurate with the progress make in generate image , and still lag far behind likelihood base method . In this paper , we take a step towards generate natural language with a GAN objective alone . We introduce a simple baseline that address the discrete output space problem without rely on gradient estimator and show that it be able to achieve state-of-the-art result on a Chinese poem generation dataset . We present quantitative result on generate sentence from context-free and probabilistic context-free grammar , and qualitative language model result . A conditional version be also describe that can generate sequence condition on sentence characteristic . ","http://arxiv.org/pdf/1705.10929v1","cs.CL	cs.AI	cs.NE	stat.ML","Adversarial Generation of Natural Language","sai.rajeswar.mudumba	sandeep.subramanian.1	aaron.courville}@umontreal.ca	frdutil@gmail.com,	christopher.pal@polymtl.ca"
"Leila Arras	GrÃ©goire Montavon	Klaus-Robert MÃ¼ller	Wojciech Samek","22","6","2017","Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.","Recently , a technique call Layer-wise Relevance Propagation ( LRP ) be show to deliver insightful explanation in the form of input space relevance for understand feed-forward neural network classification decision . In the present work , we extend the usage of LRP to recurrent neural network . We propose a specific propagation rule applicable to multiplicative connection a they arise in recurrent network architecture such a LSTMs and GRUs . We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task , and evaluate the result LRP relevance both qualitatively and quantitatively , obtain good result than a gradient-based related method which be use in previous work . ","http://arxiv.org/pdf/1706.07206v2","cs.CL	cs.AI	cs.NE	stat.ML","Explaining Recurrent Neural Network Predictions in Sentiment Analysis","leila.arras@hhi.fraunhofer.de	wojciech.samek@hhi.fraunhofer.de"
"Emmanuel Dufourq	Bruce A. Bassett","20","9","2017","Can textual data be compressed intelligently without losing accuracy in evaluating sentiment? In this study, we propose a novel evolutionary compression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression), which makes use of Parts-of-Speech tags to compress text in a way that sacrifices minimal classification accuracy when used in conjunction with sentiment analysis algorithms. An analysis of PARSEC with eight commercial and non-commercial sentiment analysis algorithms on twelve English sentiment data sets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss in sentiment classification accuracy for (20%, 50%, 75%) data compression with PARSEC using LingPipe, the most accurate of the sentiment algorithms. Other sentiment analysis algorithms are more severely affected by compression. We conclude that significant compression of text data is possible for sentiment analysis depending on the accuracy demands of the specific application and the specific sentiment analysis algorithm used.","Can textual data be compress intelligently without lose accuracy in evaluate sentiment ? In this study , we propose a novel evolutionary compression algorithm , PARSEC ( PARts-of-Speech for sEntiment Compression ) , which make use of Parts-of-Speech tag to compress text in a way that sacrifice minimal classification accuracy when use in conjunction with sentiment analysis algorithm . An analysis of PARSEC with eight commercial and non-commercial sentiment analysis algorithm on twelve English sentiment data set reveals that accurate compression be possible with ( 0 % , 1.3 % , 3.3 % ) loss in sentiment classification accuracy for ( 20 % , 50 % , 75 % ) data compression with PARSEC use LingPipe , the most accurate of the sentiment algorithm . Other sentiment analysis algorithm be more severely affect by compression . We conclude that significant compression of text data be possible for sentiment analysis depend on the accuracy demand of the specific application and the specific sentiment analysis algorithm use . ","http://arxiv.org/pdf/1709.06990v1","cs.NE	cs.AI	cs.CL	stat.ML","Text Compression for Sentiment Analysis via Evolutionary Algorithms","edufourq@gmail.com	bruce.a.bassett@gmail.com"
"Kartik Audhkhasi	Brian Kingsbury	Bhuvana Ramabhadran	George Saon	Michael Picheny","8","12","2017","Direct acoustics-to-word (A2W) models in the end-to-end paradigm have received increasing attention compared to conventional sub-word based automatic speech recognition models using phones, characters, or context-dependent hidden Markov model states. This is because A2W models recognize words from speech without any decoder, pronunciation lexicon, or externally-trained language model, making training and decoding with such models simple. Prior work has shown that A2W models require orders of magnitude more training data in order to perform comparably to conventional models. Our work also showed this accuracy gap when using the English Switchboard-Fisher data set. This paper describes a recipe to train an A2W model that closes this gap and is at-par with state-of-the-art sub-word based models. We achieve a word error rate of 8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder or language model. We find that model initialization, training data order, and regularization have the most impact on the A2W model performance. Next, we present a joint word-character A2W model that learns to first spell the word and then recognize it. This model provides a rich output to the user instead of simple word hypotheses, making it especially useful in the case of words unseen or rarely-seen during training.","Direct acoustics-to-word ( A2W ) model in the end-to-end paradigm have receive increase attention compare to conventional sub-word base automatic speech recognition model use phone , character , or context-dependent hidden Markov model state . This be because A2W model recognize word from speech without any decoder , pronunciation lexicon , or externally-trained language model , make training and decode with such model simple . Prior work have show that A2W model require order of magnitude more training data in order to perform comparably to conventional model . Our work also show this accuracy gap when use the English Switchboard-Fisher data set . This paper describe a recipe to train an A2W model that close this gap and be at-par with state-of-the-art sub-word base model . We achieve a word error rate of 8.8 % /13.9 % on the Hub5-2000 Switchboard/CallHome test set without any decoder or language model . We find that model initialization , train data order , and regularization have the most impact on the A2W model performance . Next , we present a joint word-character A2W model that learn to first spell the word and then recognize it . This model provide a rich output to the user instead of simple word hypothesis , make it especially useful in the case of word unseen or rarely-seen during training . ","http://arxiv.org/pdf/1712.03133v1","cs.CL	cs.AI	cs.NE	stat.ML","Building competitive direct acoustics-to-word models for English   conversational speech recognition",""
"Huijuan Xu	Kate Saenko","17","11","2015","We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to the VQA task. Memory networks are recurrent neural networks with an explicit attention mechanism that selects certain parts of the information stored in memory. Our Spatial Memory Network stores neuron activations from different spatial regions of the image in its memory, and uses the question to choose relevant regions for computing the answer, a process of which constitutes a single "hop" in the network. We propose a novel spatial attention architecture that aligns words with image patches in the first hop, and obtain improved results by adding a second attention hop which considers the whole question to choose visual evidence based on the results of the first hop. To better understand the inference process learned by the network, we design synthetic questions that specifically require spatial inference and visualize the attention weights. We evaluate our model on two published visual question answering datasets, DAQUAR [1] and VQA [2], and obtain improved results compared to a strong deep baseline model (iBOWIMG) which concatenates image and question features to predict the answer [3].","We address the problem of Visual Question Answering ( VQA ) , which require joint image and language understanding to answer a question about a give photograph . Recent approach have apply deep image caption method base on convolutional-recurrent network to this problem , but have fail to model spatial inference . To remedy this , we propose a model we call the Spatial Memory Network and apply it to the VQA task . Memory network be recurrent neural network with an explicit attention mechanism that select certain part of the information store in memory . Our Spatial Memory Network store neuron activation from different spatial region of the image in it memory , and use the question to choose relevant region for compute the answer , a process of which constitute a single `` hop '' in the network . We propose a novel spatial attention architecture that align word with image patch in the first hop , and obtain improved result by add a second attention hop which consider the whole question to choose visual evidence base on the result of the first hop . To well understand the inference process learn by the network , we design synthetic question that specifically require spatial inference and visualize the attention weight . We evaluate our model on two publish visual question answer datasets , DAQUAR [ 1 ] and VQA [ 2 ] , and obtain improved result compare to a strong deep baseline model ( iBOWIMG ) which concatenate image and question feature to predict the answer [ 3 ] . ","http://arxiv.org/pdf/1511.05234v2","cs.CV	cs.AI	cs.CL	cs.NE","Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for   Visual Question Answering","hxu1@cs.uml.edu,	saenko@cs.uml.edu"
"Yuetan Lin	Zhangyang Pang	Donghui Wang	Yueting Zhuang","22","2","2017","Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.","Visual question answering ( VQA ) have witness great progress since May , 2015 a a classic problem unify visual and textual data into a system . Many enlighten VQA work explore deep into the image and question encoding and fuse method , of which attention be the most effective and infusive mechanism . Current attention base method focus on adequate fusion of visual and textual feature , but lack the attention to where people focus to ask question about the image . Traditional attention base method attach a single value to the feature at each spatial location , which loss many useful information . To remedy these problem , we propose a general method to perform saliency-like pre-selection on overlapped region feature by the interrelation of bidirectional LSTM ( BiLSTM ) , and use a novel element-wise multiplication base attention method to capture more competent correlation information between visual and textual feature . We conduct experiment on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrate by strong empirical result . ","http://arxiv.org/pdf/1702.06700v1","cs.CV	cs.AI	cs.CL	cs.NE","Task-driven Visual Saliency and Attention-based Visual Question   Answering","linyuetan@zju.edu.cn	pzy@zju.edu.cn	dhwang@zju.edu.cn	yzhuang@zju.edu.cn"
"Akash Kumar Dhaka	Giampiero Salvi","29","6","2016","We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction.","We present a systematic analysis on the performance of a phonetic recogniser when the window of input feature be not symmetric with respect to the current frame . The recogniser be base on Context Dependent Deep Neural Networks ( CD-DNNs ) and Hidden Markov Models ( HMMs ) . The objective be to reduce the latency of the system by reduce the number of future feature frame require to estimate the current output . Our test perform on the TIMIT database show that the performance do not degrade when the input window be shift up to 5 frame in the past compare to common practice ( no future frame ) . This correspond to improve the latency by 50 m in our setting . Our test also show that the best result be not obtain with the symmetric window commonly employ , but with an asymmetric window with eight past and two future context frame , although this observation should be confirm on other data set . The reduction in latency suggest by our result be critical for specific application such a real-time lip synchronisation for tele-presence , but may also be beneficial in general application to improve the lag in human-machine spoken interaction . ","http://arxiv.org/pdf/1606.09163v1","cs.CL	cs.CV	cs.NE	stat.ML","Optimising The Input Window Alignment in CD-DNN Based Phoneme   Recognition for Low Latency Processing","akashd@kth.se	giampi@kth.se"
"Peng Qian	Xipeng Qiu	Xuanjing Huang","22","4","2016","Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading.","Recently , the long short-term memory neural network ( LSTM ) have attract wide interest due to it success in many task . LSTM architecture consist of a memory cell and three gate , which look similar to the neuronal network in the brain . However , there still lack the evidence of the cognitive plausibility of LSTM architecture as well a it work mechanism . In this paper , we study the cognitive plausibility of LSTM by align it internal architecture with the brain activity observe via fMRI when the subject read a story . Experiment result show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activity , indicate the correlation between LSTM architecture and the cognitive process of story reading . ","http://arxiv.org/pdf/1604.06635v1","cs.CL	cs.AI	cs.LG	cs.NE","Bridging LSTM Architecture and the Neural Dynamics during Reading","pqian11@fudan.edu.cn	xpqiu@fudan.edu.cn	xjhuang@fudan.edu.cn"
"Jiwei Li","11","12","2014","This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform "weight tuning" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.","This paper address how a recursive neural network model can automatically leave out useless information and emphasize important evidence , in other word , to perform `` weight tuning '' for higher-level representation acquisition . We propose two model , Weighted Neural Network ( WNN ) and Binary-Expectation Neural Network ( BENN ) , which automatically control how much one specific unit contribute to the higher-level representation . The propose model can be view a incorporate a more powerful compositional function for embed acquisition in recursive neural network . Experimental result demonstrate the significant improvement over standard neural model . ","http://arxiv.org/pdf/1412.3714v2","cs.NE	cs.AI	cs.CL	cs.LG","Feature Weight Tuning for Recursive Neural Networks","jiweil@stanford.edu"
"Sadikin Mujiono	Mohamad Ivan Fanany	Chan Basaruddin","6","10","2016","One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.","One essential task in information extraction from the medical corpus be drug name recognition . Compared with text source come from other domain , the medical text be special and have unique characteristic . In addition , the medical text mining pose more challenge , e.g. , more unstructured text , the fast growing of new term addition , a wide range of name variation for the same drug . The mining be even more challenging due to the lack of labeled dataset source and external knowledge , as well a multiple token representation for a single drug name that be more common in the real application set . Although many approach have be propose to overwhelm the task , some problem remain with poor F-score performance ( less than 0.75 ) . This paper present a new treatment in data representation technique to overcome some of those challenge . We propose three data representation technique base on the characteristic of word distribution and word similarity a a result of word embed training . The first technique be evaluate with the standard NN model , i.e. , MLP ( Multi-Layer Perceptrons ) . The second technique involve two deep network classifier , i.e. , DBN ( Deep Belief Networks ) , and SAE ( Stacked Denoising Encoders ) . The third technique represent the sentence a a sequence that be evaluate with a recurrent NN model , i.e. , LSTM ( Long Short Term Memory ) . In extract the drug name entity , the third technique give the best F-score performance compare to the state of the art , with it average F-score be 0.8645 . ","http://arxiv.org/pdf/1610.01891v1","cs.CL	cs.AI	cs.LG	cs.NE	68Txx	I.2.4","A New Data Representation Based on Training Data Characteristics to   Extract Drug Named-Entity in Medical Text","mujiono.sadikin@mercubuana.ac.id"
"Eric Malmi	Pyry Takala	Hannu Toivonen	Tapani Raiko	Aristides Gionis","18","5","2015","Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.","Writing rap lyric require both creativity to construct a meaningful , interesting story and lyrical skill to produce complex rhyme pattern , which form the cornerstone of good flow . We present a rap lyric generation method that capture both of these aspect . First , we develop a prediction model to identify the next line of exist lyric from a set of candidate next line . This model be base on two machine-learning technique : the RankSVM algorithm and a deep neural network model with a novel structure . Results show that the prediction model can identify the true next line among 299 randomly select line with an accuracy of 17 % , i.e. , over 50 time more likely than by random . Second , we employ the prediction model to combine line from exist song , produce lyric with rhyme and a meaning . An evaluation of the produced lyric show that in term of quantitative rhyme density , the method outperform the best human rapper by 21 % . The rap lyric generator have be deploy a an online tool call DeepBeat , and the performance of the tool have be assess by analyze it usage log . This analysis show that machine-learned ranking correlate with user preference . ","http://arxiv.org/pdf/1505.04771v2","cs.LG	cs.AI	cs.CL	cs.NE	I.2.7; H.3.3","DopeLearning: A Computational Approach to Rap Lyrics Generation","eric.malmi@aalto.fi	pyry.takala@aalto.fi	hannu.toivonen@cs.helsinki.fi	tapani.raiko@aalto.fi	aristides.gionis@aalto.fi"
"Shengxian Wan	Yanyan Lan	Jun Xu	Jiafeng Guo	Liang Pang	Xueqi Cheng","15","4","2016","Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architecture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spatial RNN is applied to integrate the local interactions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact matching scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpretation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.","Semantic matching , which aim to determine the match degree between two text , be a fundamental problem for many NLP application . Recently , deep learning approach have be apply to this problem and significant improvement have be achieve . In this paper , we propose to view the generation of the global interaction between two text a a recursive process : i.e . the interaction of two text at each position be a composition of the interaction between their prefix as well a the word level interaction at the current position . Based on this idea , we propose a novel deep architecture , namely Match-SRNN , to model the recursive matching structure . Firstly , a tensor be construct to capture the word level interaction . Then a spatial RNN be apply to integrate the local interaction recursively , with importance determine by four type of gate . Finally , the match score be calculate base on the global interaction . We show that , after degenerate to the exact matching scenario , Match-SRNN can approximate the dynamic programming process of long common subsequence . Thus , there exist a clear interpretation for Match-SRNN . Our experiment on two semantic match task show the effectiveness of Match-SRNN , and it ability of visualize the learned matching structure . ","http://arxiv.org/pdf/1604.04378v1","cs.CL	cs.AI	cs.LG	cs.NE","Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN","wanshengxian	pangliang}@software.ict.ac.cn	lanyanyan@ict.ac.cn	junxu@ict.ac.cn	guojiafeng@ict.ac.cn	cxq@ict.ac.cn"
"Iulian V. Serban	Alexander G. Ororbia II	Joelle Pineau	Aaron Courville","1","12","2016","Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - which are incapable of representing complex latent factors efficiently. To overcome this restriction, we propose the simple, but highly flexible, piecewise constant distribution. This distribution has the capacity to represent an exponential number of modes of a latent target distribution, while remaining mathematically tractable. Our results demonstrate that incorporating this new latent distribution into different models yields substantial improvements in natural language processing tasks such as document modeling and natural language generation for dialogue.","Advances in neural variational inference have facilitate the learning of powerful directed graphical model with continuous latent variable , such a variational autoencoders . The hope be that such model will learn to represent rich , multi-modal latent factor in real-world data , such a natural language text . However , current model often assume simplistic prior on the latent variables - such a the uni-modal Gaussian distribution - which be incapable of represent complex latent factor efficiently . To overcome this restriction , we propose the simple , but highly flexible , piecewise constant distribution . This distribution have the capacity to represent an exponential number of mode of a latent target distribution , while remain mathematically tractable . Our result demonstrate that incorporate this new latent distribution into different model yield substantial improvement in natural language process task such a document modeling and natural language generation for dialogue . ","http://arxiv.org/pdf/1612.00377v4","cs.CL	cs.AI	cs.LG	cs.NE	I.5.1; I.2.7","Piecewise Latent Variables for Neural Variational Text Processing",""
"Baolin Peng	Kaisheng Yao","31","5","2015","Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of simple RNNs is limited because of the gradient vanishing and exploding problem. We propose to use an external memory to improve memorization capability of RNNs. We conducted experiments on the ATIS dataset, and observed that the proposed model was able to achieve the state-of-the-art results. We compare our proposed model with alternative models and report analysis results that may provide insights for future research.","Recurrent Neural Networks ( RNNs ) have become increasingly popular for the task of language understanding . In this task , a semantic tagger be deploy to associate a semantic label to each word in an input sequence . The success of RNN may be attribute to it ability to memorize long-term dependence that relate the current-time semantic label prediction to the observation many time instance away . However , the memory capacity of simple RNNs be limited because of the gradient vanishing and explode problem . We propose to use an external memory to improve memorization capability of RNNs . We conduct experiment on the ATIS dataset , and observe that the propose model be able to achieve the state-of-the-art result . We compare our propose model with alternative model and report analysis result that may provide insight for future research . ","http://arxiv.org/pdf/1506.00195v1","cs.CL	cs.AI	cs.LG	cs.NE","Recurrent Neural Networks with External Memory for Language   Understanding","blpeng@se.cuhk.edu.hk,	kaisheny@microsoft.com"
"Alessandro Sordoni	Michel Galley	Michael Auli	Chris Brockett	Yangfeng Ji	Margaret Mitchell	Jian-Yun Nie	Jianfeng Gao	Bill Dolan","22","6","2015","We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.","We present a novel response generation system that can be trained end to end on large quantity of unstructured Twitter conversation . A neural network architecture be use to address sparsity issue that arise when integrate contextual information into classic statistical model , allow the system to take into account previous dialog utterance . Our dynamic-context generative model show consistent gain over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baseline . ","http://arxiv.org/pdf/1506.06714v1","cs.CL	cs.AI	cs.LG	cs.NE","A Neural Network Approach to Context-Sensitive Generation of   Conversational Responses",""
"Ryan Lowe	Nissan Pow	Iulian Serban	Joelle Pineau","30","6","2015","This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.","This paper introduce the Ubuntu Dialogue Corpus , a dataset contain almost 1 million multi-turn dialogue , with a total of over 7 million utterance and 100 million word . This provide a unique resource for research into build dialogue manager base on neural language model that can make use of large amount of unlabeled data . The dataset have both the multi-turn property of conversation in the Dialog State Tracking Challenge datasets , and the unstructured nature of interaction from microblog service such a Twitter . We also describe two neural learn architecture suitable for analyze this dataset , and provide benchmark performance on the task of select the best next response . ","http://arxiv.org/pdf/1506.08909v3","cs.CL	cs.AI	cs.LG	cs.NE","The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured   Multi-Turn Dialogue Systems",""
"Iulian V. Serban	Alessandro Sordoni	Yoshua Bengio	Aaron Courville	Joelle Pineau","17","7","2015","We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.","We investigate the task of build open domain , conversational dialogue system base on large dialogue corpus use generative model . Generative model produce system response that be autonomously generate word-by-word , open up the possibility for realistic , flexible interaction . In support of this goal , we extend the recently propose hierarchical recurrent encoder-decoder neural network to the dialogue domain , and demonstrate that this model be competitive with state-of-the-art neural language model and back-off n-gram model . We investigate the limitation of this and similar approach , and show how it performance can be improve by bootstrapping the learning from a large question-answer pair corpus and from pretrained word embeddings . ","http://arxiv.org/pdf/1507.04808v3","cs.CL	cs.AI	cs.LG	cs.NE	I.5.1; I.2.7","Building End-To-End Dialogue Systems Using Generative Hierarchical   Neural Network Models",""
"Dzmitry Bahdanau	Jan Chorowski	Dmitriy Serdyuk	Philemon Brakel	Yoshua Bengio","18","8","2015","Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.","Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems ( LVCSR ) be hybrid of neural network and Hidden Markov Models ( HMMs ) . Most of these system contain separate component that deal with the acoustic modelling , language modelling and sequence decoding . We investigate a more direct approach in which the HMM be replace with a Recurrent Neural Network ( RNN ) that perform sequence prediction directly at the character level . Alignment between the input feature and the desired character sequence be learn automatically by an attention mechanism build into the RNN . For each predicted character , the attention mechanism scan the input sequence and choose relevant frame . We propose two method to speed up this operation : limit the scan to a subset of most promising frame and pool over time the information contain in neighbor frame , thereby reduce source sequence length . Integrating an n-gram language model into the decode process yield recognition accuracy similar to other HMM-free RNN-based approach . ","http://arxiv.org/pdf/1508.04395v2","cs.CL	cs.AI	cs.LG	cs.NE","End-to-End Attention-based Large Vocabulary Speech Recognition",""
"Baolin Peng	Zhengdong Lu	Hang Li	Kam-Fai Wong","22","8","2015","We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.","We propose Neural Reasoner , a framework for neural network-based reasoning over natural language sentence . Given a question , Neural Reasoner can infer over multiple support fact and find an answer to the question in specific form . Neural Reasoner have 1 ) a specific interaction-pooling mechanism , allow it to examine multiple fact , and 2 ) a deep architecture , allow it to model the complicated logical relation in reason task . Assuming no particular structure exist in the question and fact , Neural Reasoner be able to accommodate different type of reasoning and different form of language expression . Despite the model complexity , Neural Reasoner can still be train effectively in an end-to-end manner . Our empirical study show that Neural Reasoner can outperform exist neural reason system with remarkable margin on two difficult artificial task ( Positional Reasoning and Path Finding ) propose in [ 8 ] . For example , it improve the accuracy on Path Finding ( 10K ) from 33.4 % [ 6 ] to over 98 % . ","http://arxiv.org/pdf/1508.05508v1","cs.AI	cs.CL	cs.LG	cs.NE","Towards Neural Network-based Reasoning","blpeng@se.cuhk.edu.hk	kfwong@se.cuhk.edu.hk	Lu.Zhengdong@huawei.com	HangLi.HL@huawei.com"
"Hongyuan Mei	Mohit Bansal	Matthew R. Walter","2","9","2015","We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.","We propose an end-to-end , domain-independent neural encoder-aligner-decoder model for selective generation , i.e. , the joint task of content selection and surface realization . Our model first encode a full set of over-determined database event record via an LSTM-based recurrent neural network , then utilize a novel coarse-to-fine aligner to identify the small subset of salient record to talk about , and finally employ a decoder to generate free-form description of the align , select record . Our model achieve the best selection and generation result report to-date ( with 59 % relative improvement in generation ) on the benchmark WeatherGov dataset , despite use no specialized feature or linguistic resource . Using an improved k-nearest neighbor beam filter help far . We also perform a series of ablation and visualization to elucidate the contribution of our key model component . Lastly , we evaluate the generalizability of our model on the RoboCup dataset , and get result that be competitive with or good than the state-of-the-art , despite be severely data-starved . ","http://arxiv.org/pdf/1509.00838v2","cs.CL	cs.AI	cs.LG	cs.NE","What to talk about and how? Selective Generation using LSTMs with   Coarse-to-Fine Alignment","hongyuan@ttic.edu	mbansal@ttic.edu	mwalter@ttic.edu"
"Tim RocktÃ¤schel	Edward Grefenstette	Karl Moritz Hermann	TomÃ¡Å¡ KoÄiskÃ½	Phil Blunsom","22","9","2015","While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifier. In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units. We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases. Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities. On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin. It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.","While most approach to automatically recognize entailment relation have use classifier employ hand engineer feature derive from complex natural language processing pipeline , in practice their performance have be only slightly good than bag-of-word pair classifier use only lexical similarity . The only attempt so far to build an end-to-end differentiable neural network for entailment fail to outperform such a simple similarity classifier . In this paper , we propose a neural model that read two sentence to determine entailment use long short-term memory unit . We extend this model with a word-by-word neural attention mechanism that encourage reason over entailment of pair of word and phrase . Furthermore , we present a qualitative analysis of attention weight produce by this model , demonstrate such reasoning capability . On a large entailment dataset this model outperform the previous best neural model and a classifier with engineer feature by a substantial margin . It be the first generic end-to-end differentiable system that achieve state-of-the-art accuracy on a textual entailment dataset . ","http://arxiv.org/pdf/1509.06664v4","cs.CL	cs.AI	cs.LG	cs.NE	68T50	I.2.6; I.2.7","Reasoning about Entailment with Neural Attention","t.rocktaschel@cs.ucl.ac.uk	etg@google.com	kmh@google.com	tkocisky@google.com	pblunsom@google.com"
"Yu Zhang	Guoguo Chen	Dong Yu	Kaisheng Yao	Sanjeev Khudanpur	James Glass","30","10","2015","In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and $5.3\%$ relative improvement respectively.","In this paper , we extend the deep long short-term memory ( DLSTM ) recurrent neural network by introduce gate direct connection between memory cell in adjacent layer . These direct link , call highway connection , enable unimpeded information flow across different layer and thus alleviate the gradient vanish problem when building deeper LSTMs . We further introduce the latency-controlled bidirectional LSTMs ( BLSTMs ) which can exploit the whole history while keep the latency under control . Efficient algorithm be propose to train these novel network use both frame and sequence discriminative criterion . Experiments on the AMI distant speech recognition ( DSR ) task indicate that we can train deep LSTMs and achieve good improvement from sequence training with highway LSTMs ( HLSTMs ) . Our novel model obtain $ 43.9/47.7\ % $ WER on AMI ( SDM ) dev and eval set , outperform all previous work . It beat the strong DNN and DLSTM baseline with $ 15.7\ % $ and $ 5.3\ % $ relative improvement respectively . ","http://arxiv.org/pdf/1510.08983v2","cs.NE	cs.AI	cs.CL	cs.LG","Highway Long Short-Term Memory RNNs for Distant Speech Recognition","yzhang87	glass}@mit.edu	guoguo	khudanpur}@jhu.edu	dongyu@microsoft.com	Kaisheng.YAO@microsoft.com"
"Pengcheng Yin	Zhengdong Lu	Hang Li	Ben Kao","3","12","2015","We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully "neuralized": it not only gives distributional representation of the query and the knowledge-base, but also realizes the execution of compositional queries as a series of differentiable operations, with intermediate results (consisting of annotations of the tables at different levels) saved on multiple layers of memory. Neural Enquirer can be trained with gradient descent, with which not only the parameters of the controlling components and semantic parsing component, but also the embeddings of the tables and query words can be learned from scratch. The training can be done in an end-to-end fashion, but it can take stronger guidance, e.g., the step-by-step supervision for complicated queries, and benefit from it. Neural Enquirer is one step towards building neural network systems which seek to understand language by executing it on real-world. Our experiments show that Neural Enquirer can learn to execute fairly complicated NL queries on tables with rich structures.","We propose Neural Enquirer a a neural network architecture to execute a natural language ( NL ) query on a knowledge-base ( KB ) for answer . Basically , Neural Enquirer find the distributed representation of a query and then execute it on knowledge-base table to obtain the answer a one of the value in the table . Unlike similar effort in end-to-end training of semantic parser , Neural Enquirer be fully `` neuralized '' : it not only give distributional representation of the query and the knowledge-base , but also realize the execution of compositional query a a series of differentiable operation , with intermediate result ( consist of annotation of the table at different level ) save on multiple layer of memory . Neural Enquirer can be train with gradient descent , with which not only the parameter of the control component and semantic parsing component , but also the embeddings of the table and query word can be learn from scratch . The training can be do in an end-to-end fashion , but it can take strong guidance , e.g. , the step-by-step supervision for complicated query , and benefit from it . Neural Enquirer be one step towards build neural network system which seek to understand language by execute it on real-world . Our experiment show that Neural Enquirer can learn to execute fairly complicate NL query on table with rich structure . ","http://arxiv.org/pdf/1512.00965v2","cs.AI	cs.CL	cs.LG	cs.NE","Neural Enquirer: Learning to Query Tables with Natural Language","pcyin@cs.hku.hk	kao@cs.hku.hk	Lu.Zhengdong@huawei.com	HangLi.HL@huawei.com"
"Petr BaudiÅ¡	Jan Pichl	TomÃ¡Å¡ VyskoÄil	Jan Å edivÃ½","19","3","2016","We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks.   We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common IR metrics and popular convolutional, recurrent and attention-based neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence model. We set a new state-of-art in performance on the Ubuntu Dialogue dataset.","We review the task of Sentence Pair Scoring , popular in the literature in various form - view a Answer Sentence Selection , Semantic Text Scoring , Next Utterance Ranking , Recognizing Textual Entailment , Paraphrasing or e.g . a component of Memory Networks . We argue that all such task be similar from the model perspective and propose new baseline by compare the performance of common IR metric and popular convolutional , recurrent and attention-based neural model across many Sentence Pair Scoring task and datasets . We discuss the problem of evaluate randomized model , propose a statistically grounded methodology , and attempt to improve comparison by release new datasets that be much hard than some of the currently use well explored benchmark . We introduce a unified open source software framework with easily pluggable model and task , which enable u to experiment with multi-task reusability of trained sentence model . We set a new state-of-art in performance on the Ubuntu Dialogue dataset . ","http://arxiv.org/pdf/1603.06127v4","cs.CL	cs.AI	cs.LG	cs.NE","Sentence Pair Scoring: Towards Unified Framework for Text Comprehension","baudipet@fel.cvut.cz"
"Jiatao Gu	Zhengdong Lu	Hang Li	Victor O. K. Li","21","3","2016","We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.","We address an important problem in sequence-to-sequence ( Seq2Seq ) learning refer to a copying , in which certain segment in the input sequence be selectively replicate in the output sequence . A similar phenomenon be observable in human language communication . For example , human tend to repeat entity name or even long phrase in conversation . The challenge with regard to copy in Seq2Seq be that new machinery be need to decide when to perform the operation . In this paper , we incorporate copy into neural network-based Seq2Seq learning and propose a new model call CopyNet with encoder-decoder structure . CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper place in the output sequence . Our empirical study on both synthetic data set and real world data set demonstrate the efficacy of CopyNet . For example , CopyNet can outperform regular RNN-based model with remarkable margin on text summarization task . ","http://arxiv.org/pdf/1603.06393v3","cs.CL	cs.AI	cs.LG	cs.NE","Incorporating Copying Mechanism in Sequence-to-Sequence Learning","jiataogu@eee.hku.hk	vli@eee.hku.hk	lu.zhengdong@huawei.com	hangli.hl@huawei.com"
"Iulian Vlad Serban	Alberto GarcÃ­a-DurÃ¡n	Caglar Gulcehre	Sungjin Ahn	Sarath Chandar	Aaron Courville	Yoshua Bengio","22","3","2016","Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.","Over the past decade , large-scale supervise learn corpus have enable machine learn researcher to make substantial advance . However , to this date , there be no large-scale question-answer corpus available . In this paper we present the 30M Factoid Question-Answer Corpus , an enormous question answer pair corpus produce by apply a novel neural network architecture on the knowledge base Freebase to transduce fact into natural language question . The produced question answer pair be evaluate both by human evaluator and use automatic evaluation metric , include well-established machine translation and sentence similarity metric . Across all evaluation criteria the question-generation model outperform the compete template-based baseline . Furthermore , when present to human evaluator , the generated question appear comparable in quality to real human-generated question . ","http://arxiv.org/pdf/1603.06807v2","cs.CL	cs.AI	cs.LG	cs.NE	H.3.4; I.5.1; I.2.6; I.2.7","Generating Factoid Questions With Recurrent Neural Networks: The 30M   Factoid Question-Answer Corpus",""
"Chia-Wei Liu	Ryan Lowe	Iulian V. Serban	Michael Noseworthy	Laurent Charlin	Joelle Pineau","25","3","2016","We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.","We investigate evaluation metric for dialogue response generation system where supervised label , such a task completion , be not available . Recent work in response generation have adopt metric from machine translation to compare a model 's generate response to a single target response . We show that these metric correlate very weakly with human judgement in the non-technical Twitter domain , and not at all in the technical Ubuntu domain . We provide quantitative and qualitative result highlight specific weakness in exist metric , and provide recommendation for future development of good automatic evaluation metric for dialogue system . ","http://arxiv.org/pdf/1603.08023v2","cs.CL	cs.AI	cs.LG	cs.NE","How NOT To Evaluate Your Dialogue System: An Empirical Study of   Unsupervised Evaluation Metrics for Dialogue Response Generation","chia-wei.liu@mail.mcgill.ca	ryan.lowe@mail.mcgill.ca	michael.noseworthy@mail.mcgill.ca	lcharlin@cs.mcgill.ca	jpineau@cs.mcgill.ca	iulian.vlad.serban@umontreal.ca"
"Iulian Vlad Serban	Alessandro Sordoni	Ryan Lowe	Laurent Charlin	Joelle Pineau	Aaron Courville	Yoshua Bengio","19","5","2016","Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with recent neural network architectures. We evaluate the model performance through automatic evaluation metrics and by carrying out a human evaluation. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate the generation of long outputs and maintain the context.","Sequential data often possess a hierarchical structure with complex dependency between subsequence , such a found between the utterance in a dialogue . In an effort to model this kind of generative process , we propose a neural network-based generative architecture , with latent stochastic variable that span a variable number of time step . We apply the propose model to the task of dialogue response generation and compare it with recent neural network architecture . We evaluate the model performance through automatic evaluation metric and by carry out a human evaluation . The experiment demonstrate that our model improve upon recently propose model and that the latent variable facilitate the generation of long output and maintain the context . ","http://arxiv.org/pdf/1605.06069v3","cs.CL	cs.AI	cs.LG	cs.NE	I.5.1; I.2.7","A Hierarchical Latent Variable Encoder-Decoder Model for Generating   Dialogues","iulian.vlad.serban@umontreal.ca	aaron.courville@umontreal.ca	yoshua.bengio@umontreal.ca	alessandro.sordoni@maluuba.com	ryan.lowe@cs.mcgill.ca	lcharlin@cs.mcgill.ca	jpineau@cs.mcgill.ca"
"Dirk Weissenborn","13","6","2016","Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.","Many important NLP problem can be pose a dual-sequence or sequence-to-sequence modeling task . Recent advance in build end-to-end neural architecture have be highly successful in solve such task . In this work we propose a new architecture for dual-sequence modeling that be base on associative memory . We derive AM-RNNs , a recurrent associative memory ( AM ) which augment generic recurrent neural network ( RNN ) . This architecture be extend to the Dual AM-RNN which operate on two AMs at once . Our model achieve very competitive result on textual entailment . A qualitative analysis demonstrate that long range dependency between source and target-sequence can be bridge effectively use Dual AM-RNNs . However , an initial experiment on auto-encoding reveals that these benefit be not exploit by the system when learn to solve sequence-to-sequence task which indicate that additional supervision or regularization be need . ","http://arxiv.org/pdf/1606.03864v2","cs.NE	cs.AI	cs.CL	cs.LG","Neural Associative Memory for Dual-Sequence Modeling","dirk.weissenborn@dfki.de"
"Marc Dymetman	Chunyang Xiao","8","7","2016","We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiring that each combination is directly observed in the training data (as the softmax does). Second, it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features.   We conduct experiments in the domain of language modelling of French, that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline RNN.   We provide other motivating iillustrations, and finally argue that the log-linear and the neural-network components contribute complementary strengths to the LL-RNN: the LL aspect allows the model to incorporate rich prior knowledge, while the NN aspect, according to the "representation learning" paradigm, allows the model to discover novel combination of characteristics.","We introduce LL-RNNs ( Log-Linear RNNs ) , an extension of Recurrent Neural Networks that replace the softmax output layer by a log-linear output layer , of which the softmax be a special case . This conceptually simple move have two main advantage . First , it allow the learner to combat train data sparsity by allow it to model word ( or more generally , output symbol ) a complex combination of attribute without require that each combination be directly observe in the training data ( a the softmax do ) . Second , it permit the inclusion of flexible prior knowledge in the form of a priori specify modular feature , where the neural network component learn to dynamically control the weight of a log-linear distribution exploit these feature . We conduct experiment in the domain of language modelling of French , that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline RNN . We provide other motivate iillustrations , and finally argue that the log-linear and the neural-network component contribute complementary strength to the LL-RNN : the LL aspect allow the model to incorporate rich prior knowledge , while the NN aspect , accord to the `` representation learning '' paradigm , allow the model to discover novel combination of characteristic . ","http://arxiv.org/pdf/1607.02467v2","cs.AI	cs.CL	cs.LG	cs.NE","Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior   Knowledge","marc.dymetman@xrce.xerox.com	chunyang.xiao@xrce.xerox.com"
"Ondrej Bajgar	Rudolf Kadlec	Jan Kleindienst","4","10","2016","There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.","There be a practically unlimited amount of natural language data available . Still , recent work in text comprehension have focus on datasets which be small relative to current compute possibility . This article be make a case for the community to move to large data and a a step in that direction it be propose the BookTest , a new dataset similar to the popular Children 's Book Test ( CBT ) , however more than 60 time large . We show that train on the new data improve the accuracy of our Attention-Sum Reader model on the original CBT test data by a much large margin than many recent attempt to improve the model architecture . On one version of the dataset our ensemble even exceed the human baseline provide by Facebook . We then show in our own human study that there be still space for further improvement . ","http://arxiv.org/pdf/1610.00956v1","cs.CL	cs.AI	cs.LG	cs.NE","Embracing data abundance: BookTest Dataset for Reading Comprehension","obajgar@cz.ibm.com	rudolf kadlec@cz.ibm.com	jankle@cz.ibm.com"
"James Bradbury	Stephen Merity	Caiming Xiong	Richard Socher","5","11","2016","Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.","Recurrent neural network be a powerful tool for model sequential data , but the dependence of each timestep 's computation on the previous timestep 's output limit parallelism and make RNNs unwieldy for very long sequence . We introduce quasi-recurrent neural network ( QRNNs ) , an approach to neural sequence modeling that alternate convolutional layer , which apply in parallel across timesteps , and a minimalist recurrent pool function that apply in parallel across channel . Despite lack trainable recurrent layer , stack QRNNs have well predictive accuracy than stack LSTMs of the same hidden size . Due to their increased parallelism , they be up to 16 time faster at train and test time . Experiments on language modeling , sentiment classification , and character-level neural machine translation demonstrate these advantage and underline the viability of QRNNs a a basic building block for a variety of sequence task . ","http://arxiv.org/pdf/1611.01576v2","cs.NE	cs.AI	cs.CL	cs.LG","Quasi-Recurrent Neural Networks","james.bradbury@salesforce.com	smerity@salesforce.com	cxiong@salesforce.com	rsocher@salesforce.com"
"Jakob N. Foerster	Justin Gilmer	Jan Chorowski	Jascha Sohl-Dickstein	David Sussillo","28","11","2016","There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task. Despite this ease of interpretation, the input switched affine network achieves reasonable performance on a text modeling tasks, and allows greater computational efficiency than networks with standard nonlinearities.","There exist many problem domain where the interpretability of neural network model be essential for deployment . Here we introduce a recurrent architecture compose of input-switched affine transformation - in other word an RNN without any explicit nonlinearities , but with input-dependent recurrent weight . This simple form allow the RNN to be analyze via straightforward linear method : we can exactly characterize the linear contribution of each input to the model prediction ; we can use a change-of-basis to disentangle input , output , and computational hidden unit subspace ; we can fully reverse-engineer the architecture 's solution to a simple task . Despite this ease of interpretation , the input switch affine network achieves reasonable performance on a text modeling task , and allow great computational efficiency than network with standard nonlinearities . ","http://arxiv.org/pdf/1611.09434v2","cs.AI	cs.CL	cs.LG	cs.NE","Input Switched Affine Networks: An RNN Architecture Designed for   Interpretability",""
"MichaÅ Daniluk	Tim RocktÃ¤schel	Johannes Welbl	Sebastian Riedel","15","2","2017","Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.","Neural language model predict the next token use a latent representation of the immediate token history . Recently , various method for augment neural language model with an attention mechanism over a differentiable memory have be propose . For predict the next token , these model query information from a memory of the recent history which can facilitate learning mid- and long-range dependency . However , conventional attention mechanism use in memory-augmented neural language model produce a single output vector per time step . This vector be use both for predict the next token as well a for the key and value of a differentiable memory of a token history . In this paper , we propose a neural language model with a key-value attention mechanism that output separate representation for the key and value of a differentiable memory , as well a for encode the next-word distribution . This model outperform exist memory-augmented neural language model on two corpus . Yet , we find that our method mainly utilize a memory of the five most recent output representation . This lead to the unexpected main finding that a much simple model base only on the concatenation of recent output representation from previous time step be on par with more sophisticated memory-augmented neural language model . ","http://arxiv.org/pdf/1702.04521v1","cs.CL	cs.AI	cs.LG	cs.NE","Frustratingly Short Attention Spans in Neural Language Modeling","michal.daniluk.15@ucl.ac.uk,	t.rocktaschel@cs.ucl.ac.uk	j.welbl@cs.ucl.ac.uk	s.riedel@cs.ucl.ac.uk"
"Zhouhan Lin	Minwei Feng	Cicero Nogueira dos Santos	Mo Yu	Bing Xiang	Bowen Zhou	Yoshua Bengio","9","3","2017","This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.","This paper propose a new model for extract an interpretable sentence embedding by introduce self-attention . Instead of use a vector , we use a 2-D matrix to represent the embedding , with each row of the matrix attend on a different part of the sentence . We also propose a self-attention mechanism and a special regularization term for the model . As a side effect , the embed come with an easy way of visualize what specific part of the sentence be encode into the embedding . We evaluate our model on 3 different task : author profiling , sentiment classification , and textual entailment . Results show that our model yield a significant performance gain compare to other sentence embed method in all of the 3 task . ","http://arxiv.org/pdf/1703.03130v1","cs.CL	cs.AI	cs.LG	cs.NE","A Structured Self-attentive Sentence Embedding","lin.zhouhan@gmail.com	mfeng@us.ibm.com	cicerons@us.ibm.com	yum@us.ibm.com	bingxia@us.ibm.com	zhou@us.ibm.com"
"Samuel RÃ¶nnqvist	Niko Schenk	Christian Chiarcos","26","4","2017","We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.","We introduce an attention-based Bi-LSTM for Chinese implicit discourse relation and demonstrate that model argument pair a a joint sequence can outperform word order-agnostic approach . Our model benefit from a partial sampling scheme and be conceptually simple , yet achieve state-of-the-art performance on the Chinese Discourse Treebank . We also visualize it attention activity to illustrate the model 's ability to selectively focus on the relevant part of an input sequence . ","http://arxiv.org/pdf/1704.08092v1","cs.CL	cs.AI	cs.LG	cs.NE","A Recurrent Neural Model with Attention for the Recognition of Chinese   Implicit Discourse Relations","sronnqvi@abo.fi	schenk@informatik.uni-frankfurt.de	chiarcos@informatik.uni-frankfurt.de"
"Lara J. Martin	Prithviraj Ammanabrolu	Xinyu Wang	William Hancock	Shruti Singh	Brent Harrison	Mark O. Riedl","5","6","2017","Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the question of event representations that provide a mid-level of abstraction between words and sentences in order to retain the semantic information of the original data while minimizing event sparsity. We present a technique for preprocessing textual story data into event sequences. We then present a technique for automated story generation whereby we decompose the problem into the generation of successive events (event2event) and the generation of natural language sentences from events (event2sentence). We give empirical results comparing different event representations and their effects on event successor generation and the translation of events to natural language.","Automated story generation be the problem of automatically select a sequence of event , action , or word that can be tell a a story . We seek to develop a system that can generate story by learn everything it need to know from textual story corpus . To date , recurrent neural network that learn language model at character , word , or sentence level have have little success generate coherent story . We explore the question of event representation that provide a mid-level of abstraction between word and sentence in order to retain the semantic information of the original data while minimize event sparsity . We present a technique for preprocessing textual story data into event sequence . We then present a technique for automated story generation whereby we decompose the problem into the generation of successive event ( event2event ) and the generation of natural language sentence from event ( event2sentence ) . We give empirical result compare different event representation and their effect on event successor generation and the translation of event to natural language . ","http://arxiv.org/pdf/1706.01331v3","cs.CL	cs.AI	cs.LG	cs.NE","Event Representations for Automated Story Generation with Deep Neural   Nets","riedl]@gatech.edu"
"Tong Wang	Xingdi Yuan	Adam Trischler","5","6","2017","We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model's novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.","We propose a generative machine comprehension model that learn jointly to ask and answer question base on document . The propose model use a sequence-to-sequence framework that encode the document and generate a question ( answer ) give an answer ( question ) . Significant improvement in model performance be observe empirically on the SQuAD corpus , confirm our hypothesis that the model benefit from jointly learn to perform both task . We believe the joint model 's novelty offer a new perspective on machine comprehension beyond architectural engineering , and serve a a first step towards autonomous information seek . ","http://arxiv.org/pdf/1706.01450v1","cs.CL	cs.AI	cs.LG	cs.NE","A Joint Model for Question Answering and Question Generation",""
"Wei Wen	Yuxiong He	Samyam Rajbhandari	Minjia Zhang	Wenhan Wang	Fang Liu	Bin Hu	Yiran Chen	Hai Li","15","9","2017","Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently reducing the sizes of basic structures can result in inconsistent dimensions among them, and consequently, end up with invalid LSTM units. To overcome the problem, we propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS will simultaneously decrease the sizes of all basic structures by one and thereby always maintain the dimension consistency. By learning ISS within LSTM units, the obtained LSTMs remain regular while having much smaller basic structures. Based on group Lasso regularization, our method achieves 10.59x speedup without losing any perplexity of a language modeling of Penn TreeBank dataset. It is also successfully evaluated through a compact model with only 2.69M weights for machine Question Answering of SQuAD dataset. Our approach is successfully extended to non- LSTM RNNs, like Recurrent Highway Networks (RHNs). Our source code is publicly available at https://github.com/wenwei202/iss-rnns","Model compression be significant for the wide adoption of Recurrent Neural Networks ( RNNs ) in both user device possess limited resource and business cluster require quick response to large-scale service request . This work aim to learn structurally-sparse Long Short-Term Memory ( LSTM ) by reduce the size of basic structure within LSTM unit , include input update , gate , hidden state , cell state and output . Independently reduce the size of basic structure can result in inconsistent dimension among them , and consequently , end up with invalid LSTM unit . To overcome the problem , we propose Intrinsic Sparse Structures ( ISS ) in LSTMs . Removing a component of ISS will simultaneously decrease the size of all basic structure by one and thereby always maintain the dimension consistency . By learn ISS within LSTM unit , the obtain LSTMs remain regular while have much small basic structure . Based on group Lasso regularization , our method achieve 10.59x speedup without lose any perplexity of a language modeling of Penn TreeBank dataset . It be also successfully evaluate through a compact model with only 2.69M weight for machine Question Answering of SQuAD dataset . Our approach be successfully extend to non- LSTM RNNs , like Recurrent Highway Networks ( RHNs ) . Our source code be publicly available at http : //github.com/wenwei202/iss-rnns ","http://arxiv.org/pdf/1709.05027v7","cs.LG	cs.AI	cs.CL	cs.NE","Learning Intrinsic Sparse Structures within Long Short-Term Memory","wei.wen@duke.edu	yiran.chen@duke.edu	hai.li@duke.edu	yuxhe@microsoft.com	samyamr@microsoft.com	minjiaz@microsoft.com	wenhanw@microsoft.com	fangliu@microsoft.com	binhu@microsoft.com"
"Huda Hakami	Danushka Bollegala	Hayashi Kohei","19","9","2017","Representing the semantic relations that exist between two given words (or entities) is an important first step in a wide-range of NLP applications such as analogical reasoning, knowledge base completion and relational information retrieval. A simple, yet surprisingly accurate method for representing a relation between two words is to compute the vector offset (\PairDiff) between their corresponding word embeddings. Despite the empirical success, it remains unclear as to whether \PairDiff is the best operator for obtaining a relational representation from word embeddings. We conduct a theoretical analysis of generalised bilinear operators that can be used to measure the $\ell_{2}$ relational distance between two word-pairs. We show that, if the word embeddings are standardised and uncorrelated, such an operator will be independent of bilinear terms, and can be simplified to a linear form, where \PairDiff is a special case. For numerous word embedding types, we empirically verify the uncorrelation assumption, demonstrating the general applicability of our theoretical result. Moreover, we experimentally discover \PairDiff from the bilinear relation composition operator on several benchmark analogy datasets.","Representing the semantic relation that exist between two give word ( or entity ) be an important first step in a wide-range of NLP application such a analogical reasoning , knowledge base completion and relational information retrieval . A simple , yet surprisingly accurate method for represent a relation between two word be to compute the vector offset ( \PairDiff ) between their corresponding word embeddings . Despite the empirical success , it remain unclear a to whether \PairDiff be the best operator for obtain a relational representation from word embeddings . We conduct a theoretical analysis of generalised bilinear operator that can be use to measure the $ \ell_ { 2 } $ relational distance between two word-pairs . We show that , if the word embeddings be standardise and uncorrelated , such an operator will be independent of bilinear term , and can be simplify to a linear form , where \PairDiff be a special case . For numerous word embed type , we empirically verify the uncorrelation assumption , demonstrate the general applicability of our theoretical result . Moreover , we experimentally discover \PairDiff from the bilinear relation composition operator on several benchmark analogy datasets . ","http://arxiv.org/pdf/1709.06673v2","cs.CL	cs.AI	cs.LG	cs.NE","Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational   Compositional Operators for Analogy Detection","H.A.Hakami@liverpool.ac.uk	danushka@liverpool.ac.uk	hayashi.kohei@gmail.com"
"Zhengdong Lu	Haotian Cui	Xianggen Liu	Yukun Yan	Daqi Zheng","26","9","2017","We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure (referred to as ontology in this paper) that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and during the process it builds and updates an intermediate ontology to summarize its partial understanding of the text it covers. OONP supports a rich family of operations (both symbolic and differentiable) for composing the ontology, and a big variety of forms (both symbolic and differentiable) for representing the state and the document. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL) , reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.","We propose Object-oriented Neural Programming ( OONP ) , a framework for semantically parse document in specific domain . Basically , OONP read a document and parse it into a predesigned object-oriented data structure ( refer to a ontology in this paper ) that reflect the domain-specific semantics of the document . An OONP parser model semantic parsing a a decision process : a neural net-based Reader sequentially go through the document , and during the process it build and update an intermediate ontology to summarize it partial understanding of the text it cover . OONP support a rich family of operation ( both symbolic and differentiable ) for compose the ontology , and a big variety of form ( both symbolic and differentiable ) for represent the state and the document . An OONP parser can be train with supervision of different form and strength , include supervise learning ( SL ) , reinforcement learning ( RL ) and hybrid of the two . Our experiment on both synthetic and real-world document parse task have show that OONP can learn to handle fairly complicate ontology with train data of modest size . ","http://arxiv.org/pdf/1709.08853v4","cs.LG	cs.AI	cs.CL	cs.NE","Object-oriented Neural Programming (OONP) for Document Understanding","luz@deeplycurious.ai	da@deeplycurious.ai	cht15@mails.tsinghua.edu.cn	liuxg16@mails.tsinghua.edu.cn	yanyk13@mails.tsinghua.edu.cn"
"Bin Bi	Hao Ma","29","9","2017","This paper proposes a novel neural machine reading model for open-domain question answering at scale. Existing machine comprehension models typically assume that a short piece of relevant text containing answers is already identified and given to the models, from which the models are designed to extract answers. This assumption, however, is not realistic for building a large-scale open-domain question answering system which requires both deep text understanding and identifying relevant text from corpus simultaneously.   In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates both passage ranking and answer extraction in one single framework. A Q&A system based on this framework allows users to issue an open-domain question without needing to provide a piece of text that must contain the answer. Experiments show that the unified NCR model is able to outperform the states-of-the-art in both retrieval of relevant text and answer extraction.","This paper propose a novel neural machine reading model for open-domain question answer at scale . Existing machine comprehension model typically assume that a short piece of relevant text contain answer be already identify and give to the model , from which the model be design to extract answer . This assumption , however , be not realistic for build a large-scale open-domain question answer system which require both deep text understanding and identify relevant text from corpus simultaneously . In this paper , we introduce Neural Comprehensive Ranker ( NCR ) that integrate both passage ranking and answer extraction in one single framework . A Q & A system base on this framework allow user to issue an open-domain question without need to provide a piece of text that must contain the answer . Experiments show that the unified NCR model be able to outperform the states-of-the-art in both retrieval of relevant text and answer extraction . ","http://arxiv.org/pdf/1709.10204v2","cs.CL	cs.AI	cs.LG	cs.NE","A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering",""
"Mirco Ravanelli	Philemon Brakel	Maurizio Omologo	Yoshua Bengio","29","9","2017","Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates.   This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU.","Speech recognition be largely taking advantage of deep learning , show that substantial benefit can be obtain by modern Recurrent Neural Networks ( RNNs ) . The most popular RNNs be Long Short-Term Memory ( LSTMs ) , which typically reach state-of-the-art performance in many task thanks to their ability to learn long-term dependency and robustness to vanish gradient . Nevertheless , LSTMs have a rather complex design with three multiplicative gate , that might impair their efficient implementation . An attempt to simplify LSTMs have recently lead to Gated Recurrent Units ( GRUs ) , which be base on just two multiplicative gate . This paper build on these effort by further revise GRUs and propose a simplified architecture potentially more suitable for speech recognition . The contribution of this work be two-fold . First , we suggest to remove the reset gate in the GRU design , result in a more efficient single-gate architecture . Second , we propose to replace tanh with ReLU activation in the state update equation . Results show that , in our implementation , the revised architecture reduce the per-epoch training time with more than 30 % and consistently improve recognition performance across different task , input feature , and noisy condition when compare to a standard GRU . ","http://arxiv.org/pdf/1710.00641v1","cs.CL	cs.AI	cs.LG	cs.NE","Improving speech recognition by revising gated recurrent units","mravanelli@fbk.eu,	pbpop3@gmail.com,	omologo@fbk.eu,	yoshua.umontreal@gmail.com"
"Baolin Peng	Xiujun Li	Jianfeng Gao	Jingjing Liu	Kam-Fai Wong","18","1","2018","Training a task-completion dialogue agent with real users via reinforcement learning (RL) could be prohibitively expensive, because it requires many interactions with users. One alternative is to resort to a user simulator, while the discrepancy of between simulated and real users makes the learned policy unreliable in practice. This paper addresses these challenges by integrating planning into the dialogue policy learning based on Dyna-Q framework, and provides a more sample-efficient approach to learn the dialogue polices. The proposed agent consists of a planner trained on-line with limited real user experience that can generate large amounts of simulated experience to supplement with limited real user experience, and a policy model trained on these hybrid experiences. The effectiveness of our approach is validated on a movie-booking task in both a simulation setting and a human-in-the-loop setting.","Training a task-completion dialogue agent with real user via reinforcement learning ( RL ) could be prohibitively expensive , because it require many interaction with user . One alternative be to resort to a user simulator , while the discrepancy of between simulate and real user make the learned policy unreliable in practice . This paper address these challenge by integrate plan into the dialogue policy learn base on Dyna-Q framework , and provide a more sample-efficient approach to learn the dialogue police . The propose agent consist of a planner train on-line with limited real user experience that can generate large amount of simulated experience to supplement with limited real user experience , and a policy model train on these hybrid experience . The effectiveness of our approach be validate on a movie-booking task in both a simulation setting and a human-in-the-loop setting . ","http://arxiv.org/pdf/1801.06176v1","cs.CL	cs.AI	cs.LG	cs.NE","Integrating planning for task-completion dialogue policy learning","blpeng@se.cuhk.edu.hk	kfwong@se.cuhk.edu.hk	xiul@microsoft.com	jfgao@microsoft.com	jingjl@microsoft.com"
"Andrew L. Maas	Peng Qi	Ziang Xie	Awni Y. Hannun	Christopher T. Lengerich	Daniel Jurafsky	Andrew Y. Ng","30","6","2014","Deep neural networks (DNNs) are now a central component of nearly all state-of-the-art speech recognition systems. Building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance. We report DNN classifier performance and final speech recognizer word error rates, and compare DNNs using several metrics to quantify factors influencing differences in task performance. Our first set of experiments use the standard Switchboard benchmark corpus, which contains approximately 300 hours of conversational telephone speech. We compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. We additionally build systems on a corpus of 2,100 hours of training data by combining the Switchboard and Fisher corpora. This larger corpus allows us to more thoroughly examine performance of large DNN models -- with up to ten times more parameters than those typically used in speech recognition systems. Our results suggest that a relatively simple DNN architecture and optimization technique produces strong results. These findings, along with previous work, help establish a set of best practices for building DNN hybrid speech recognition systems with maximum likelihood training. Our experiments in DNN optimization additionally serve as a case study for training DNNs with discriminative loss functions for speech tasks, as well as DNN classifiers more generally.","Deep neural network ( DNNs ) be now a central component of nearly all state-of-the-art speech recognition system . Building neural network acoustic model require several design decision include network architecture , size , and train loss function . This paper offer an empirical investigation on which aspect of DNN acoustic model design be most important for speech recognition system performance . We report DNN classifier performance and final speech recognizer word error rate , and compare DNNs use several metric to quantify factor influence difference in task performance . Our first set of experiment use the standard Switchboard benchmark corpus , which contain approximately 300 hour of conversational telephone speech . We compare standard DNNs to convolutional network , and present the first experiment use locally-connected , untied neural network for acoustic modeling . We additionally build system on a corpus of 2,100 hour of train data by combine the Switchboard and Fisher corpus . This large corpus allow u to more thoroughly examine performance of large DNN model -- with up to ten time more parameter than those typically use in speech recognition system . Our result suggest that a relatively simple DNN architecture and optimization technique produce strong result . These finding , along with previous work , help establish a set of best practice for building DNN hybrid speech recognition system with maximum likelihood training . Our experiment in DNN optimization additionally serve a a case study for train DNNs with discriminative loss function for speech task , as well a DNN classifier more generally . ","http://arxiv.org/pdf/1406.7806v2","cs.CL	cs.LG	cs.NE	stat.ML","Building DNN Acoustic Models for Large Vocabulary Speech Recognition",""
"William Chan	Ian Lane","7","4","2015","We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models.","We present a novel deep Recurrent Neural Network ( RNN ) model for acoustic modelling in Automatic Speech Recognition ( ASR ) . We term our contribution a a TC-DNN-BLSTM-DNN model , the model combine a Deep Neural Network ( DNN ) with Time Convolution ( TC ) , follow by a Bidirectional Long Short-Term Memory ( BLSTM ) , and a final DNN . The first DNN act a a feature processor to our model , the BLSTM then generate a context from the sequence acoustic signal , and the final DNN take the context and model the posterior probability of the acoustic state . We achieve a 3.47 WER on the Wall Street Journal ( WSJ ) eval92 task or more than 8 % relative improvement over the baseline DNN model . ","http://arxiv.org/pdf/1504.01482v1","cs.LG	cs.CL	cs.NE	stat.ML","Deep Recurrent Neural Networks for Acoustic Modelling","williamchan@cmu.edu,	lane@cmu.edu"
"David Krueger	Roland Memisevic","26","11","2015","We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms.   This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout.   We achieve competitive performance (18.6\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer.   With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance.   Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.","We stabilize the activation of Recurrent Neural Networks ( RNNs ) by penalize the square distance between successive hidden state ' norm . This penalty term be an effective regularizer for RNNs include LSTMs and IRNNs , improve performance on character-level language modeling and phoneme recognition , and outperform weight noise and dropout . We achieve competitive performance ( 18.6\ % PER ) on the TIMIT phoneme recognition task for RNNs evaluate without beam search or an RNN transducer . With this penalty term , IRNN can achieve similar performance to LSTM on language modeling , although add the penalty term to the LSTM result in superior performance . Our penalty term also prevent the exponential growth of IRNN 's activation outside of their training horizon , allow them to generalize to much long sequence . ","http://arxiv.org/pdf/1511.08400v7","cs.NE	cs.CL	cs.LG	stat.ML","Regularizing RNNs by Stabilizing Activations",""
"Noam Shazeer	Azalia Mirhoseini	Krzysztof Maziarz	Andy Davis	Quoc Le	Geoffrey Hinton	Jeff Dean","23","1","2017","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.","The capacity of a neural network to absorb information be limit by it number of parameter . Conditional computation , where part of the network be active on a per-example basis , have be propose in theory a a way of dramatically increase model capacity without a proportional increase in computation . In practice , however , there be significant algorithmic and performance challenge . In this work , we address these challenge and finally realize the promise of conditional computation , achieve great than 1000x improvement in model capacity with only minor loss in computational efficiency on modern GPU cluster . We introduce a Sparsely-Gated Mixture-of-Experts layer ( MoE ) , consist of up to thousand of feed-forward sub-networks . A trainable gating network determine a sparse combination of these expert to use for each example . We apply the MoE to the task of language modeling and machine translation , where model capacity be critical for absorb the vast quantity of knowledge available in the training corpus . We present model architecture in which a MoE with up to 137 billion parameter be apply convolutionally between stack LSTM layer . On large language modeling and machine translation benchmark , these model achieve significantly good result than state-of-the-art at low computational cost . ","http://arxiv.org/pdf/1701.06538v1","cs.LG	cs.CL	cs.NE	stat.ML","Outrageously Large Neural Networks: The Sparsely-Gated   Mixture-of-Experts Layer","noam@google.com	azalia@google.com	andydavis@google.com	qvl@google.com	geoffhinton@google.com	jeff@google.com	krzysztof.maziarz@student.uj.edu.pl"
"Yacine Jernite	Samuel R. Bowman	David Sontag","23","4","2017","This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.","This work present a novel objective function for the unsupervised training of neural network sentence encoders . It exploit signal from paragraph-level discourse coherence to train these model to understand text . Our objective be purely discriminative , allow u to train model many time faster than be possible under prior method , and it yield model which perform well in extrinsic evaluation . ","http://arxiv.org/pdf/1705.00557v1","cs.CL	cs.LG	cs.NE	stat.ML","Discourse-Based Objectives for Fast Unsupervised Sentence Representation   Learning","yacine.jernite@nyu.edu	bowman@nyu.edu	dsontag@mit.edu"
"Zhengyang Wang	Shuiwang Ji","18","5","2017","Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized for text data, such as width and depth, we present our "CNN Inception + Gate" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks like text classification, are not suitable in visual question answering.","Visual question answering be a recently propose artificial intelligence task that require a deep understanding of both image and text . In deep learning , image be typically model through convolutional neural network , and text be typically model through recurrent neural network . While the requirement for model image be similar to traditional computer vision task , such a object recognition and image classification , visual question answer raise a different need for textual representation a compare to other natural language processing task . In this work , we perform a detailed analysis on natural language question in visual question answer . Based on the analysis , we propose to rely on convolutional neural network for learn textual representation . By explore the various property of convolutional neural network specialize for text data , such a width and depth , we present our `` CNN Inception + Gate '' model . We show that our model improve question representation and thus the overall accuracy of visual question answer model . We also show that the text representation requirement in visual question answering be more complicated and comprehensive than that in conventional natural language processing task , make it a well task to evaluate textual representation method . Shallow model like fastText , which can obtain comparable result with deep learning model in task like text classification , be not suitable in visual question answering . ","http://arxiv.org/pdf/1705.06824v1","cs.LG	cs.CL	cs.NE	stat.ML","Learning Convolutional Text Representations for Visual Question   Answering","zwang6@eecs.wsu.edu	sji@eecs.wsu.edu"
"Kyunghyun Cho	Bart van Merrienboer	Caglar Gulcehre	Dzmitry Bahdanau	Fethi Bougares	Holger Schwenk	Yoshua Bengio","3","6","2014","In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","In this paper , we propose a novel neural network model call RNN Encoder-Decoder that consist of two recurrent neural network ( RNN ) . One RNN encode a sequence of symbol into a fixed-length vector representation , and the other decode the representation into another sequence of symbol . The encoder and decoder of the propose model be jointly train to maximize the conditional probability of a target sequence give a source sequence . The performance of a statistical machine translation system be empirically find to improve by use the conditional probability of phrase pair compute by the RNN Encoder-Decoder a an additional feature in the exist log-linear model . Qualitatively , we show that the propose model learn a semantically and syntactically meaningful representation of linguistic phrase . ","http://arxiv.org/pdf/1406.1078v3","cs.CL	cs.LG	cs.NE	stat.ML","Learning Phrase Representations using RNN Encoder-Decoder for   Statistical Machine Translation","firstname.lastname@umontreal.ca	d.bahdanau@jacobs-university.de	firstname.lastname@lium.univ-lemans.fr	find.me@on.the.web"
"Zhiyuan Tang	Dong Wang	Zhiyong Zhang","18","5","2015","Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervision. This knowledge transfer learning has been employed to train simple neural nets with a complex one, so that the final performance can reach a level that is infeasible to obtain by regular training. In this paper, we employ the knowledge transfer learning approach to train RNNs (precisely LSTM) using a deep neural network (DNN) model as the teacher. This is different from most of the existing research on knowledge transfer learning, since the teacher (DNN) is assumed to be weaker than the child (RNN); however, our experiments on an ASR task showed that it works fairly well: without applying any tricks on the learning scheme, this approach can train RNNs successfully even with limited training data.","Recurrent neural network ( RNNs ) , particularly long short-term memory ( LSTM ) , have gain much attention in automatic speech recognition ( ASR ) . Although some successful story have be report , train RNNs remain highly challenging , especially with limited training data . Recent research find that a well-trained model can be use a a teacher to train other child model , by use the prediction generate by the teacher model a supervision . This knowledge transfer learning have be employ to train simple neural net with a complex one , so that the final performance can reach a level that be infeasible to obtain by regular training . In this paper , we employ the knowledge transfer learn approach to train RNNs ( precisely LSTM ) use a deep neural network ( DNN ) model a the teacher . This be different from most of the exist research on knowledge transfer learning , since the teacher ( DNN ) be assume to be weak than the child ( RNN ) ; however , our experiment on an ASR task show that it work fairly well : without apply any trick on the learning scheme , this approach can train RNNs successfully even with limited training data . ","http://arxiv.org/pdf/1505.04630v5","stat.ML	cs.CL	cs.LG	cs.NE","Recurrent Neural Network Training with Dark Knowledge Transfer","tangzy@cslt.riit.tsinghua.edu.cn	zhangzy@cslt.riit.tsinghua.edu.cn	Author:wangdong99@mails.tsinghua.edu.cn"
"HaÅim Sak	Andrew Senior	FranÃ§oise Beaufays","5","2","2014","Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.","Long Short-Term Memory ( LSTM ) be a recurrent neural network ( RNN ) architecture that have be design to address the vanishing and explode gradient problem of conventional RNNs . Unlike feedforward neural network , RNNs have cyclic connection make them powerful for model sequence . They have be successfully use for sequence labeling and sequence prediction task , such a handwrite recognition , language modeling , phonetic labeling of acoustic frame . However , in contrast to the deep neural network , the use of RNNs in speech recognition have be limit to phone recognition in small scale task . In this paper , we present novel LSTM base RNN architecture which make more effective use of model parameter to train acoustic model for large vocabulary speech recognition . We train and compare LSTM , RNN and DNN model at various number of parameter and configuration . We show that LSTM model converge quickly and give state of the art speech recognition performance for relatively small sized model . ","http://arxiv.org/pdf/1402.1128v1","cs.NE	cs.CL	cs.LG	stat.ML","Long Short-Term Memory Based Recurrent Neural Network Architectures for   Large Vocabulary Speech Recognition","hasim@google.com}	andrewsenior@google.com}	fsb@google.com@google.com}"
"Peter Wittek	SÃ¡ndor DarÃ¡nyi	Efstratios Kontopoulos	Theodoros Moysiadis	Ioannis Kompatsiaris","5","2","2015","Based on the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Falling back on the distributional hypothesis to statistically model word meaning, we used evolving fields as a metaphor to express time-dependent changes in a vector space model by a combination of random indexing and evolving self-organizing maps (ESOM). To monitor semantic drifts within the observation period, an experiment was carried out on the term space of a collection of 12.8 million Amazon book reviews. For evaluation, the semantic consistency of ESOM term clusters was compared with their respective neighbourhoods in WordNet, and contrasted with distances among term vectors by random indexing. We found that at 0.05 level of significance, the terms in the clusters showed a high level of semantic consistency. Tracking the drift of distributional patterns in the term space across time periods, we found that consistency decreased, but not at a statistically significant level. Our method is highly scalable, with interpretations in philosophy.","Based on the Aristotelian concept of potentiality vs. actuality allow for the study of energy and dynamic in language , we propose a field approach to lexical analysis . Falling back on the distributional hypothesis to statistically model word meaning , we use evolve field a a metaphor to express time-dependent change in a vector space model by a combination of random indexing and evolve self-organizing map ( ESOM ) . To monitor semantic drift within the observation period , an experiment be carry out on the term space of a collection of 12.8 million Amazon book review . For evaluation , the semantic consistency of ESOM term cluster be compare with their respective neighbourhood in WordNet , and contrast with distance among term vector by random indexing . We find that at 0.05 level of significance , the term in the cluster show a high level of semantic consistency . Tracking the drift of distributional pattern in the term space across time period , we find that consistency decrease , but not at a statistically significant level . Our method be highly scalable , with interpretation in philosophy . ","http://arxiv.org/pdf/1502.01753v1","cs.CL	cs.LG	cs.NE	stat.ML","Monitoring Term Drift Based on Semantic Consistency in an Evolving   Vector Field",""
"Jan Chorowski	Navdeep Jaitly","8","12","2016","The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in its predictions and a tendency to produce incomplete transcriptions when language models are used. We propose practical solutions to both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate language models we reach 10.6% WER, while together with a trigram language model, we reach 6.7% WER.","The recently propose Sequence-to-Sequence ( seq2seq ) framework advocate replace complex data processing pipeline , such a an entire automatic speech recognition system , with a single neural network train in an end-to-end fashion . In this contribution , we analyse an attention-based seq2seq speech recognition system that directly transcribe recording into character . We observe two shortcoming : overconfidence in it prediction and a tendency to produce incomplete transcription when language model be use . We propose practical solution to both problem achieve competitive speaker independent word error rate on the Wall Street Journal dataset : without separate language model we reach 10.6 % WER , while together with a trigram language model , we reach 6.7 % WER . ","http://arxiv.org/pdf/1612.02695v1","cs.NE	cs.CL	cs.LG	stat.ML","Towards better decoding and language model integration in sequence to   sequence models","jan.chorowski@cs.uni.wroc.pl,ndjaitly@google.com"
"Dzmitry Bahdanau	Kyunghyun Cho	Yoshua Bengio","1","9","2014","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.","Neural machine translation be a recently propose approach to machine translation . Unlike the traditional statistical machine translation , the neural machine translation aim at build a single neural network that can be jointly tune to maximize the translation performance . The model propose recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encode a source sentence into a fixed-length vector from which a decoder generate a translation . In this paper , we conjecture that the use of a fixed-length vector be a bottleneck in improve the performance of this basic encoder-decoder architecture , and propose to extend this by allow a model to automatically ( soft- ) search for part of a source sentence that be relevant to predict a target word , without have to form these part a a hard segment explicitly . With this new approach , we achieve a translation performance comparable to the exist state-of-the-art phrase-based system on the task of English-to-French translation . Furthermore , qualitative analysis reveals that the ( soft- ) alignment find by the model agree well with our intuition . ","http://arxiv.org/pdf/1409.0473v7","cs.CL	cs.LG	cs.NE	stat.ML","Neural Machine Translation by Jointly Learning to Align and Translate",""
"Jean Pouget-Abadie	Dzmitry Bahdanau	Bart van Merrienboer	Kyunghyun Cho	Yoshua Bengio","3","9","2014","The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model. Once each segment has been independently translated by the neural machine translation model, the translated clauses are concatenated to form a final translation. Empirical results show a significant improvement in translation quality for long sentences.","The author of ( Cho et al. , 2014a ) have show that the recently introduce neural network translation system suffer from a significant drop in translation quality when translate long sentence , unlike exist phrase-based translation system . In this paper , we propose a way to address this issue by automatically segment an input sentence into phrase that can be easily translate by the neural network translation model . Once each segment have be independently translate by the neural machine translation model , the translated clause be concatenate to form a final translation . Empirical result show a significant improvement in translation quality for long sentence . ","http://arxiv.org/pdf/1409.1257v2","cs.CL	cs.LG	cs.NE	stat.ML","Overcoming the Curse of Sentence Length for Neural Machine Translation   using Automatic Segmentation",""
"William Chan	Nan Rosemary Ke	Ian Lane","7","4","2015","Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to train a small DNN that learns the output distribution of a large DNN. In this paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We use the RNN model to generate soft alignments and minimize the Kullback-Leibler divergence against the small DNN. The small DNN trained on the soft RNN alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task compared to a baseline 4.54 WER or more than 13% relative improvement.","Deep Neural Network ( DNN ) acoustic model have yield many state-of-the-art result in Automatic Speech Recognition ( ASR ) task . More recently , Recurrent Neural Network ( RNN ) model have be show to outperform DNNs counterpart . However , state-of-the-art DNN and RNN model tend to be impractical to deploy on embedded system with limited computational capacity . Traditionally , the approach for embedded platform be to either train a small DNN directly , or to train a small DNN that learn the output distribution of a large DNN . In this paper , we utilize a state-of-the-art RNN to transfer knowledge to small DNN . We use the RNN model to generate soft alignment and minimize the Kullback-Leibler divergence against the small DNN . The small DNN train on the soft RNN alignment achieve a 3.93 WER on the Wall Street Journal ( WSJ ) eval92 task compare to a baseline 4.54 WER or more than 13 % relative improvement . ","http://arxiv.org/pdf/1504.01483v1","cs.LG	cs.CL	cs.NE	stat.ML","Transferring Knowledge from a RNN to a DNN","williamchan@cmu.edu,	rosemary.ke@sv.cmu.edu,	lane@cmu.edu"
"Sarath Chandar	Mitesh M. Khapra	Hugo Larochelle	Balaraman Ravindran","27","4","2015","Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.","Common Representation Learning ( CRL ) , wherein different description ( or view ) of the data be embed in a common subspace , be receive a lot of attention recently . Two popular paradigm here be Canonical Correlation Analysis ( CCA ) base approach and Autoencoder ( AE ) base approach . CCA base approach learn a joint representation by maximize correlation of the view when project to the common subspace . AE base method learn a common representation by minimize the error of reconstruct the two view . Each of these approach have it own advantage and disadvantage . For example , while CCA base approach outperform AE base approach for the task of transfer learning , they be not as scalable a the latter . In this work we propose an AE base approach call Correlational Neural Network ( CorrNet ) , that explicitly maximize correlation among the view when project to the common subspace . Through a series of experiment , we demonstrate that the propose CorrNet be well than the above mention approach with respect to it ability to learn correlated common representation . Further , we employ CorrNet for several cross language task and show that the representation learn use CorrNet perform well than the one learn use other state of the art approach . ","http://arxiv.org/pdf/1504.07225v3","cs.CL	cs.LG	cs.NE	stat.ML","Correlational Neural Networks","apsarathchandar@gmail.com	mikhapra@in.ibm.com	hugo.larochelle@usherbrooke.ca	ravi@cse.iitm.ac.in"
"Jan Chorowski	Dzmitry Bahdanau	Dmitriy Serdyuk	Kyunghyun Cho	Yoshua Bengio","24","6","2015","Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the at- tention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.","Recurrent sequence generator condition on input data through an attention mechanism have recently show very good performance on a range of task in- cluding machine translation , handwrite synthesis and image caption gen- eration . We extend the attention-mechanism with feature need for speech recognition . We show that while an adaptation of the model use for machine translation in reach a competitive 18.7 % phoneme error rate ( PER ) on the TIMIT phoneme recognition task , it can only be apply to utterance which be roughly as long a the one it be train on . We offer a qualitative explanation of this failure and propose a novel and generic method of add location-awareness to the attention mechanism to alleviate this issue . The new method yield a model that be robust to long input and achieve 18 % PER in single utterance and 20 % in 10-times longer ( repeat ) utterance . Finally , we propose a change to the at- tention mechanism that prevent it from concentrate too much on single frame , which far reduce PER to 17.6 % level . ","http://arxiv.org/pdf/1506.07503v1","cs.CL	cs.LG	cs.NE	stat.ML","Attention-Based Models for Speech Recognition","jan.chorowski@ii.uni.wroc.pl"
"HaÅim Sak	Andrew Senior	Kanishka Rao	FranÃ§oise Beaufays","24","7","2015","We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.","We have recently show that deep Long Short-Term Memory ( LSTM ) recurrent neural network ( RNNs ) outperform fee forward deep neural network ( DNNs ) a acoustic model for speech recognition . More recently , we have show that the performance of sequence train context dependent ( CD ) hide Markov model ( HMM ) acoustic model use such LSTM RNNs can be equal by sequence train phone model initialize with connectionist temporal classification ( CTC ) . In this paper , we present technique that further improve performance of LSTM RNN acoustic model for large vocabulary speech recognition . We show that frame stacking and reduced frame rate lead to more accurate model and faster decode . CD phone model lead to further improvement . We also present initial result for LSTM RNN model output word directly . ","http://arxiv.org/pdf/1507.06947v1","cs.CL	cs.LG	cs.NE	stat.ML","Fast and Accurate Recurrent Neural Network Acoustic Models for Speech   Recognition","hasim@google.com	andrewsenior@google.com	kanishkarao@google.com	fsb@google.com"
"William Chan	Navdeep Jaitly	Quoc V. Le	Oriol Vinyals","5","8","2015","We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%.","We present Listen , Attend and Spell ( LAS ) , a neural network that learn to transcribe speech utterance to character . Unlike traditional DNN-HMM model , this model learn all the component of a speech recognizer jointly . Our system have two component : a listener and a speller . The listener be a pyramidal recurrent network encoder that accept filter bank spectrum a input . The speller be an attention-based recurrent network decoder that emit character a output . The network produce character sequence without make any independence assumption between the character . This be the key improvement of LAS over previous end-to-end CTC model . On a subset of the Google voice search task , LAS achieve a word error rate ( WER ) of 14.1 % without a dictionary or a language model , and 10.3 % with language model rescoring over the top 32 beam . By comparison , the state-of-the-art CLDNN-HMM model achieve a WER of 8.0 % . ","http://arxiv.org/pdf/1508.01211v2","cs.CL	cs.LG	cs.NE	stat.ML","Listen, Attend and Spell","williamchan@cmu.edu	ndjaitly@google.com	qvl@google.com	vinyals@google.com"
"Shihao Ji	S. V. N. Vishwanathan	Nadathur Satish	Michael J. Anderson	Pradeep Dubey","21","11","2015","We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers.","We propose BlackOut , an approximation algorithm to efficiently train massive recurrent neural network language model ( RNNLMs ) with million word vocabulary . BlackOut be motivate by use a discriminative loss , and we describe a new sampling strategy which significantly reduce computation while improve stability , sample efficiency , and rate of convergence . One way to understand BlackOut be to view it a an extension of the DropOut strategy to the output layer , wherein we use a discriminative training loss and a weighted sampling scheme . We also establish close connection between BlackOut , importance sampling , and noise contrastive estimation ( NCE ) . Our experiment , on the recently release one billion word language model benchmark , demonstrate scalability and accuracy of BlackOut ; we outperform the state-of-the art , and achieve the low perplexity score on this dataset . Moreover , unlike other establish method which typically require GPUs or CPU cluster , we show that a carefully implement version of BlackOut require only 1-10 day on a single machine to train a RNNLM with a million word vocabulary and billion of parameter on one billion word . Although we describe BlackOut in the context of RNNLM training , it can be use to any network with large softmax output layer . ","http://arxiv.org/pdf/1511.06909v7","cs.LG	cs.CL	cs.NE	stat.ML","BlackOut: Speeding up Recurrent Neural Network Language Models With Very   Large Vocabularies","shihao.ji@intel.com	vishy@ucsc.edu	nadathur.rajagopalan.satish@intel.com	michael.j.anderson@intel.com	pradeep.dubey@intel.com"
"Marta R. Costa-JussÃ 	JosÃ© A. R. Fonollosa","2","3","2016","Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embeddings are tested in a state-of-the-art neural MT based on an attention-based bidirectional recurrent neural network. The proposed MT scheme provides improved results even when the source language is not morphologically rich. Improvements up to 3 BLEU points are obtained in the German-English WMT task.","Neural Machine Translation ( MT ) have reach state-of-the-art result . However , one of the main challenge that neural MT still face be deal with very large vocabulary and morphologically rich language . In this paper , we propose a neural MT system use character-based embeddings in combination with convolutional and highway layer to replace the standard lookup-based word representation . The result unlimited-vocabulary and affix-aware source word embeddings be test in a state-of-the-art neural MT base on an attention-based bidirectional recurrent neural network . The propose MT scheme provide improved result even when the source language be not morphologically rich . Improvements up to 3 BLEU point be obtain in the German-English WMT task . ","http://arxiv.org/pdf/1603.00810v3","cs.CL	cs.LG	cs.NE	stat.ML","Character-based Neural Machine Translation","marta.ruiz@upc.edu	jose.fonollosa@upc.edu"
"Yangfeng Ji	Gholamreza Haffari	Jacob Eisenstein","7","3","2016","This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalized, depending on the task. The resulting model can therefore employ a training objective that includes not only discourse relation classification, but also word prediction. As a result, it outperforms state-of-the-art alternatives for two tasks: implicit discourse relation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline.","This paper present a novel latent variable recurrent neural network architecture for jointly model sequence of word and ( possibly latent ) discourse relation between adjacent sentence . A recurrent neural network generates individual word , thus reap the benefit of discriminatively-trained vector representation . The discourse relation be represent with a latent variable , which can be predict or marginalize , depend on the task . The result model can therefore employ a training objective that include not only discourse relation classification , but also word prediction . As a result , it outperform state-of-the-art alternative for two task : implicit discourse relation classification in the Penn Discourse Treebank , and dialog act classification in the Switchboard corpus . Furthermore , by marginalize over latent discourse relation at test time , we obtain a discourse inform language model , which improve over a strong LSTM baseline . ","http://arxiv.org/pdf/1603.01913v2","cs.CL	cs.LG	cs.NE	stat.ML","A Latent Variable Recurrent Neural Network for Discourse Relation   Language Models","jiyfeng@gatech.edu	jacobe@gatech.edu"
"Zhiyuan Tang	Lantian Li	Dong Wang","31","3","2016","Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to the input of the other, leading to a multi-task recurrent network. Experiments show that the joint model outperforms the task-specific models on both the two tasks.","Although highly correlate , speech and speaker recognition have be regard a two independent task and study by two community . This be certainly not the way that people behave : we decipher both speech content and speaker trait at the same time . This paper present a unified model to perform speech and speaker recognition simultaneously and altogether . The model be base on a unified neural network where the output of one task be feed to the input of the other , lead to a multi-task recurrent network . Experiments show that the joint model outperform the task-specific model on both the two task . ","http://arxiv.org/pdf/1603.09643v4","cs.CL	cs.LG	cs.NE	stat.ML","Multi-task Recurrent Model for Speech and Speaker Recognition","tangzy@cslt.riit.tsinghua.edu.cn	lilt@cslt.riit.tsinghua.edu.cn	wangdong99@mails.tsinghua.edu.cn"
"Sarath Chandar	Sungjin Ahn	Hugo Larochelle	Pascal Vincent	Gerald Tesauro	Yoshua Bengio","24","5","2016","Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are challenging to train successfully. In this paper, we explore a form of hierarchical memory network, which can be considered as a hybrid between hard and soft attention memory networks. The memory is organized in a hierarchical structure such that reading from it is done with less computation than soft attention over a flat memory, while also being easier to train than hard attention over a flat memory. Specifically, we propose to incorporate Maximum Inner Product Search (MIPS) in the training and inference procedures for our hierarchical memory network. We explore the use of various state-of-the art approximate MIPS techniques and report results on SimpleQuestions, a challenging large scale factoid question answering task.","Memory network be neural network with an explicit memory component that can be both read and write to by the network . The memory be often address in a soft way use a softmax function , make end-to-end training with backpropagation possible . However , this be not computationally scalable for application which require the network to read from extremely large memory . On the other hand , it be well know that hard attention mechanism base on reinforcement learning be challenge to train successfully . In this paper , we explore a form of hierarchical memory network , which can be consider a a hybrid between hard and soft attention memory network . The memory be organize in a hierarchical structure such that read from it be do with less computation than soft attention over a flat memory , while also be easy to train than hard attention over a flat memory . Specifically , we propose to incorporate Maximum Inner Product Search ( MIPS ) in the training and inference procedure for our hierarchical memory network . We explore the use of various state-of-the art approximate MIPS technique and report result on SimpleQuestions , a challenge large scale factoid question answer task . ","http://arxiv.org/pdf/1605.07427v1","stat.ML	cs.CL	cs.LG	cs.NE","Hierarchical Memory Networks",""
"Sam Wiseman	Alexander M. Rush","9","6","2016","Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.","Sequence-to-Sequence ( seq2seq ) modeling have rapidly become an important general-purpose NLP tool that have prove effective for many text-generation and sequence-labeling task . Seq2seq build on deep neural language modeling and inherit it remarkable accuracy in estimate local , next-word distribution . In this work , we introduce a model and beam-search training scheme , base on the work of Daume III and Marcu ( 2005 ) , that extend seq2seq to learn global sequence score . This structured approach avoids classical bias associate with local training and unify the training loss with the test-time usage , while preserve the proven model architecture of seq2seq and it efficient training approach . We show that our system outperform a highly-optimized attention-based seq2seq system and other baseline on three different sequence to sequence task : word ordering , parsing , and machine translation . ","http://arxiv.org/pdf/1606.02960v2","cs.CL	cs.LG	cs.NE	stat.ML","Sequence-to-Sequence Learning as Beam-Search Optimization","swiseman@seas.harvard.edu	srush@seas.harvard.edu"
"Ankit Vani	Yacine Jernite	David Sontag","23","5","2017","In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process "grounding"). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.","In this work , we present the Grounded Recurrent Neural Network ( GRNN ) , a recurrent neural network architecture for multi-label prediction which explicitly tie label to specific dimension of the recurrent hidden state ( we call this process `` grounding '' ) . The approach be particularly well-suited for extract large number of concept from text . We apply the new model to address an important problem in healthcare of understand what medical concept be discuss in clinical text . Using a publicly available dataset derive from Intensive Care Units , we learn to label a patient 's diagnosis and procedure from their discharge summary . Our evaluation show a clear advantage to use our propose architecture over a variety of strong baseline . ","http://arxiv.org/pdf/1705.08557v1","stat.ML	cs.CL	cs.LG	cs.NE","Grounded Recurrent Neural Networks","ankit.vani@nyu.edu,	jernite@cs.nyu.edu,	dsontag@csail.mit.edu"
"Tsung-Hsien Wen	Yishu Miao	Phil Blunsom	Steve Young","29","5","2017","Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.","Developing a dialogue agent that be capable of make autonomous decision and communicating by natural language be one of the long-term goal of machine learn research . Traditional approach either rely on hand-crafting a small state-action set for apply reinforcement learning that be not scalable or construct deterministic model for learn dialogue sentence that fail to capture natural conversational variability . In this paper , we propose a Latent Intention Dialogue Model ( LIDM ) that employ a discrete latent variable to learn underlying dialogue intention in the framework of neural variational inference . In a goal-oriented dialogue scenario , these latent intention can be interpret a action guide the generation of machine response , which can be further refine autonomously by reinforcement learning . The experimental evaluation of LIDM show that the model out-performs publish benchmark for both corpus-based and human evaluation , demonstrate the effectiveness of discrete latent variable model for learn goal-oriented dialogue . ","http://arxiv.org/pdf/1705.10229v1","cs.CL	cs.LG	cs.NE	stat.ML","Latent Intention Dialogue Models",""
"Julius Kunze	Louis Kirsch	Ilia Kurenkov	Andreas Krug	Jens Johannsmeier	Sebastian Stober","1","6","2017","End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.","End-to-end training of automated speech recognition ( ASR ) system require massive data and compute resource . We explore transfer learn base on model adaptation a an approach for train ASR model under constrain GPU memory , throughput and training data . We conduct several systematic experiment adapt a Wav2Letter convolutional neural network originally train for English ASR to the German language . We show that this technique allow faster train on consumer-grade resource while require less train data in order to achieve the same accuracy , thereby lower the cost of train ASR model in other language . Model introspection reveal that small adaptation to the network 's weight be sufficient for good performance , especially for inner layer . ","http://arxiv.org/pdf/1706.00290v1","cs.LG	cs.CL	cs.NE	stat.ML","Transfer Learning for Speech Recognition on a Budget","juliuskunze@gmail.com,	mail@louiskirsch.com	kurenkov@uni-potsdam.de	ankrug@uni-potsdam.de	johannsmeier@uni-potsdam.de	sstober@uni-potsdam.de"
"Matt Shannon","8","6","2017","State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the reference and hypothesized acoustic states differ. It may be preferable to optimize the expected WER, but WER does not interact well with the expectation semiring, and previous approaches based on computing expected WER exactly involve expanding the lattices used during training. In this paper we show how to perform optimization of the expected WER by sampling paths from the lattices used during conventional sMBR training. The gradient of the expected WER is itself an expectation, and so may be approximated using Monte Carlo sampling. We show experimentally that optimizing WER during acoustic model training gives 5% relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task (Google Home).","State-level minimum Bayes risk ( sMBR ) training have become the de facto standard for sequence-level training of speech recognition acoustic model . It have an elegant formulation use the expectation semiring , and give large improvement in word error rate ( WER ) over model train solely use cross-entropy ( CE ) or connectionist temporal classification ( CTC ) . sMBR training optimize the expected number of frame at which the reference and hypothesize acoustic state differ . It may be preferable to optimize the expected WER , but WER do not interact well with the expectation semiring , and previous approach base on compute expect WER exactly involve expand the lattice use during training . In this paper we show how to perform optimization of the expect WER by sample path from the lattice use during conventional sMBR training . The gradient of the expected WER be itself an expectation , and so may be approximate use Monte Carlo sample . We show experimentally that optimize WER during acoustic model train give 5 % relative improvement in WER over a well-tuned sMBR baseline on a 2-channel query recognition task ( Google Home ) . ","http://arxiv.org/pdf/1706.02776v1","cs.CL	cs.LG	cs.NE	stat.ML","Optimizing expected word error rate via sampling for speech recognition","mattshannon@google.com"
"Artem M. Grachev	Dmitry I. Ignatov	Andrey V. Savchenko","20","8","2017","In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treebank (PTB) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for LSTM networks in terms of model size and suitability for fast inference.","In this paper , we consider several compression technique for the language model problem base on recurrent neural network ( RNNs ) . It be know that conventional RNNs , e.g , LSTM-based network in language modeling , be characterize with either high space complexity or substantial inference time . This problem be especially crucial for mobile application , in which the constant interaction with the remote server be inappropriate . By use the Penn Treebank ( PTB ) dataset we compare prune , quantization , low-rank factorization , tensor train decomposition for LSTM network in term of model size and suitability for fast inference . ","http://arxiv.org/pdf/1708.05963v1","stat.ML	cs.CL	cs.LG	cs.NE	62M45, 68T50	I.2.7, I.2.6, I.5.1, I.5.4","Neural Networks Compression for Language Modeling","grachev.art@gmail.com"
"Mostafa Dehghani	Aliaksei Severyn	Sascha Rothe	Jaap Kamps","1","11","2017","Training deep neural networks requires massive amounts of training data, but for many tasks only limited labeled data is available. This makes weak supervision attractive, using weak or noisy signals like the output of heuristic methods or user click-through data for training. In a semi-supervised setting, we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the parameters with a small amount of data with true labels. This feels intuitively sub-optimal as these two independent stages leave the model unaware about the varying label quality. What if we could somehow inform the model about the label quality? In this paper, we propose a semi-supervised learning method where we train two neural networks in a multi-task fashion: a "target network" and a "confidence network". The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to weight the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model. We evaluate our learning strategy on two different tasks: document ranking and sentiment classification. The results demonstrate that our approach not only enhances the performance compared to the baselines but also speeds up the learning process from weak labels.","Training deep neural network require massive amount of training data , but for many task only limit label data be available . This make weak supervision attractive , use weak or noisy signal like the output of heuristic method or user click-through data for training . In a semi-supervised setting , we can use a large set of data with weak label to pretrain a neural network and then fine-tune the parameter with a small amount of data with true label . This feel intuitively sub-optimal a these two independent stage leave the model unaware about the varying label quality . What if we could somehow inform the model about the label quality ? In this paper , we propose a semi-supervised learning method where we train two neural network in a multi-task fashion : a `` target network '' and a `` confidence network '' . The target network be optimize to perform a give task and be train use a large set of unlabeled data that be weakly annotate . We propose to weight the gradient update to the target network use the score provide by the second confidence network , which be train on a small amount of supervise data . Thus we avoid that the weight update compute from noisy label harm the quality of the target network model . We evaluate our learn strategy on two different task : document ranking and sentiment classification . The result demonstrate that our approach not only enhance the performance compare to the baseline but also speed up the learning process from weak label . ","http://arxiv.org/pdf/1711.00313v2","cs.LG	cs.CL	cs.NE	stat.ML","Avoiding Your Teacher's Mistakes: Training Neural Networks with   Controlled Weak Supervision","dehghani@uva.nl,	severyn@google.com,	rothe@google.com,	kamps@uva.nl"
"Christopher Tegho	PaweÅ Budzianowski	Milica GaÅ¡iÄ","30","11","2017","In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.","In statistical dialogue management , the dialogue manager learn a policy that map a belief state to an action for the system to perform . Efficient exploration be key to successful policy optimisation . Current deep reinforcement learn method be very promising but rely on epsilon-greedy exploration , thus subject the user to a random choice of action during learn . Alternative approach such a Gaussian Process SARSA ( GPSARSA ) estimate uncertainty and be sample efficient , lead to well user experience , but on the expense of a great computational complexity . This paper examine approach to extract uncertainty estimate from deep Q-networks ( DQN ) in the context of dialogue management . We perform an extensive benchmark of deep Bayesian method to extract uncertainty estimate , namely Bayes-By-Backprop , dropout , it concrete variation , bootstrapped ensemble and alpha-divergences , combine it with DQN algorithm . ","http://arxiv.org/pdf/1711.11486v1","stat.ML	cs.CL	cs.LG	cs.NE","Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy   Optimisation","christegho@gmail.com	pfb30@cam.ac.uk	mg436@cam.ac.uk"
"Yuanhang Su	Yuzhong Huang	C. -C. Jay Kuo","27","2","2018","In this work, we investigate the memory capability of recurrent neural networks (RNNs), where this capability is defined as a function that maps an element in a sequence to the current output. We first analyze the system function of a recurrent neural network (RNN) cell, and provide analytical results for three RNNs. They are the simple recurrent neural network (SRN), the long short-term memory (LSTM), and the gated recurrent unit (GRU). Based on the analysis, we propose a new design to extend the memory length of a cell, and call it the extended long short-term memory (ELSTM). Next, we present a dependent bidirectional recurrent neural network (DBRNN) for the sequence-in-sequence-out (SISO) problem, which is more robust to previous erroneous predictions. Extensive experiments are carried out on different language tasks to demonstrate the superiority of our proposed ELSTM and DBRNN solutions.","In this work , we investigate the memory capability of recurrent neural network ( RNNs ) , where this capability be define a a function that map an element in a sequence to the current output . We first analyze the system function of a recurrent neural network ( RNN ) cell , and provide analytical result for three RNNs . They be the simple recurrent neural network ( SRN ) , the long short-term memory ( LSTM ) , and the gated recurrent unit ( GRU ) . Based on the analysis , we propose a new design to extend the memory length of a cell , and call it the extended long short-term memory ( ELSTM ) . Next , we present a dependent bidirectional recurrent neural network ( DBRNN ) for the sequence-in-sequence-out ( SISO ) problem , which be more robust to previous erroneous prediction . Extensive experiment be carry out on different language task to demonstrate the superiority of our propose ELSTM and DBRNN solution . ","http://arxiv.org/pdf/1803.01686v1","cs.LG	cs.CL	cs.NE	stat.ML","On Extended Long Short-term Memory and Dependent Bidirectional Recurrent   Neural Network",""
"Lin Ma	Zhengdong Lu	Hang Li","1","6","2015","In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.","In this paper , we propose to employ the convolutional neural network ( CNN ) for the image question answer ( QA ) . Our propose CNN provide an end-to-end framework with convolutional architecture for learn not only the image and question representation , but also their inter-modal interaction to produce the answer . More specifically , our model consist of three CNNs : one image CNN to encode the image content , one sentence CNN to compose the word of the question , and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer word . We demonstrate the efficacy of our propose model on the DAQUAR and COCO-QA datasets , which be two benchmark datasets for the image QA , with the performance significantly outperform the state-of-the-art . ","http://arxiv.org/pdf/1506.00333v2","cs.CL	cs.CV	cs.LG	cs.NE","Learning to Answer Questions From Image Using Convolutional Neural   Network","forest.linma@gmail.com	Lu.Zhengdong@huawei.com	HangLi,HL@huawei.com"
"Zichao Yang	Xiaodong He	Jianfeng Gao	Li Deng	Alex Smola","7","11","2015","This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches. The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.","This paper present stacked attention network ( SANs ) that learn to answer natural language question from image . SANs use semantic representation of a question a query to search for the region in an image that be relate to the answer . We argue that image question answer ( QA ) often require multiple step of reason . Thus , we develop a multiple-layer SAN in which we query an image multiple time to infer the answer progressively . Experiments conduct on four image QA data set demonstrate that the propose SANs significantly outperform previous state-of-the-art approach . The visualization of the attention layer illustrate the progress that the SAN locate the relevant visual clue that lead to the answer of the question layer-by-layer . ","http://arxiv.org/pdf/1511.02274v2","cs.LG	cs.CL	cs.CV	cs.NE","Stacked Attention Networks for Image Question Answering","zichaoy@cs.cmu.edu,	xiaohe	jfgao	deng}@microsoft.com	alex@smola.org"
"Jacob Andreas	Marcus Rohrbach	Trevor Darrell	Dan Klein","9","11","2015","Visual question answering is fundamentally compositional in nature---a question like "where is the dog?" shares substructure with questions like "what color is the dog?" and "where is the cat?" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.","Visual question answering be fundamentally compositional in nature -- -a question like `` where be the dog ? '' share substructure with question like `` what color be the dog ? '' and `` where be the cat ? '' This paper seek to simultaneously exploit the representational capacity of deep network and the compositional linguistic structure of question . We describe a procedure for construct and learn *neural module networks* , which compose collection of jointly-trained neural `` module '' into deep network for question answering . Our approach decompose question into their linguistic substructure , and use these structure to dynamically instantiate modular network ( with reusable component for recognize dog , classify color , etc. ) . The result compound network be jointly train . We evaluate our approach on two challenge datasets for visual question answering , achieve state-of-the-art result on both the VQA natural image dataset and a new dataset of complex question about abstract shape . ","http://arxiv.org/pdf/1511.02799v4","cs.CV	cs.CL	cs.LG	cs.NE","Neural Module Networks","jdau	rohrbachu	trevoru	klein}@{csu	eecsu	eecsu	csu"
"Federico Raue	Andreas Dengel	Thomas M. Breuel	Marcus Liwicki","13","11","2015","In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence, Psychology, and Neuroscience. In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. Our method uses two parallel Long Short-Term Memories (LSTMs) with a learning rule based on EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. The motivation behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs. We evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). The performance of our extension reaches better results than the original model and similar results to individual LSTM trained in each modality.","In this paper , we extend a symbolic association framework for be able to handle miss element in multimodal sequence . The general scope of the work be the symbolic association of object-word mapping a it happen in language development in infant . In other word , two different representation of the same abstract concept can associate in both direction . This scenario have be long interested in Artificial Intelligence , Psychology , and Neuroscience . In this work , we extend a recent approach for multimodal sequence ( visual and audio ) to also cope with miss element in one or both modality . Our method us two parallel Long Short-Term Memories ( LSTMs ) with a learning rule base on EM-algorithm . It align both LSTM output via Dynamic Time Warping ( DTW ) . We propose to include an extra step for the combination with the max operation for exploit the common element between both sequence . The motivation behind be that the combination act a a condition selector for choose the best representation from both LSTMs . We evaluate the propose extension in the following scenario : miss element in one modality ( visual or audio ) and miss element in both modality ( visual and sound ) . The performance of our extension reach good result than the original model and similar result to individual LSTM train in each modality . ","http://arxiv.org/pdf/1511.04401v5","cs.CV	cs.CL	cs.LG	cs.NE","Symbol Grounding Association in Multimodal Sequences with Missing   Elements","federico.raue@dfki.de	andreas.dengel@dfki.de	tmb@cs.uni-kl.de	liwicki@cs.uni-kl.de"
"Dan Hendrycks	Mantas Mazeika	Duncan Wilson	Kevin Gimpel","14","2","2018","The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets, non-expert labeling, and label corruption by data poisoning adversaries. In the latter case, corruptions may be arbitrarily bad, even so bad that a classifier predicts the wrong labels with high confidence. To protect against such sources of noise, we leverage the fact that a small set of clean labels is often easy to procure. We demonstrate that robustness to label noise up to severe strengths can be achieved by using a set of trusted data with clean labels, and propose a loss correction that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.","The grow importance of massive datasets with the advent of deep learning make robustness to label noise a critical property for classifier to have . Sources of label noise include automatic labeling for large datasets , non-expert labeling , and label corruption by data poison adversary . In the latter case , corruption may be arbitrarily bad , even so bad that a classifier predict the wrong label with high confidence . To protect against such source of noise , we leverage the fact that a small set of clean label be often easy to procure . We demonstrate that robustness to label noise up to severe strength can be achieve by use a set of trusted data with clean label , and propose a loss correction that utilize trusted example in a data-efficient manner to mitigate the effect of label noise on deep neural network classifier . Across vision and natural language processing task , we experiment with various label noise at several strength , and show that our method significantly outperform exist method . ","http://arxiv.org/pdf/1802.05300v1","cs.LG	cs.CL	cs.CV	cs.NE","Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe   Noise",""
"Kyunghyun Cho	Aaron Courville	Yoshua Bengio","4","7","2015","Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.","Whereas deep neural network be first mostly use for classification task , they be rapidly expand in the realm of structured output problem , where the observed target be compose of multiple random variable that have a rich joint distribution , give the input . We focus in this paper on the case where the input also have a rich structure and the input and output structure be somehow related . We describe system that learn to attend to different place in the input , for each element of the output , for a variety of task : machine translation , image caption generation , video clip description and speech recognition . All these system be base on a share set of building block : gate recurrent neural network and convolutional neural network , along with trained attention mechanism . We report on experimental result with these system , show impressively good performance and the advantage of the attention mechanism . ","http://arxiv.org/pdf/1507.01053v1","cs.NE	cs.CL	cs.CV	cs.LG","Describing Multimedia Content using Attention-based Encoder--Decoder   Networks",""
"Desmond Elliott	Stella Frank	Eva Hasler","15","10","2015","In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image description experiments on the IAPR-TC12 dataset of images aligned with English and German sentences, we find significant and substantial improvements in BLEU4 and Meteor scores for models trained over multiple languages, compared to a monolingual baseline.","In this paper we present an approach to multi-language image description bring together insight from neural machine translation and neural image description . To create a description of an image for a give target language , our sequence generation model condition on feature vector from the image , the description from the source language , and/or a multimodal vector compute over the image and a description in the source language . In image description experiment on the IAPR-TC12 dataset of image align with English and German sentence , we find significant and substantial improvement in BLEU4 and Meteor score for model train over multiple language , compare to a monolingual baseline . ","http://arxiv.org/pdf/1510.04709v2","cs.CL	cs.CV	cs.LG	cs.NE","Multilingual Image Description with Neural Sequence Models","d.elliott@uva.nl	s.c.frank@uva.nl	ech57@cam.ac.uk"
"Oswaldo Ludwig	Xiao Liu	Parisa Kordjamshidi	Marie-Francine Moens","28","3","2016","This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval.","This paper introduce the visually inform embedding of word ( VIEW ) , a continuous vector representation for a word extract from a deep neural model train use the Microsoft COCO data set to forecast the spatial arrangement between visual object , give a textual description . The model be compose of a deep multilayer perceptron ( MLP ) stack on the top of a Long Short Term Memory ( LSTM ) network , the latter be precede by an embed layer . The VIEW be apply to transfer multimodal background knowledge to Spatial Role Labeling ( SpRL ) algorithm , which recognize spatial relation between object mention in the text . This work also contribute with a new method to select complementary feature and a fine-tuning method for MLP that improve the $ F1 $ measure in classify the word into spatial role . The VIEW be evaluate with the Task 3 of SemEval-2013 benchmark data set , SpaceEval . ","http://arxiv.org/pdf/1603.08474v1","cs.CL	cs.CV	cs.LG	cs.NE","Deep Embedding for Spatial Role Labeling",""
"Yuntian Deng	Anssi Kanervisto	Jeffrey Ling	Alexander M. Rush","16","9","2016","We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.","We present a neural encoder-decoder model to convert image into presentational markup base on a scalable coarse-to-fine attention mechanism . Our method be evaluate in the context of image-to-LaTeX generation , and we introduce a new dataset of real-world rendered mathematical expression pair with LaTeX markup . We show that unlike neural OCR technique use CTC-based model , attention-based approach can tackle this non-standard OCR task . Our approach outperform classical mathematical OCR system by a large margin on in-domain render data , and , with pretraining , also perform well on out-of-domain handwritten data . To reduce the inference complexity associate with the attention-based approach , we introduce a new coarse-to-fine attention layer that select a support region before apply attention . ","http://arxiv.org/pdf/1609.04938v2","cs.CV	cs.CL	cs.LG	cs.NE","Image-to-Markup Generation with Coarse-to-Fine Attention",""
"Sumeet S. Singh","15","2","2018","We present a deep recurrent neural network model with soft visual attention that learns to generate LaTeX markup of real-world math formulas given their images. Applying neural sequence generation techniques that have been very successful in the fields of machine translation and image/handwriting/speech captioning, recognition, transcription and synthesis, we construct an image-to-markup model that learns to produce syntactically and semantically correct LaTeX markup code of over 150 words long and achieves a BLEU score of 89%; the best reported so far for the Im2Latex problem. We also visually demonstrate that the model learns to scan the image left-right / up-down much as a human would read it.","We present a deep recurrent neural network model with soft visual attention that learn to generate LaTeX markup of real-world math formula give their image . Applying neural sequence generation technique that have be very successful in the field of machine translation and image/handwriting/speech captioning , recognition , transcription and synthesis , we construct an image-to-markup model that learn to produce syntactically and semantically correct LaTeX markup code of over 150 word long and achieve a BLEU score of 89 % ; the best report so far for the Im2Latex problem . We also visually demonstrate that the model learn to scan the image left-right / up-down much a a human would read it . ","http://arxiv.org/pdf/1802.05415v1","cs.LG	cs.CL	cs.CV	cs.NE","Teaching Machines to Code: Neural Markup Generation with Visual   Attention",""
"Mohammad Javad Shafiee	Elnaz Barshan	Alexander Wong","7","4","2017","A promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence, which mimics biological evolution processes to progressively synthesize more efficient networks. A crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks. In this study, we take a deeper look at the notion of synaptic cluster-driven evolution of deep neural networks which guides the evolution process towards the formation of a highly sparse set of synaptic clusters in offspring networks. Utilizing a synaptic cluster-driven genetic encoding, the probabilistic encoding of synaptic traits considers not only individual synaptic properties but also inter-synaptic relationships within a deep neural network. This process results in highly sparse offspring networks which are particularly tailored for parallel computational devices such as GPUs and deep neural network accelerator chips. Comprehensive experimental results using four well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and DetectNet) on two different tasks (object categorization and object detection) demonstrate the efficiency of the proposed method. Cluster-driven genetic encoding scheme synthesizes networks that can achieve state-of-the-art performance with significantly smaller number of synapses than that of the original ancestor network. ($\sim$125-fold decrease in synapses for MNIST). Furthermore, the improved cluster efficiency in the generated offspring networks ($\sim$9.71-fold decrease in clusters for MNIST and a $\sim$8.16-fold decrease in clusters for KITTI) is particularly useful for accelerated performance on parallel computing hardware architectures such as those in GPUs and deep neural network accelerator chips.","A promising paradigm for achieve highly efficient deep neural network be the idea of evolutionary deep intelligence , which mimic biological evolution process to progressively synthesize more efficient network . A crucial design factor in evolutionary deep intelligence be the genetic encoding scheme use to simulate heredity and determine the architecture of offspring network . In this study , we take a deep look at the notion of synaptic cluster-driven evolution of deep neural network which guide the evolution process towards the formation of a highly sparse set of synaptic cluster in offspring network . Utilizing a synaptic cluster-driven genetic encoding , the probabilistic encoding of synaptic trait considers not only individual synaptic property but also inter-synaptic relationship within a deep neural network . This process result in highly sparse offspring network which be particularly tailor for parallel computational device such a GPUs and deep neural network accelerator chip . Comprehensive experimental result use four well-known deep neural network architecture ( LeNet-5 , AlexNet , ResNet-56 , and DetectNet ) on two different task ( object categorization and object detection ) demonstrate the efficiency of the propose method . Cluster-driven genetic encode scheme synthesizes network that can achieve state-of-the-art performance with significantly small number of synapsis than that of the original ancestor network . ( $ \sim $ 125-fold decrease in synapsis for MNIST ) . Furthermore , the improved cluster efficiency in the generated offspring network ( $ \sim $ 9.71-fold decrease in cluster for MNIST and a $ \sim $ 8.16-fold decrease in cluster for KITTI ) be particularly useful for accelerated performance on parallel compute hardware architecture such a those in GPUs and deep neural network accelerator chip . ","http://arxiv.org/pdf/1704.02081v1","cs.NE	cs.AI	cs.CV	stat.ML","Evolution in Groups: A deeper look at synaptic cluster driven evolution   of deep neural networks","mjshafiee@uwaterloo.ca	ebarshan@uwaterloo.ca	a28wong@uwaterloo.ca"
"Mete Ozay	Ilke Ãztekin	Uygar Ãztekin	Fatos T. Yarman Vural","10","5","2012","A relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis (MVPA), which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation. MVPA methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain, based on distributed patterns of neural activity. In the current investigation, we propose a new approach for representation of neural data for pattern analysis, namely a Mesh Learning Model. In this approach, at each time instant, a star mesh is formed around each voxel, such that the voxel corresponding to the center node is surrounded by its p-nearest neighbors. The arc weights of each mesh are estimated from the voxel intensity values by least squares method. The estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs), are then used to train a classifier, such as Neural Networks, k-Nearest Neighbor, Na\"ive Bayes and Support Vector Machines. The proposed Mesh Model was tested on neuroimaging data acquired via functional magnetic resonance imaging (fMRI) during a recognition memory experiment using categorized word lists, employing a previously established experimental paradigm (\"Oztekin & Badre, 2011). Results suggest that the proposed Mesh Learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing.","A relatively recent advance in cognitive neuroscience have be multi-voxel pattern analysis ( MVPA ) , which enable researcher to decode brain state and/or the type of information represent in the brain during a cognitive operation . MVPA method utilize machine learn algorithm to distinguish among type of information or cognitive state represent in the brain , base on distributed pattern of neural activity . In the current investigation , we propose a new approach for representation of neural data for pattern analysis , namely a Mesh Learning Model . In this approach , at each time instant , a star mesh be form around each voxel , such that the voxel correspond to the center node be surround by it p-nearest neighbor . The arc weight of each mesh be estimate from the voxel intensity value by least square method . The estimated arc weight of all the mesh , call Mesh Arc Descriptors ( MADs ) , be then use to train a classifier , such a Neural Networks , k-Nearest Neighbor , Na\ '' ive Bayes and Support Vector Machines . The propose Mesh Model be test on neuroimaging data acquire via functional magnetic resonance imaging ( fMRI ) during a recognition memory experiment use categorized word list , employ a previously establish experimental paradigm ( \ '' Oztekin & Badre , 2011 ) . Results suggest that the propose Mesh Learning approach can provide an effective algorithm for pattern analysis of brain activity during cognitive processing . ","http://arxiv.org/pdf/1205.2382v3","cs.NE	cs.AI	cs.CV	stat.ML","Mesh Learning for Classifying Cognitive Processes",""
"A. H. Karimi	M. J. Shafiee	A. Ghodsi	A. Wong","1","7","2017","In this work, we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions, and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a CNN with convolutional layer synaptic strengths drawn from biologically-inspired distributions such as log-normal or correlated center-surround distributions performed relatively well suggesting a possibility for designing deep neural network architectures that do not require many data samples to learn, and can sidestep current training procedures while maintaining or boosting modelling performance.","In this work , we perform an exploratory study on synthesize deep neural network use biological synaptic strength distribution , and the potential influence of different distribution on model performance particularly for the scenario associate with small data set . Surprisingly , a CNN with convolutional layer synaptic strength draw from biologically-inspired distribution such a log-normal or correlate center-surround distribution perform relatively well suggest a possibility for design deep neural network architecture that do not require many data sample to learn , and can sidestep current training procedure while maintain or boost modelling performance . ","http://arxiv.org/pdf/1707.00081v1","cs.NE	cs.AI	cs.CV	stat.ML","Synthesizing Deep Neural Network Architectures using Biological Synaptic   Strength Distributions","(a6karimi@uwaterloo.ca)1	(mjshafiee@uwaterloo.ca)2	(aghodsib@uwaterloo.ca)3	(a28wong@uwaterloo.ca)2"
"Yukun Bao	Zhongyi Hu	Tao Xiong","9","1","2014","Addressing the issue of SVMs parameters optimization, this study proposes an efficient memetic algorithm based on Particle Swarm Optimization algorithm (PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is responsible for exploration of the search space and the detection of the potential regions with optimum solutions, while pattern search (PS) is used to produce an effective exploitation on the potential regions obtained by PSO. Moreover, a novel probabilistic selection strategy is proposed to select the appropriate individuals among the current population to undergo local refinement, keeping a well balance between exploration and exploitation. Experimental results confirm that the local refinement with PS and our proposed selection strategy are effective, and finally demonstrate effectiveness and robustness of the proposed PSO-PS based MA for SVMs parameters optimization.","Addressing the issue of SVMs parameter optimization , this study propose an efficient memetic algorithm base on Particle Swarm Optimization algorithm ( PSO ) and Pattern Search ( PS ) . In the propose memetic algorithm , PSO be responsible for exploration of the search space and the detection of the potential region with optimum solution , while pattern search ( PS ) be use to produce an effective exploitation on the potential region obtain by PSO . Moreover , a novel probabilistic selection strategy be propose to select the appropriate individual among the current population to undergo local refinement , keep a well balance between exploration and exploitation . Experimental result confirm that the local refinement with PS and our propose selection strategy be effective , and finally demonstrate effectiveness and robustness of the propose PSO-PS base MA for SVMs parameter optimization . ","http://arxiv.org/pdf/1401.1926v1","cs.LG	cs.AI	cs.NE	stat.ML","A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters   Optimization",""
"Laurent Dinh	Jascha Sohl-Dickstein	Samy Bengio","27","5","2016","Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.","Unsupervised learning of probabilistic model be a central yet challenge problem in machine learning . Specifically , design model with tractable learning , sample , inference and evaluation be crucial in solve this task . We extend the space of such model use real-valued non-volume preserving ( real NVP ) transformation , a set of powerful invertible and learnable transformation , result in an unsupervised learning algorithm with exact log-likelihood computation , exact sampling , exact inference of latent variable , and an interpretable latent space . We demonstrate it ability to model natural image on four datasets through sampling , log-likelihood evaluation and latent variable manipulation . ","http://arxiv.org/pdf/1605.08803v3","cs.LG	cs.AI	cs.NE	stat.ML","Density estimation using Real NVP",""
"Tim Salimans	Jonathan Ho	Xi Chen	Szymon Sidor	Ilya Sutskever","10","3","2017","We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.","We explore the use of Evolution Strategies ( ES ) , a class of black box optimization algorithms , a an alternative to popular MDP-based RL technique such a Q-learning and Policy Gradients . Experiments on MuJoCo and Atari show that ES be a viable solution strategy that scale extremely well with the number of CPUs available : By use a novel communication strategy base on common random number , our ES implementation only need to communicate scalar , make it possible to scale to over a thousand parallel worker . This allow u to solve 3D humanoid walking in 10 minute and obtain competitive result on most Atari game after one hour of training . In addition , we highlight several advantage of ES a a black box optimization technique : it be invariant to action frequency and delayed reward , tolerant of extremely long horizon , and do not need temporal discounting or value function approximation . ","http://arxiv.org/pdf/1703.03864v2","stat.ML	cs.AI	cs.LG	cs.NE","Evolution Strategies as a Scalable Alternative to Reinforcement Learning",""
"Peter Karkus	David Hsu	Wee Sun Lee","20","3","2017","This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and "transfer" to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.","This paper introduce the QMDP-net , a neural network architecture for plan under partial observability . The QMDP-net combine the strength of model-free learning and model-based planning . It be a recurrent policy network , but it represent a policy for a parameterized set of task by connect a model with a planning algorithm that solve the model , thus embed the solution structure of planning in a network learn architecture . The QMDP-net be fully differentiable and allow for end-to-end training . We train a QMDP-net on different task so that it can generalize to new one in the parameterized task set and `` transfer '' to other similar task beyond the set . In preliminary experiment , QMDP-net show strong performance on several robotic task in simulation . Interestingly , while QMDP-net encode the QMDP algorithm , it sometimes outperform the QMDP algorithm in the experiment , a a result of end-to-end learning . ","http://arxiv.org/pdf/1703.06692v3","cs.AI	cs.LG	cs.NE	stat.ML","QMDP-Net: Deep Learning for Planning under Partial Observability","karkus@comp.nus.edu.sg	dyhsu@comp.nus.edu.sg	leews@comp.nus.edu.sg"
"Gregory Farquhar	Tim RocktÃ¤schel	Maximilian Igl	Shimon Whiteson","31","10","2017","Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning. To address these challenges, we propose TreeQN, a differentiable, recursive, tree-structured model that serves as a drop-in replacement for any value function network in deep RL with discrete actions. TreeQN dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and then aggregating predicted rewards and state-values using a tree backup to estimate Q-values. We also propose ATreeC, an actor-critic variant that augments TreeQN with a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end, such that the learned model is optimised for its actual use in the tree. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task, as well as n-step DQN and value prediction networks (Oh et al. 2017) on multiple Atari games. Furthermore, we present ablation studies that demonstrate the effect of different auxiliary losses on learning transition models.","Combining deep model-free reinforcement learn with on-line planning be a promising approach to building on the success of deep RL . On-line plan with look-ahead tree have prove successful in environment where transition model be know a priori . However , in complex environment where transition model need to be learn from data , the deficiency of learned model have limit their utility for plan . To address these challenge , we propose TreeQN , a differentiable , recursive , tree-structured model that serve a a drop-in replacement for any value function network in deep RL with discrete action . TreeQN dynamically construct a tree by recursively apply a transition model in a learned abstract state space and then aggregate predict reward and state-values use a tree backup to estimate Q-values . We also propose ATreeC , an actor-critic variant that augment TreeQN with a softmax layer to form a stochastic policy network . Both approach be trained end-to-end , such that the learned model be optimise for it actual use in the tree . We show that TreeQN and ATreeC outperform n-step DQN and A2C on a box-pushing task , as well a n-step DQN and value prediction network ( Oh et al . 2017 ) on multiple Atari game . Furthermore , we present ablation study that demonstrate the effect of different auxiliary loss on learn transition model . ","http://arxiv.org/pdf/1710.11417v2","cs.AI	cs.LG	cs.NE	stat.ML","TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep   Reinforcement Learning","gregory.farquhar@cs.ox.ac.uk	tim.rocktaschel@cs.ox.ac.uk	maximilian.igl@cs.ox.ac.uk	shimon.whiteson@cs.ox.ac.uk"
"Nan Rosemary Ke	Anirudh Goyal	Olexa Bilaniuk	Jonathan Binas	Laurent Charlin	Chris Pal	Yoshua Bengio","7","11","2017","A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through time is used as a heuristic. However, this usually leads to biased estimates of the gradient in which longer term dependencies are ignored. Addressing this issue, we propose an alternative algorithm, Sparse Attentive Backtracking, which might also be related to principles used by brains to learn long-term dependencies. Sparse Attentive Backtracking learns an attention mechanism over the hidden states of the past and selectively backpropagates through paths with high attention weights. This allows the model to learn long term dependencies while only backtracking for a small number of time steps, not just from the recent past but also from attended relevant past states.","A major drawback of backpropagation through time ( BPTT ) be the difficulty of learn long-term dependency , come from have to propagate credit information backwards through every single step of the forward computation . This make BPTT both computationally impractical and biologically implausible . For this reason , full backpropagation through time be rarely use on long sequence , and truncate backpropagation through time be use a a heuristic . However , this usually lead to bias estimate of the gradient in which long term dependency be ignore . Addressing this issue , we propose an alternative algorithm , Sparse Attentive Backtracking , which might also be relate to principle use by brain to learn long-term dependency . Sparse Attentive Backtracking learn an attention mechanism over the hidden state of the past and selectively backpropagates through path with high attention weight . This allow the model to learn long term dependency while only backtrack for a small number of time step , not just from the recent past but also from attend relevant past state . ","http://arxiv.org/pdf/1711.02326v1","cs.AI	cs.LG	cs.NE	stat.ML","Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent   Networks",""
"Anakha V Babu	Bipin Rajendran","9","11","2017","We study the performance of stochastically trained deep neural networks (DNNs) whose synaptic weights are implemented using emerging memristive devices that exhibit limited dynamic range, resolution, and variability in their programming characteristics. We show that a key device parameter to optimize the learning efficiency of DNNs is the variability in its programming characteristics. DNNs with such memristive synapses, even with dynamic range as low as $15$ and only $32$ discrete levels, when trained based on stochastic updates suffer less than $3\%$ loss in accuracy compared to floating point software baseline. We also study the performance of stochastic memristive DNNs when used as inference engines with noise corrupted data and find that if the device variability can be minimized, the relative degradation in performance for the Stochastic DNN is better than that of the software baseline. Hence, our study presents a new optimization corner for memristive devices for building large noise-immune deep learning systems.","We study the performance of stochastically trained deep neural network ( DNNs ) whose synaptic weight be implement use emerge memristive device that exhibit limited dynamic range , resolution , and variability in their programming characteristic . We show that a key device parameter to optimize the learning efficiency of DNNs be the variability in it programming characteristic . DNNs with such memristive synapsis , even with dynamic range as low a $ 15 $ and only $ 32 $ discrete level , when train base on stochastic update suffer less than $ 3\ % $ loss in accuracy compare to float point software baseline . We also study the performance of stochastic memristive DNNs when use a inference engine with noise corrupt data and find that if the device variability can be minimize , the relative degradation in performance for the Stochastic DNN be good than that of the software baseline . Hence , our study present a new optimization corner for memristive device for build large noise-immune deep learning system . ","http://arxiv.org/pdf/1711.03640v1","stat.ML	cs.AI	cs.LG	cs.NE","Stochastic Deep Learning in Memristive Networks","av442@njit.edu	bipin@njit.edu"
"Yukun Bao	Tao Xiong	Zhongyi Hu","31","12","2013","Multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. Recently, the multiple-input several multiple-outputs (MISMO) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and the direct strategies. Built on the established MISMO strategy, this study proposes a particle swarm optimization (PSO)-based MISMO modeling strategy, which is capable of determining the number of sub-models in a self-adaptive mode, with varying prediction horizons. Rather than deriving crisp divides with equal-size s prediction horizons from the established MISMO, the proposed PSO-MISMO strategy, implemented with neural networks, employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models, providing considerable flexibility in model construction, which has been validated with simulated and real datasets.","Multi-step-ahead time series prediction be one of the most challenging research topic in the field of time series modeling and prediction , and be continually under research . Recently , the multiple-input several multiple-outputs ( MISMO ) model strategy have be propose a a promising alternative for multi-step-ahead time series prediction , exhibit advantage compare with the two currently dominate strategy , the iterated and the direct strategy . Built on the establish MISMO strategy , this study propose a particle swarm optimization ( PSO ) -based MISMO model strategy , which be capable of determine the number of sub-models in a self-adaptive mode , with vary prediction horizon . Rather than derive crisp divide with equal-size s prediction horizon from the establish MISMO , the propose PSO-MISMO strategy , implement with neural network , employ a heuristic to create flexible divide with vary size of prediction horizon and to generate corresponding sub-models , provide considerable flexibility in model construction , which have be validate with simulated and real datasets . ","http://arxiv.org/pdf/1401.0104v1","cs.AI	cs.LG	cs.NE	stat.ML","PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction",""
"Behnam Neyshabur	Ryota Tomioka	Nathan Srebro","27","2","2015","We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.","We investigate the capacity , convexity and characterization of a general family of norm-constrained feed-forward network . ","http://arxiv.org/pdf/1503.00036v2","cs.LG	cs.AI	cs.NE	stat.ML","Norm-Based Capacity Control in Neural Networks",""
"Konrad Zolna","5","12","2016","The method presented extends a given regression neural network to make its performance improve. The modification affects the learning procedure only, hence the extension may be easily omitted during evaluation without any change in prediction. It means that the modified model may be evaluated as quickly as the original one but tends to perform better.   This improvement is possible because the modification gives better expressive power, provides better behaved gradients and works as a regularization. The knowledge gained by the temporarily extended neural network is contained in the parameters shared with the original neural network.   The only cost is an increase in learning time.","The method present extend a give regression neural network to make it performance improve . The modification affect the learning procedure only , hence the extension may be easily omit during evaluation without any change in prediction . It mean that the modified model may be evaluate as quickly a the original one but tend to perform well . This improvement be possible because the modification give well expressive power , provide good behave gradient and work a a regularization . The knowledge gain by the temporarily extend neural network be contain in the parameter share with the original neural network . The only cost be an increase in learning time . ","http://arxiv.org/pdf/1612.01589v1","cs.LG	cs.AI	cs.NE	stat.ML","Improving the Performance of Neural Networks in Regression Tasks Using   Drawering","konrad.zolna@im.uj.edu.pl,	konrad.zolna@rtbhouse.com"
"Yujia Li	Kevin Swersky	Richard Zemel","17","12","2014","A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irrelevant underlying data dimensions. One very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy (MMD) [5], a measure of distance between probability distributions. We are not the first to suggest that MMD can be a useful criterion in developing representations that apply across multiple domains or tasks [1]. However, in this paper we describe a number of novel applications of this criterion that we have devised, all based on the idea of developing unbiased representations. These formulations include: a standard domain adaptation framework; a method of learning invariant representations; an approach based on noise-insensitive autoencoders; and a novel form of generative model.","A key element in transfer learning be representation learning ; if representation can be develop that expose the relevant factor underlie the data , then new task and domain can be learn readily base on mapping of these salient factor . We propose that an important aim for these representation be to be unbiased . Different form of representation learning can be derive from alternative definition of unwanted bias , e.g. , bias to particular task , domain , or irrelevant underlying data dimension . One very useful approach to estimate the amount of bias in a representation come from maximum mean discrepancy ( MMD ) [ 5 ] , a measure of distance between probability distribution . We be not the first to suggest that MMD can be a useful criterion in develop representation that apply across multiple domain or task [ 1 ] . However , in this paper we describe a number of novel application of this criterion that we have devise , all base on the idea of develop unbiased representation . These formulation include : a standard domain adaptation framework ; a method of learn invariant representation ; an approach base on noise-insensitive autoencoders ; and a novel form of generative model . ","http://arxiv.org/pdf/1412.5244v1","cs.LG	cs.AI	cs.NE	stat.ML","Learning unbiased features","yujiali@cs.toronto.edu	kswersky@cs.toronto.edu	zemel@cs.toronto.edu"
"David Balduzzi	Muhammad Ghifary","10","9","2015","This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.","This paper propose GProp , a deep reinforcement learn algorithm for continuous policy with compatible function approximation . The algorithm be base on two innovation . Firstly , we present a temporal-difference base method for learn the gradient of the value-function . Secondly , we present the deviator-actor-critic ( DAC ) model , which comprise three neural network that estimate the value function , it gradient , and determine the actor 's policy respectively . We evaluate GProp on two challenge task : a contextual bandit problem construct from nonparametric regression datasets that be design to probe the ability of reinforcement learn algorithm to accurately estimate gradient ; and the octopus arm , a challenging reinforcement learn benchmark . GProp be competitive with fully supervise method on the bandit task and achieve the best performance to date on the octopus arm . ","http://arxiv.org/pdf/1509.03005v1","cs.LG	cs.AI	cs.NE	stat.ML","Compatible Value Gradients for Reinforcement Learning of Continuous Deep   Policies","david.balduzzi@vuw.ac.nz	muhammad.ghifary@ecs.vuw.ac.nz"
"Takayuki Osogami	Makoto Otsuka","29","9","2015","We propose a particularly structured Boltzmann machine, which we refer to as a dynamic Boltzmann machine (DyBM), as a stochastic model of a multi-dimensional time-series. The DyBM can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. This proposed structure is motivated by postulates and observations, from biological neural networks, that the synaptic weight is strengthened or weakened, depending on the timing of spikes (i.e., spike-timing dependent plasticity or STDP). We show that the learning rule of updating the parameters of the DyBM in the direction of maximizing the likelihood of given time-series can be interpreted as STDP with long term potentiation and long term depression. The learning rule has a guarantee of convergence and can be performed in a distributed matter (i.e., local in space) with limited memory (i.e., local in time).","We propose a particularly structured Boltzmann machine , which we refer to a a dynamic Boltzmann machine ( DyBM ) , a a stochastic model of a multi-dimensional time-series . The DyBM can have infinitely many layer of unit but allow exact and efficient inference and learning when it parameter have a propose structure . This proposed structure be motivate by postulate and observation , from biological neural network , that the synaptic weight be strengthen or weaken , depend on the timing of spike ( i.e. , spike-timing dependent plasticity or STDP ) . We show that the learning rule of update the parameter of the DyBM in the direction of maximize the likelihood of give time-series can be interpret a STDP with long term potentiation and long term depression . The learning rule have a guarantee of convergence and can be perform in a distributed matter ( i.e. , local in space ) with limited memory ( i.e. , local in time ) . ","http://arxiv.org/pdf/1509.08634v1","cs.NE	cs.AI	cs.LG	stat.ML","Learning dynamic Boltzmann machines with spike-timing dependent   plasticity",""
"Yujia Li	Daniel Tarlow	Marc Brockschmidt	Richard Zemel","17","11","2015","Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.","Graph-structured data appear frequently in domain include chemistry , natural language semantics , social network , and knowledge base . In this work , we study feature learn technique for graph-structured input . Our start point be previous work on Graph Neural Networks ( Scarselli et al. , 2009 ) , which we modify to use gated recurrent unit and modern optimization technique and then extend to output sequence . The result be a flexible and broadly useful class of neural network model that have favorable inductive bias relative to purely sequence-based model ( e.g. , LSTMs ) when the problem be graph-structured . We demonstrate the capability on some simple AI ( bAbI ) and graph algorithm learn task . We then show it achieve state-of-the-art performance on a problem from program verification , in which subgraphs need to be match to abstract data structure . ","http://arxiv.org/pdf/1511.05493v4","cs.LG	cs.AI	cs.NE	stat.ML","Gated Graph Sequence Neural Networks","yujiali@cs.toronto.edu	zemel@cs.toronto.edu	mabrocks@microsoft.com	dtarlow@microsoft.com"
"Gabriel Dulac-Arnold	Richard Evans	Hado van Hasselt	Peter Sunehag	Timothy Lillicrap	Jonathan Hunt	Timothy Mann	Theophane Weber	Thomas Degris	Ben Coppin","24","12","2015","Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.","Being able to reason in an environment with a large number of discrete action be essential to bring reinforcement learn to a large class of problem . Recommender system , industrial plant and language model be only some of the many real-world task involve large number of discrete action for which current method be difficult or even often impossible to apply . An ability to generalize over the set of action as well a sub-linear complexity relative to the size of the set be both necessary to handle such task . Current approach be not able to provide both of these , which motivate the work in this paper . Our propose approach leverage prior information about the action to embed them in a continuous space upon which it can generalize . Additionally , approximate nearest-neighbor method allow for logarithmic-time lookup complexity relative to the number of action , which be necessary for time-wise tractable training . This combined approach allow reinforcement learn method to be apply to large-scale learning problem previously intractable with current method . We demonstrate our algorithm 's ability on a series of task have up to one million action . ","http://arxiv.org/pdf/1512.07679v2","cs.AI	cs.LG	cs.NE	stat.ML","Deep Reinforcement Learning in Large Discrete Action Spaces",""
"Aviv Tamar	Yi Wu	Garrett Thomas	Sergey Levine	Pieter Abbeel","9","2","2016","We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.","We introduce the value iteration network ( VIN ) : a fully differentiable neural network with a ` planning module ' embed within . VINs can learn to plan , and be suitable for predict outcome that involve planning-based reasoning , such a policy for reinforcement learning . Key to our approach be a novel differentiable approximation of the value-iteration algorithm , which can be represent a a convolutional neural network , and train end-to-end use standard backpropagation . We evaluate VIN base policy on discrete and continuous path-planning domain , and on a natural-language base search task . We show that by learn an explicit planning computation , VIN policy generalize good to new , unseen domain . ","http://arxiv.org/pdf/1602.02867v4","cs.AI	cs.LG	cs.NE	stat.ML","Value Iteration Networks",""
"Mikael Henaff	Arthur Szlam	Yann LeCun","22","2","2016","Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.","Although RNNs have be show to be powerful tool for process sequential data , find architecture or optimization strategy that allow them to model very long term dependency be still an active area of research . In this work , we carefully analyze two synthetic datasets originally outline in ( Hochreiter and Schmidhuber , 1997 ) which be use to evaluate the ability of RNNs to store information over many time step . We explicitly construct RNN solution to these problem , and use these construction , illuminate both the problem themselves and the way in which RNNs store different type of information in their hidden state . These construction furthermore explain the success of recent method that specify unitary initialization or constraint on the transition matrix . ","http://arxiv.org/pdf/1602.06662v2","cs.NE	cs.AI	cs.LG	stat.ML","Recurrent Orthogonal Networks and Long-Memory Tasks",""
"Hado van Hasselt	Arthur Guez	Matteo Hessel	Volodymyr Mnih	David Silver","24","2","2016","Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.","Most learning algorithm be not invariant to the scale of the function that be be approximate . We propose to adaptively normalize the target use in learn . This be useful in value-based reinforcement learning , where the magnitude of appropriate value approximation can change over time when we update the policy of behavior . Our main motivation be prior work on learn to play Atari game , where the reward be all clip to a predetermined range . This clip facilitates learn across many different game with a single learning algorithm , but a clipped reward function can result in qualitatively different behavior . Using the adaptive normalization we can remove this domain-specific heuristic without diminish overall performance . ","http://arxiv.org/pdf/1602.07714v2","cs.LG	cs.AI	cs.NE	stat.ML","Learning values across many orders of magnitude",""
"Laura Deming	Sasha Targ	Nate Sauder	Diogo Almeida	Chun Jimmie Ye","23","5","2016","Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that discovers an optimal architecture which simultaneously learns general genomic patterns and identifies the most important sequence motifs in predicting functional genomic outcomes. The architectures we find using this algorithm succeed at using only RNA expression data to predict gene regulatory structure, learn human-interpretable visualizations of key sequence motifs, and surpass state-of-the-art results on benchmark genomics challenges.","Each human genome be a 3 billion base pair set of encode instruction . Decoding the genome use deep learning fundamentally differs from most task , a we do not know the full structure of the data and therefore can not design architecture to suit it . As such , architecture that fit the structure of genomics should be learn not prescribe . Here , we develop a novel search algorithm , applicable across domain , that discover an optimal architecture which simultaneously learn general genomic pattern and identify the most important sequence motif in predict functional genomic outcome . The architecture we find use this algorithm succeed at use only RNA expression data to predict gene regulatory structure , learn human-interpretable visualization of key sequence motif , and surpass state-of-the-art result on benchmark genomics challenge . ","http://arxiv.org/pdf/1605.07156v1","cs.LG	cs.AI	cs.NE	stat.ML","Genetic Architect: Discovering Genomic Structure with Learned Neural   Architectures","ldeming.www@gmail.com,	sasha.targ@ucsf.edu	nate@enlitic.com,	diogo@enlitic.com,	jimmie.ye@ucsf.edu"
"Tejas D. Kulkarni	Ardavan Saeedi	Simanta Gautam	Samuel J. Gershman","8","6","2016","Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.","Learning robust value function give raw observation and reward be now possible with model-free and model-based deep reinforcement learn algorithm . There be a third alternative , call Successor Representations ( SR ) , which decompose the value function into two component -- a reward predictor and a successor map . The successor map represent the expected future state occupancy from any give state and the reward predictor map state to scalar reward . The value function of a state can be compute a the inner product between the successor map and the reward weight . In this paper , we present DSR , which generalize SR within an end-to-end deep reinforcement learn framework . DSR have several appeal property include : increased sensitivity to distal reward change due to factorization of reward and world dynamic , and the ability to extract bottleneck state ( subgoals ) give successor map train under a random policy . We show the efficacy of our approach on two diverse environment give raw pixel observation -- simple grid-world domain ( MazeBase ) and the Doom game engine . ","http://arxiv.org/pdf/1606.02396v1","stat.ML	cs.AI	cs.LG	cs.NE","Deep Successor Reinforcement Learning","tejask@mit.edu	ardavans@mit.edu	simanta@mit.edu	gershman@fas.harvard.edu"
"Yan Duan	John Schulman	Xi Chen	Peter L. Bartlett	Ilya Sutskever	Pieter Abbeel","9","11","2016","Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL$^2$ is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL$^2$ on a vision-based navigation task and show that it scales up to high-dimensional problems.","Deep reinforcement learning ( deep RL ) have be successful in learn sophisticated behavior automatically ; however , the learning process require a huge number of trial . In contrast , animal can learn new task in just a few trial , benefit from their prior knowledge about the world . This paper seek to bridge this gap . Rather than design a `` fast '' reinforcement learn algorithm , we propose to represent it a a recurrent neural network ( RNN ) and learn it from data . In our propose method , RL $ ^2 $ , the algorithm be encode in the weight of the RNN , which be learn slowly through a general-purpose ( `` slow '' ) RL algorithm . The RNN receive all information a typical RL algorithm would receive , include observation , action , reward , and termination flag ; and it retain it state across episode in a give Markov Decision Process ( MDP ) . The activation of the RNN store the state of the `` fast '' RL algorithm on the current ( previously unseen ) MDP . We evaluate RL $ ^2 $ experimentally on both small-scale and large-scale problem . On the small-scale side , we train it to solve randomly generate multi-arm bandit problem and finite MDPs . After RL $ ^2 $ be train , it performance on new MDPs be close to human-designed algorithm with optimality guarantee . On the large-scale side , we test RL $ ^2 $ on a vision-based navigation task and show that it scale up to high-dimensional problem . ","http://arxiv.org/pdf/1611.02779v2","cs.AI	cs.LG	cs.NE	stat.ML","RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning","rocky	joschu	peter}@openai.com	peter@berkeley.edu,	ilyasu@openai.com	pieter@openai.com"
"Jasmine Collins	Jascha Sohl-Dickstein	David Sussillo","29","11","2016","Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.","Two potential bottleneck on the expressiveness of recurrent neural network ( RNNs ) be their ability to store information about the task in their parameter , and to store information about the input history in their unit . We show experimentally that all common RNN architecture achieve nearly the same per-task and per-unit capacity bound with careful training , for a variety of task and stack depth . They can store an amount of task information which be linear in the number of parameter , and be approximately 5 bit per parameter . They can additionally store approximately one real number from their input history per hidden unit . We further find that for several task it be the per-task parameter capacity bound that determine performance . These result suggest that many previous result compare RNN architecture be drive primarily by difference in training effectiveness , rather than difference in capacity . Supporting this observation , we compare train difficulty for several architecture , and show that vanilla RNNs be far more difficult to train , yet have slightly high capacity . Finally , we propose two novel RNN architecture , one of which be easy to train than the LSTM or GRU for deeply stacked architecture . ","http://arxiv.org/pdf/1611.09913v3","stat.ML	cs.AI	cs.LG	cs.NE","Capacity and Trainability in Recurrent Neural Networks","jlcollins@google.com	jaschasd@google.com	sussillo@google.com"
"Mohammad Taha Bahadori	Krzysztof Chalupka	Edward Choi	Robert Chen	Walter F. Stewart	Jimeng Sun","8","2","2017","In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We perform non-linear causality analysis by causally regularizing a special neural network architecture. We also show that the proposed causal regularizer can be used together with neural representation learning algorithms to yield up to 20% improvement over multilayer perceptron in detecting multivariate causation, a situation common in healthcare, where many causal factors should occur simultaneously to have an effect on the target variable.","In application domain such a healthcare , we want accurate predictive model that be also causally interpretable . In pursuit of such model , we propose a causal regularizer to steer predictive model towards causally-interpretable solution and theoretically study it property . In a large-scale analysis of Electronic Health Records ( EHR ) , our causally-regularized model outperform it L1-regularized counterpart in causal accuracy and be competitive in predictive performance . We perform non-linear causality analysis by causally regularize a special neural network architecture . We also show that the propose causal regularizer can be use together with neural representation learn algorithm to yield up to 20 % improvement over multilayer perceptron in detect multivariate causation , a situation common in healthcare , where many causal factor should occur simultaneously to have an effect on the target variable . ","http://arxiv.org/pdf/1702.02604v2","cs.LG	cs.AI	cs.NE	stat.ML","Causal Regularization",""
"Dario Garcia-Gasulla	Ferran ParÃ©s	Armand Vilalta	Jonatan Moreno	Eduard AyguadÃ©	JesÃºs Labarta	Ulises CortÃ©s	Toyotaro Suzumura","3","3","2017","Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classifier. This approach has provided consistent results, although its relevance is limited to classification tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces.","Deep neural network be representation learn technique . During training , a deep net be capable of generate a descriptive language of unprecedented size and detail in machine learning . Extracting the descriptive language cod within a trained CNN model ( in the case of image data ) , and reuse it for other purpose be a field of interest , a it provide access to the visual descriptor previously learn by the CNN after process million of image , without require an expensive training phase . Contributions to this field ( commonly know a feature representation transfer or transfer learn ) have be purely empirical so far , extract all CNN feature from a single layer close to the output and test their performance by feed them to a classifier . This approach have provide consistent result , although it relevance be limit to classification task . In a completely different approach , in this paper we statistically measure the discriminative power of every single feature find within a deep CNN , when use for characterize every class of 11 datasets . We seek to provide new insight into the behavior of CNN feature , particularly the one from convolutional layer , a this can be relevant for their application to knowledge representation and reasoning . Our result confirm that low and middle level feature may behave differently to high level feature , but only under certain condition . We find that all CNN feature can be use for knowledge representation purpose both by their presence or by their absence , double the information a single CNN feature may provide . We also study how much noise these feature may include , and propose a thresholding approach to discard most of it . All these insight have a direct application to the generation of CNN embed space . ","http://arxiv.org/pdf/1703.01127v4","cs.NE	cs.AI	cs.LG	stat.ML","On the Behavior of Convolutional Nets for Feature Extraction","dario.garcia@bsc.es	ferran.pares@bsc.es	armand.vilalta@bsc.es"
"Aditya Grover	Manik Dhar	Stefano Ermon","24","5","2017","Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.","Adversarial learning of probabilistic model have recently emerge a a promising alternative to maximum likelihood . Implicit model such a generative adversarial network ( GAN ) often generate good sample compare to explicit model train by maximum likelihood . Yet , GANs sidestep the characterization of an explicit density which make quantitative evaluation challenge . To bridge this gap , we propose Flow-GANs , a generative adversarial network for which we can perform exact likelihood evaluation , thus support both adversarial and maximum likelihood training . When train adversarially , Flow-GANs generate high-quality sample but attain extremely poor log-likelihood score , inferior even to a mixture model memorize the training data ; the opposite be true when train by maximum likelihood . Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihood while retain visual fidelity in the generated sample . ","http://arxiv.org/pdf/1705.08868v2","cs.LG	cs.AI	cs.NE	stat.ML","Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in   Generative Models","adityag@cs.stanford.edu	dmanik@cs.stanford.edu	ermon@cs.stanford.edu"
"Chris J. Maddison	Dieterich Lawson	George Tucker	Nicolas Heess	Mohammad Norouzi	Andriy Mnih	Arnaud Doucet	Yee Whye Teh","25","5","2017","When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.","When use a a surrogate objective for maximum likelihood estimation in latent variable model , the evidence low bound ( ELBO ) produce state-of-the-art result . Inspired by this , we consider the extension of the ELBO to a family of low bound define by a particle filter 's estimator of the marginal likelihood , the filter variational objective ( FIVOs ) . FIVOs take the same argument a the ELBO , but can exploit a model 's sequential structure to form tight bound . We present result that relate the tightness of FIVO 's bound to the variance of the particle filter 's estimator by consider the generic case of bound define a log-transformed likelihood estimator . Experimentally , we show that train with FIVO result in substantial improvement over train the same model architecture with the ELBO on sequential data . ","http://arxiv.org/pdf/1705.09279v3","cs.LG	cs.AI	cs.NE	stat.ML","Filtering Variational Objectives","cmaddis@google.com	dieterichl@google.com	gjt@google.com"
"Jiaxin Shi	Shengyang Sun	Jun Zhu","29","5","2017","Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.","Recent progress in variational inference have pay much attention to the flexibility of variational posterior . One promising direction be to use implicit distribution , i.e. , distribution without tractable density a the variational posterior . However , exist method on implicit posterior still face challenge of noisy estimation and computational infeasibility when apply to model with high-dimensional latent variable . In this paper , we present a new approach name Kernel Implicit Variational Inference that address these challenge . As far a we know , for the first time implicit variational inference be successfully apply to Bayesian neural network , which show promise result on both regression and classification task . ","http://arxiv.org/pdf/1705.10119v3","stat.ML	cs.AI	cs.LG	cs.NE","Kernel Implicit Variational Inference","shijx15@mails.tsinghua.edu.cn,	ssy@cs.toronto.edu,	dcszj@tsinghua.edu.cn"
"Julien Perez	Tomi Silander","31","5","2017","Partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards. Despite numerous attempts during the two last decades, the majority of reinforcement learning algorithms and associated approximate models, applied to this context, still assume Markovian state transitions. In this paper, we explore the use of a recently proposed attention-based model, the Gated End-to-End Memory Network, for sequential control. We call the resulting model the Gated End-to-End Memory Policy Network. More precisely, we use a model-free value-based algorithm to learn policies for partially observed domains using this memory-enhanced neural network. This model is end-to-end learnable and it features unbounded memory. Indeed, because of its attention mechanism and associated non-parametric memory, the proposed model allows us to define an attention mechanism over the observation stream unlike recurrent models. We show encouraging results that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading. We also present an OpenAI Gym environment for simulated stock exchange and explain its relevance as a benchmark for the field of non-Markovian decision process learning.","Partially observable environment present an important open challenge in the domain of sequential control learn with delayed reward . Despite numerous attempt during the two last decade , the majority of reinforcement learn algorithm and associate approximate model , apply to this context , still assume Markovian state transition . In this paper , we explore the use of a recently propose attention-based model , the Gated End-to-End Memory Network , for sequential control . We call the result model the Gated End-to-End Memory Policy Network . More precisely , we use a model-free value-based algorithm to learn policy for partially observed domain use this memory-enhanced neural network . This model be end-to-end learnable and it feature unbounded memory . Indeed , because of it attention mechanism and associate non-parametric memory , the propose model allow u to define an attention mechanism over the observation stream unlike recurrent model . We show encouraging result that illustrate the capability of our attention-based model in the context of the continuous-state non-stationary control problem of stock trading . We also present an OpenAI Gym environment for simulate stock exchange and explain it relevance a a benchmark for the field of non-Markovian decision process learning . ","http://arxiv.org/pdf/1705.10993v1","stat.ML	cs.AI	cs.LG	cs.NE","Non-Markovian Control with Gated End-to-End Memory Policy Networks","julien.perez@xrce.xerox.com	tomi.silander@xrce.xerox.com"
"Emmanuel Dufourq	Bruce A. Bassett","3","7","2017","Regression or classification? This is perhaps the most basic question faced when tackling a new supervised learning problem. We present an Evolutionary Deep Learning (EDL) algorithm that automatically solves this by identifying the question type with high accuracy, along with a proposed deep architecture. Typically, a significant amount of human insight and preparation is required prior to executing machine learning algorithms. For example, when creating deep neural networks, the number of parameters must be selected in advance and furthermore, a lot of these choices are made based upon pre-existing knowledge of the data such as the use of a categorical cross entropy loss function. Humans are able to study a dataset and decide whether it represents a classification or a regression problem, and consequently make decisions which will be applied to the execution of the neural network. We propose the Automated Problem Identification (API) algorithm, which uses an evolutionary algorithm interface to TensorFlow to manipulate a deep neural network to decide if a dataset represents a classification or a regression problem. We test API on 16 different classification, regression and sentiment analysis datasets with up to 10,000 features and up to 17,000 unique target values. API achieves an average accuracy of $96.3\%$ in identifying the problem type without hardcoding any insights about the general characteristics of regression or classification problems. For example, API successfully identifies classification problems even with 1000 target values. Furthermore, the algorithm recommends which loss function to use and also recommends a neural network architecture. Our work is therefore a step towards fully automated machine learning.","Regression or classification ? This be perhaps the most basic question face when tackle a new supervise learning problem . We present an Evolutionary Deep Learning ( EDL ) algorithm that automatically solve this by identify the question type with high accuracy , along with a propose deep architecture . Typically , a significant amount of human insight and preparation be require prior to execute machine learn algorithm . For example , when create deep neural network , the number of parameter must be select in advance and furthermore , a lot of these choice be make base upon pre-existing knowledge of the data such a the use of a categorical cross entropy loss function . Humans be able to study a dataset and decide whether it represent a classification or a regression problem , and consequently make decision which will be apply to the execution of the neural network . We propose the Automated Problem Identification ( API ) algorithm , which use an evolutionary algorithm interface to TensorFlow to manipulate a deep neural network to decide if a dataset represent a classification or a regression problem . We test API on 16 different classification , regression and sentiment analysis datasets with up to 10,000 feature and up to 17,000 unique target value . API achieve an average accuracy of $ 96.3\ % $ in identify the problem type without hardcoding any insight about the general characteristic of regression or classification problem . For example , API successfully identify classification problem even with 1000 target value . Furthermore , the algorithm recommend which loss function to use and also recommend a neural network architecture . Our work be therefore a step towards fully automate machine learning . ","http://arxiv.org/pdf/1707.00703v1","cs.NE	cs.AI	cs.LG	stat.ML","Automated Problem Identification: Regression vs Classification via   Evolutionary Deep Networks","edufourq@gmail.com,	bruce.a.bassett@gmail.com"
"Nikhil Mishra	Mostafa Rohaninejad	Xi Chen	Pieter Abbeel","11","7","2017","Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.","Deep neural network excel in regime with large amount of data , but tend to struggle when data be scarce or when they need to adapt quickly to change in the task . In response , recent work in meta-learning proposes train a meta-learner on a distribution of similar task , in the hope of generalization to novel but related task by learn a high-level strategy that capture the essence of the problem it be ask to solve . However , many recent meta-learning approach be extensively hand-designed , either use architecture specialize to a particular application , or hard-coding algorithmic component that constrain how the meta-learner solve the task . We propose a class of simple and generic meta-learner architecture that use a novel combination of temporal convolution and soft attention ; the former to aggregate information from past experience and the latter to pinpoint specific piece of information . In the most extensive set of meta-learning experiment to date , we evaluate the result Simple Neural AttentIve Learner ( or SNAIL ) on several heavily-benchmarked task . On all task , in both supervised and reinforcement learning , SNAIL attain state-of-the-art performance by significant margin . ","http://arxiv.org/pdf/1707.03141v3","cs.AI	cs.LG	cs.NE	stat.ML","A Simple Neural Attentive Meta-Learner","nmishra@berkeley.edu	rohaninejadm@berkeley.edu	c.xi@berkeley.edu	pabbeel@berkeley.edu"
"Simone Scardapane	Steven Van Vaerenbergh	Simone Totaro	Aurelio Uncini","13","7","2017","Neural networks are generally built by interleaving (adaptable) linear layers with (fixed) nonlinear activation functions. To increase their flexibility, several authors have proposed methods for adapting the activation functions themselves, endowing them with varying degrees of flexibility. None of these approaches, however, have gained wide acceptance in practice, and research in this topic remains open. In this paper, we introduce a novel family of flexible activation functions that are based on an inexpensive kernel expansion at every neuron. Leveraging over several properties of kernel-based models, we propose multiple variations for designing and initializing these kernel activation functions (KAFs), including a multidimensional scheme allowing to nonlinearly combine information from different paths in the network. The resulting KAFs can approximate any mapping defined over a subset of the real line, either convex or nonconvex. Furthermore, they are smooth over their entire domain, linear in their parameters, and they can be regularized using any known scheme, including the use of $\ell_1$ penalties to enforce sparseness. To the best of our knowledge, no other known model satisfies all these properties simultaneously. In addition, we provide a relatively complete overview on alternative techniques for adapting the activation functions, which is currently lacking in the literature. A large set of experiments validates our proposal.","Neural network be generally build by interleave ( adaptable ) linear layer with ( fix ) nonlinear activation function . To increase their flexibility , several author have propose method for adapt the activation function themselves , endow them with vary degree of flexibility . None of these approach , however , have gain wide acceptance in practice , and research in this topic remain open . In this paper , we introduce a novel family of flexible activation function that be base on an inexpensive kernel expansion at every neuron . Leveraging over several property of kernel-based model , we propose multiple variation for design and initialize these kernel activation function ( KAFs ) , include a multidimensional scheme allow to nonlinearly combine information from different path in the network . The result KAFs can approximate any mapping define over a subset of the real line , either convex or nonconvex . Furthermore , they be smooth over their entire domain , linear in their parameter , and they can be regularize use any known scheme , include the use of $ \ell_1 $ penalty to enforce sparseness . To the best of our knowledge , no other know model satisfies all these property simultaneously . In addition , we provide a relatively complete overview on alternative technique for adapt the activation function , which be currently lack in the literature . A large set of experiment validate our proposal . ","http://arxiv.org/pdf/1707.04035v2","stat.ML	cs.AI	cs.LG	cs.NE","Kafnets: kernel-based non-parametric activation functions for neural   networks",""
"Razvan Pascanu	Yujia Li	Oriol Vinyals	Nicolas Heess	Lars Buesing	Sebastien RacaniÃ¨re	David Reichert	ThÃ©ophane Weber	Daan Wierstra	Peter Battaglia","19","7","2017","Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the "Imagination-based Planner", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable number of imagination steps, which involve proposing an imagined action and evaluating it with its model-based imagination. All imagined actions and outcomes are aggregated, iteratively, into a "plan context" which conditions future real and imagined actions. The agent can even decide how to imagine: testing out alternative imagined actions, chaining sequences of actions together, or building a more complex "imagination tree" by navigating flexibly among the previously imagined states using a learned policy. And our agent can learn to plan economically, jointly optimizing for external rewards and computational costs associated with using its imagination. We show that our architecture can learn to solve a challenging continuous control problem, and also learn elaborate planning strategies in a discrete maze-solving task. Our work opens a new direction toward learning the components of a model-based planning system and how to use them.","Conventional wisdom hold that model-based planning be a powerful approach to sequential decision-making . It be often very challenge in practice , however , because while a model can be use to evaluate a plan , it do not prescribe how to construct a plan . Here we introduce the `` Imagination-based Planner '' , the first model-based , sequential decision-making agent that can learn to construct , evaluate , and execute plan . Before any action , it can perform a variable number of imagination step , which involve propose an imagined action and evaluate it with it model-based imagination . All imagined action and outcome be aggregate , iteratively , into a `` plan context '' which condition future real and imagined action . The agent can even decide how to imagine : test out alternative imagined action , chain sequence of action together , or build a more complex `` imagination tree '' by navigate flexibly among the previously imagine state use a learned policy . And our agent can learn to plan economically , jointly optimize for external reward and computational cost associate with use it imagination . We show that our architecture can learn to solve a challenge continuous control problem , and also learn elaborate planning strategy in a discrete maze-solving task . Our work open a new direction toward learn the component of a model-based planning system and how to use them . ","http://arxiv.org/pdf/1707.06170v1","cs.AI	cs.LG	cs.NE	stat.ML","Learning model-based planning from scratch","razp@google.com	yujiali@google.com	vinyals@google.com	heess@google.com	lbuesing@google.com	sracaniere@google.com	@google.com	 reichert@google.com	theophane@google.com	wierstra@google.com	peterbattaglia@google.com"
"Isabeau PrÃ©mont-Schwarz	Alexander Ilin	Tele Hotloo Hao	Antti Rasmus	Rinu Boney	Harri Valpola","28","7","2017","We propose a recurrent extension of the Ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.","We propose a recurrent extension of the Ladder network whose structure be motivate by the inference require in hierarchical latent variable model . We demonstrate that the recurrent Ladder be able to handle a wide variety of complex learn task that benefit from iterative inference and temporal modeling . The architecture show close-to-optimal result on temporal modeling of video data , competitive result on music modeling , and improve perceptual grouping base on high order abstraction , such a stochastic texture and motion cue . We present result for fully supervise , semi-supervised , and unsupervised task . The result suggest that the propose architecture and principle be powerful tool for learn a hierarchy of abstraction , learn iterative inference and handle temporal information . ","http://arxiv.org/pdf/1707.09219v4","cs.NE	cs.AI	cs.LG	stat.ML","Recurrent Ladder Networks","isabeau@cai.fi	alexilin@cai.fi	hotloo@cai.fi	antti@cai.fi	rinu@cai.fi	harri@cai.fi"
"Kenji Kawaguchi	Leslie Pack Kaelbling	Yoshua Bengio","16","10","2017","With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.","With a direct analysis of neural network , this paper present a mathematically tight generalization theory to partially address an open problem regard the generalization of deep learning . Unlike previous bound-based theory , our main theory be quantitatively as tight a possible for every dataset individually , while produce qualitative insight competitively . Our result give insight into why and how deep learning can generalize well , despite it large capacity , complexity , possible algorithmic instability , nonrobustness , and sharp minimum , answer to an open question in the literature . We also discuss limitation of our result and propose additional open problem . ","http://arxiv.org/pdf/1710.05468v3","stat.ML	cs.AI	cs.LG	cs.NE","Generalization in Deep Learning",""
"Yannic Kilcher	Gary Becigneul	Thomas Hofmann","31","10","2017","It is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, there is a need for methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space.","It be commonly agree that the use of relevant invariance a a good statistical bias be important in machine-learning . However , most approach that explicitly incorporate invariance into a model architecture only make use of very simple transformation , such a translation and rotation . Hence , there be a need for method to model and extract rich transformation that capture much higher-level invariance . To that end , we introduce a tool allow to parametrize the set of filter of a trained convolutional neural network with the latent space of a generative adversarial network . We then show that the method can capture highly non-linear invariance of the data by visualize their effect in the data space . ","http://arxiv.org/pdf/1710.11386v1","cs.LG	cs.AI	cs.NE	stat.ML","Parametrizing filters of a CNN with a GAN","yannic.kilcher@inf.ethz.ch	gary.becigneul@inf.ethz.ch	thomas.hofmann@inf.ethz.ch"
"Zhen He	Shaobing Gao	Liang Xiao	Daxue Liu	Hangen He	David Barber","5","11","2017","Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.","Long Short-Term Memory ( LSTM ) be a popular approach to boost the ability of Recurrent Neural Networks to store long term temporal information . The capacity of an LSTM network can be increase by widen and add layer . However , usually the former introduces additional parameter , while the latter increase the runtime . As an alternative we propose the Tensorized LSTM in which the hidden state be represent by tensor and updated via a cross-layer convolution . By increase the tensor size , the network can be widen efficiently without additional parameter since the parameter be share across different location in the tensor ; by delay the output , the network can be deepen implicitly with little additional runtime since deep computation for each timestep be merge into temporal computation of the sequence . Experiments conduct on five challenge sequence learn task show the potential of the propose model . ","http://arxiv.org/pdf/1711.01577v3","stat.ML	cs.AI	cs.LG	cs.NE","Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence   Learning",""
"Shruti R. Kulkarni	John M. Alexiades	Bipin Rajendran","9","11","2017","We describe a novel spiking neural network (SNN) for automated, real-time handwritten digit classification and its implementation on a GP-GPU platform. Information processing within the network, from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain. The feature extraction layer of the SNN uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed NormAD approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights. On the standard MNIST database images of handwritten digits, our network achieves an accuracy of 99.80% on the training set and 98.06% on the test set, with nearly 7x fewer parameters compared to the state-of-the-art spiking networks. We further use this network in a GPU based user-interface system demonstrating real-time SNN simulation to infer digits written by different users. On a test set of 500 such images, this real-time platform achieves an accuracy exceeding 97% while making a prediction within an SNN emulation time of less than 100ms.","We describe a novel spike neural network ( SNN ) for automate , real-time handwritten digit classification and it implementation on a GP-GPU platform . Information processing within the network , from feature extraction to classification be implement by mimic the basic aspect of neuronal spike initiation and propagation in the brain . The feature extraction layer of the SNN use fix synaptic weight map to extract the key feature of the image and the classifier layer use the recently develop NormAD approximate gradient descent base supervise learn algorithm for spike neural network to adjust the synaptic weight . On the standard MNIST database image of handwritten digit , our network achieve an accuracy of 99.80 % on the training set and 98.06 % on the test set , with nearly 7x few parameter compare to the state-of-the-art spiking network . We far use this network in a GPU base user-interface system demonstrate real-time SNN simulation to infer digit write by different user . On a test set of 500 such image , this real-time platform achieve an accuracy exceed 97 % while make a prediction within an SNN emulation time of less than 100ms . ","http://arxiv.org/pdf/1711.03637v1","stat.ML	cs.AI	cs.LG	cs.NE","Learning and Real-time Classification of Hand-written Digits With   Spiking Neural Networks","srk68@njit.edu	jma59@njit.edu	bipin@njit.edu"
"Joan SerrÃ 	DÃ­dac SurÃ­s	Marius Miron	Alexandros Karatzoglou","4","1","2018","Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.","Catastrophic forgetting occur when a neural network lose the information learn in a previous task after train on subsequent task . This problem remain a hurdle for artificial intelligence system with sequential learn capability . In this paper , we propose a task-based hard attention mechanism that preserve previous task ' information without affect the current task 's learning . A hard attention mask be learn concurrently to every task , through stochastic gradient descent , and previous mask be exploit to condition such learning . We show that the propose mechanism be effective for reduce catastrophic forgetting , cut current rate by 45 to 80 % . We also show that it be robust to different hyperparameter choice , and that it offer a number of monitor capability . The approach feature the possibility to control both the stability and compactness of the learned knowledge , which we believe make it also attractive for online learning or network compression application . ","http://arxiv.org/pdf/1801.01423v2","cs.LG	cs.AI	cs.NE	stat.ML","Overcoming catastrophic forgetting with hard attention to the task",""
"Zachary C. Lipton	Yu-Xiang Wang	Alex Smola","12","2","2018","Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal $p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.","Faced with distribution shift between training and test set , we wish to detect and quantify the shift , and to correct our classifier without test set label . Motivated by medical diagnosis , where disease ( target ) , cause symptom ( observation ) , we focus on label shift , where the label marginal $ p ( y ) $ change but the conditional $ p ( x|y ) $ do not . We propose Black Box Shift Estimation ( BBSE ) to estimate the test distribution $ p ( y ) $ . BBSE exploit arbitrary black box predictor to reduce dimensionality prior to shift correction . While good predictor give tighter estimate , BBSE work even when predictor be bias , inaccurate , or uncalibrated , so long a their confusion matrix be invertible . We prove BBSE 's consistency , bind it error , and introduce a statistical test that use BBSE to detect shift . We also leverage BBSE to correct classifier . Experiments demonstrate accurate estimate and improve prediction , even on high-dimensional datasets of natural image . ","http://arxiv.org/pdf/1802.03916v2","cs.LG	cs.AI	cs.NE	stat.ML","Detecting and Correcting for Label Shift with Black Box Predictors","zlipton@cmu.edu,	yuxiangw@amazon.com,	smola@amazon.com"
"Kenji Kawaguchi	Yoshua Bengio","21","2","2018","This paper introduces a novel measure-theoretic learning theory to analyze generalization behaviors of practical interest. The proposed learning theory has the following abilities: 1) to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning, 2) to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces (e.g., arbitrarily large capacity and Rademacher complexity) and non-stable/non-robust learning algorithms, and 3) to clearly distinguish each individual problem instance from each other. Our generalization bounds are relative to a representation of the data, and hold true even if the representation is learned. We discuss several consequences of our results on deep learning, one-shot learning and curriculum learning. Unlike statistical learning theory, the proposed learning theory analyzes each problem instance individually via measure theory, rather than a set of problem instances via statistics. Because of the differences in the assumptions and the objectives, the proposed learning theory is meant to be complementary to previous learning theory and is not designed to compete with it.","This paper introduce a novel measure-theoretic learn theory to analyze generalization behavior of practical interest . The propose learning theory have the following ability : 1 ) to utilize the quality of each learn representation on the path from raw input to output in representation learning , 2 ) to guarantee good generalization error possibly with arbitrarily rich hypothesis space ( e.g. , arbitrarily large capacity and Rademacher complexity ) and non-stable/non-robust learning algorithm , and 3 ) to clearly distinguish each individual problem instance from each other . Our generalization bound be relative to a representation of the data , and hold true even if the representation be learn . We discuss several consequence of our result on deep learning , one-shot learning and curriculum learning . Unlike statistical learning theory , the propose learning theory analyze each problem instance individually via measure theory , rather than a set of problem instance via statistic . Because of the difference in the assumption and the objective , the propose learning theory be mean to be complementary to previous learn theory and be not design to compete with it . ","http://arxiv.org/pdf/1802.07426v1","stat.ML	cs.AI	cs.LG	cs.NE","Generalization in Machine Learning via Analytical Learning Theory",""
"Roman Novak	Yasaman Bahri	Daniel A. Abolafia	Jeffrey Pennington	Jascha Sohl-Dickstein","23","2","2018","In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets.   We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization $-$ such as full-batch training or using random labels $-$ correspond to lower robustness, while factors associated with good generalization $-$ such as data augmentation and ReLU non-linearities $-$ give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.","In practice it be often find that large over-parameterized neural network generalize good than their small counterpart , an observation that appear to conflict with classical notion of function complexity , which typically favor small model . In this work , we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metric of complexity relate to sensitivity to input perturbation . Our experiment survey thousand of model with various fully-connected architecture , optimizers , and other hyper-parameters , as well a four different image classification datasets . We find that train neural network be more robust to input perturbation in the vicinity of the training data manifold , a measure by the norm of the input-output Jacobian of the network , and that it correlate well with generalization . We further establish that factor associate with poor generalization $ - $ such a full-batch training or use random label $ - $ correspond to low robustness , while factor associate with good generalization $ - $ such a data augmentation and ReLU non-linearities $ - $ give rise to more robust function . Finally , we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test point . ","http://arxiv.org/pdf/1802.08760v1","stat.ML	cs.AI	cs.LG	cs.NE","Sensitivity and Generalization in Neural Networks: an Empirical Study","romann@google.com	yasamanb@google.com	danabo@google.com	jpennin@google.com	jaschasd@google.com"
"Ari S. Morcos	David G. T. Barrett	Neil C. Rabinowitz	Matthew Botvinick","19","3","2018","Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.","Despite their ability to memorize large datasets , deep neural network often achieve good generalization performance . However , the difference between the learned solution of network which generalize and those which do not remain unclear . Additionally , the tune property of single direction ( define a the activation of a single unit or some linear combination of unit in response to some input ) have be highlight , but their importance have not be evaluate . Here , we connect these line of inquiry to demonstrate that a network 's reliance on single direction be a good predictor of it generalization performance , across network train on datasets with different fraction of corrupted label , across ensemble of network train on datasets with unmodified label , across different hyperparameters , and over the course of training . While dropout only regularize this quantity up to a point , batch normalization implicitly discourage single direction reliance , in part by decrease the class selectivity of individual unit . Finally , we find that class selectivity be a poor predictor of task importance , suggest not only that network which generalize well minimize their dependence on individual unit by reduce their selectivity , but also that individually selective unit may not be necessary for strong network performance . ","http://arxiv.org/pdf/1803.06959v1","stat.ML	cs.AI	cs.LG	cs.NE","On the importance of single directions for generalization","arimorcos@google.com	barrettdavid@google.com	ncr@google.com	botvinick@google.com	arimorcos@google.com"
"Srinivas C. Turaga	Kevin L. Briggman	Moritz Helmstaedter	Winfried Denk	H. Sebastian Seung","28","11","2009","Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph. We present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the Rand index, a well known segmentation performance measure. The Rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation. By using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the Rand index of segmentations resulting from the graph partitioning. Our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.","Images can be segment by first use a classifier to predict an affinity graph that reflect the degree to which image pixel must be group together and then partition the graph to yield a segmentation . Machine learning have be apply to the affinity classifier to produce affinity graph that be good in the sense of minimize edge misclassification rate . However , this error measure be only indirectly relate to the quality of segmentation produce by ultimately partition the affinity graph . We present the first machine learning algorithm for train a classifier to produce affinity graph that be good in the sense of produce segmentation that directly minimize the Rand index , a well know segmentation performance measure . The Rand index measure segmentation performance by quantify the classification of the connectivity of image pixel pair after segmentation . By use the simple graph partition algorithm of find the connected component of the thresholded affinity graph , we be able to train an affinity classifier to directly minimize the Rand index of segmentation result from the graph partitioning . Our learn algorithm corresponds to the learning of maximin affinity between image pixel pair , which be predictive of the pixel-pair connectivity . ","http://arxiv.org/pdf/0911.5372v1","cs.CV	cs.AI	cs.LG	cs.NE","Maximin affinity learning of image segmentation",""
"Sergey S. Tarasenko","14","2","2011","This study is focused on the development of the cortex-like visual object recognition system. We propose a general framework, which consists of three hierarchical levels (modules). These modules functionally correspond to the V1, V4 and IT areas. Both bottom-up and top-down connections between the hierarchical levels V4 and IT are employed. The higher the degree of matching between the input and the preferred stimulus, the shorter the response time of the neuron. Therefore information about a single stimulus is distributed in time and is transmitted by the waves of spikes. The reciprocal connections and waves of spikes implement predictive coding: an initial hypothesis is generated on the basis of information delivered by the first wave of spikes and is tested with the information carried by the consecutive waves. The development is considered as extraction and accumulation of features in V4 and objects in IT. Once stored a feature can be disposed, if rarely activated. This cause update of feature repository. Consequently, objects in IT are also updated. This illustrates the growing process and dynamical change of topological structures of V4, IT and connections between these areas.","This study be focus on the development of the cortex-like visual object recognition system . We propose a general framework , which consist of three hierarchical level ( module ) . These module functionally correspond to the V1 , V4 and IT area . Both bottom-up and top-down connection between the hierarchical level V4 and IT be employ . The high the degree of match between the input and the preferred stimulus , the short the response time of the neuron . Therefore information about a single stimulus be distribute in time and be transmit by the wave of spike . The reciprocal connection and wave of spike implement predictive coding : an initial hypothesis be generate on the basis of information deliver by the first wave of spike and be test with the information carry by the consecutive wave . The development be consider a extraction and accumulation of feature in V4 and object in IT . Once store a feature can be dispose , if rarely activate . This cause update of feature repository . Consequently , object in IT be also update . This illustrate the grow process and dynamical change of topological structure of V4 , IT and connection between these area . ","http://arxiv.org/pdf/1102.2739v1","cs.CV	cs.AI	cs.LG	cs.NE","A General Framework for Development of the Cortex-like Visual Object   Recognition System: Waves of Spikes, Predictive Coding and Universal   Dictionary of Features",""
"Dan C. CireÅan	Ueli Meier	Luca M. Gambardella	JÃ¼rgen Schmidhuber","23","3","2011","The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.","The competitive MNIST handwritten digit recognition benchmark have a long history of broken record since 1998 . The most recent substantial improvement by others date back 7 year ( error rate 0.4 % ) . Recently we be able to significantly improve this result , use graphic card to greatly speed up training of simple but deep MLPs , which achieve 0.35 % , outperform all the previous more complex method . Here we report another substantial improvement : 0.31 % obtain use a committee of MLPs . ","http://arxiv.org/pdf/1103.4487v1","cs.LG	cs.AI	cs.CV	cs.NE","Handwritten Digit Recognition with a Committee of Deep Neural Nets on   GPUs",""
"Ridwan Al Iqbal","2","10","2011","Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a "Black Box" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting comprehensible rules from a trained Network. Many such extraction algorithms have been developed over the years with their respective strengths and weaknesses. They have been broadly categorized into three types based on their approach to use internal model of the Network. Eclectic Methods are hybrid algorithms that combine the other approaches to attain more performance. In this paper, we present an Eclectic method called HERETIC. Our algorithm uses Inductive Decision Tree learning combined with information of the neural network structure for extracting logical rules. Experiments and theoretical analysis show HERETIC to be better in terms of speed and performance.","Artificial Neural Network be among the most popular algorithm for supervised learning . However , Neural Networks have a well-known drawback of be a `` Black Box '' learner that be not comprehensible to the Users . This lack of transparency make it unsuitable for many high risk task such a medical diagnosis that require a rational justification for make a decision . Rule Extraction method attempt to curb this limitation by extract comprehensible rule from a trained Network . Many such extraction algorithm have be develop over the year with their respective strength and weakness . They have be broadly categorize into three type base on their approach to use internal model of the Network . Eclectic Methods be hybrid algorithm that combine the other approach to attain more performance . In this paper , we present an Eclectic method call HERETIC . Our algorithm us Inductive Decision Tree learn combine with information of the neural network structure for extract logical rule . Experiments and theoretical analysis show HERETIC to be well in term of speed and performance . ","http://arxiv.org/pdf/1110.0214v1","cs.LG	cs.AI	cs.CV	cs.NE","Eclectic Extraction of Propositional Rules from Neural Networks","ridwan@enosisbd.com"
"Arnab Ghosh	Viveka Kulharia	Vinay Namboodiri","5","12","2016","Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that we can obtain multi-agent GANs that communicate through message passing to achieve better image generation. The objectives of the individual agents in this framework are two fold: a co-operation objective and a competing objective. The co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. We analyze and visualize the messages that these GANs share among themselves in various scenarios. We quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. Qualitatively, we show that the different generators capture different traits of the underlying data distribution.","Communicating and share intelligence among agent be an important facet of achieve Artificial General Intelligence . As a first step towards this challenge , we introduce a novel framework for image generation : Message Passing Multi-Agent Generative Adversarial Networks ( MPM GANs ) . While GANs have recently be show to be very effective for image generation and other task , these network have be limit to mostly single generator-discriminator network . We show that we can obtain multi-agent GANs that communicate through message passing to achieve good image generation . The objective of the individual agent in this framework be two fold : a co-operation objective and a compete objective . The co-operation objective ensure that the message share mechanism guide the other generator to generate good than itself while the compete objective encourages each generator to generate good than it counterpart . We analyze and visualize the message that these GANs share among themselves in various scenario . We quantitatively show that the message share formulation serf a a regularizer for the adversarial training . Qualitatively , we show that the different generator capture different trait of the underlie data distribution . ","http://arxiv.org/pdf/1612.01294v1","cs.CV	cs.AI	cs.LG	cs.NE","Message Passing Multi-Agent GANs","arnabghosh93	vivekakulharia}@gmail.com	vinaypn@iitk.ac.in"
"Tong Che	Yanran Li	Athul Paul Jacob	Yoshua Bengio	Wenjie Li","7","12","2016","Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.","Although Generative Adversarial Networks achieve state-of-the-art result on a variety of generative task , they be regard a highly unstable and prone to miss mode . We argue that these bad behavior of GANs be due to the very particular functional shape of the trained discriminator in high dimensional space , which can easily make train stuck or push probability mass in the wrong direction , towards that of high concentration than that of the data generate distribution . We introduce several way of regularize the objective , which can dramatically stabilize the training of GAN model . We also show that our regularizers can help the fair distribution of probability mass across the mode of the data generate distribution , during the early phase of training and thus provide a unified solution to the miss mode problem . ","http://arxiv.org/pdf/1612.02136v5","cs.LG	cs.AI	cs.CV	cs.NE","Mode Regularized Generative Adversarial Networks","tong.che@umontreal.ca	ap.jacob@umontreal.ca	yoshua.bengio@umontreal.ca	csyli@comp.polyu.edu.hk	cswjli@comp.polyu.edu.hk"
"Bharat Singh	Soham De	Yangmuzi Zhang	Thomas Goldstein	Gavin Taylor","15","10","2015","The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also been shown that in highly non-convex problems, such as deep neural networks, there is a proliferation of high-error low curvature saddle points, which slows down learning dramatically. In this paper, we attempt to overcome the two above problems by proposing an optimization method for training deep neural networks which uses learning rates which are both specific to each layer in the network and adaptive to the curvature of the function, increasing the learning rate at low curvature points. This enables us to speed up learning in the shallow layers of the network and quickly escape high-error low curvature saddle points. We test our method on standard image classification datasets such as MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy as well as reduces the required training time over standard algorithms.","The increase complexity of deep learning architecture be result in training time require week or even month . This slow training be due in part to vanish gradient , in which the gradient use by back-propagation be extremely large for weight connect deep layer ( layer near the output layer ) , and extremely small for shallow layer ( near the input layer ) ; this result in slow learning in the shallow layer . Additionally , it have also be show that in highly non-convex problem , such a deep neural network , there be a proliferation of high-error low curvature saddle point , which slow down learn dramatically . In this paper , we attempt to overcome the two above problem by propose an optimization method for train deep neural network which use learn rate which be both specific to each layer in the network and adaptive to the curvature of the function , increase the learning rate at low curvature point . This enable u to speed up learn in the shallow layer of the network and quickly escape high-error low curvature saddle point . We test our method on standard image classification datasets such a MNIST , CIFAR10 and ImageNet , and demonstrate that our method increase accuracy as well a reduces the required training time over standard algorithm . ","http://arxiv.org/pdf/1510.04609v1","cs.CV	cs.AI	cs.LG	cs.NE","Layer-Specific Adaptive Learning Rates for Deep Networks","bharat	sohamde	tomg}@cs.umd.edu	ymzhang@umiacs.umd.edu,	taylor@usna.edu"
"Baochen Sun	Jiashi Feng	Kate Saenko","17","11","2015","Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being "frustratingly easy" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.","Unlike human learning , machine learning often fail to handle change between training ( source ) and test ( target ) input distribution . Such domain shift , common in practical scenario , severely damage the performance of conventional machine learn method . Supervised domain adaptation method have be propose for the case when the target data have label , include some that perform very well despite be `` frustratingly easy '' to implement . However , in practice , the target domain be often unlabeled , require unsupervised adaptation . We propose a simple , effective , and efficient method for unsupervised domain adaptation call CORrelation ALignment ( CORAL ) . CORAL minimize domain shift by align the second-order statistic of source and target distribution , without require any target label . Even though it be extraordinarily simple -- it can be implement in four line of Matlab code -- CORAL performs remarkably well in extensive evaluation on standard benchmark datasets . ","http://arxiv.org/pdf/1511.05547v2","cs.CV	cs.AI	cs.LG	cs.NE","Return of Frustratingly Easy Domain Adaptation","bsun@cs.uml.edu	elefjia@nus.edu.sg	saenko@cs.uml.edu"
"Lukas Cavigelli	Luca Benini","14","12","2015","An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architecture, design and implementation as well as the first reported silicon measurements of such an accelerator, outperforming previous work in terms of power-, area- and I/O-efficiency. The manufactured device provides up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it the first architecture scalable to TOp/s performance.","An ever increase number of computer vision and image/video processing challenge be be approach use deep convolutional neural network , obtain state-of-the-art result in object recognition and detection , semantic segmentation , action recognition , optical flow and superresolution . Hardware acceleration of these algorithm be essential to adopt these improvement in embedded and mobile computer vision system . We present a new architecture , design and implementation as well a the first report silicon measurement of such an accelerator , outperform previous work in term of power- , area- and I/O-efficiency . The manufactured device provide up to 196 GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power efficiency of 803 GOp/s/W . The massively reduce bandwidth requirement make it the first architecture scalable to TOp/s performance . ","http://arxiv.org/pdf/1512.04295v2","cs.CV	cs.AI	cs.LG	cs.NE	B.7.1; I.2.6","Origami: A 803 GOp/s/W Convolutional Network Accelerator",""
"Aravind S. Lakshminarayanan	Ramnandan Krishnamurthy	Peeyush Kumar	Balaraman Ravindran","17","5","2016","This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch. We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states. The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure. Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states. The connectivity information from PCCA+ is used to generate these skills or options. These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model. This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate. We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.","This paper introduce an automated skill acquisition framework in reinforcement learning which involve identify a hierarchical description of the give task in term of abstract state and extend action between abstract state . Identifying such structure present in the task provide way to simplify and speed up reinforcement learn algorithm . These structure also help to generalize such algorithm over multiple task without relearn policy from scratch . We use idea from dynamical system to find metastable region in the state space and associate them with abstract state . The spectral clustering algorithm PCCA+ be use to identify suitable abstraction align to the underlying structure . Skills be define in term of the sequence of action that lead to transition between such abstract state . The connectivity information from PCCA+ be use to generate these skill or option . These skill be independent of the learning task and can be efficiently reuse across a variety of task define over the same model . This approach work well even without the exact model of the environment by use sample trajectory to construct an approximate estimate . We also present our approach to scale the skill acquisition framework to complex task with large state space for which we perform state aggregation use the representation learn from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space . ","http://arxiv.org/pdf/1605.05359v2","cs.LG	cs.AI	cs.CV	cs.NE","Option Discovery in Hierarchical Reinforcement Learning using   Spatio-Temporal Clustering","aravindsrinivas@gmail.com	nandparikrish@gmail.com	agoovi@gmail.com	ravi@cse.iitm.ac.in"
"Andreas Veit	Michael Wilber	Serge Belongie","20","5","2016","In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.","In this work we propose a novel interpretation of residual network show that they can be see a a collection of many path of differ length . Moreover , residual network seem to enable very deep network by leverage only the short path during training . To support this observation , we rewrite residual network a an explicit collection of path . Unlike traditional model , path through residual network vary in length . Further , a lesion study reveals that these path show ensemble-like behavior in the sense that they do not strongly depend on each other . Finally , and most surprising , most path be short than one might expect , and only the short path be need during training , a long path do not contribute any gradient . For example , most of the gradient in a residual network with 110 layer come from path that be only 10-34 layer deep . Our result reveal one of the key characteristic that seem to enable the training of very deep network : Residual network avoid the vanishing gradient problem by introduce short path which can carry gradient throughout the extent of very deep network . ","http://arxiv.org/pdf/1605.06431v2","cs.CV	cs.AI	cs.LG	cs.NE","Residual Networks Behave Like Ensembles of Relatively Shallow Networks","av443@cornell.edu	mjw285@cornell.edu	sjb344@cornell.edu"
"Anh Nguyen	Alexey Dosovitskiy	Jason Yosinski	Thomas Brox	Jeff Clune","30","5","2016","Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).","Deep neural network ( DNNs ) have demonstrate state-of-the-art result on many pattern recognition task , especially vision classification problem . Understanding the inner working of such computational brain be both fascinate basic science that be interest in it own right - similar to why we study the human brain - and will enable researcher to further improve DNNs . One path to understand how a neural network function internally be to study what each of it neuron have learn to detect . One such method be call activation maximization ( AM ) , which synthesize an input ( e.g . an image ) that highly activate a neuron . Here we dramatically improve the qualitative state of the art of activation maximization by harness a powerful , learned prior : a deep generator network ( DGN ) . The algorithm ( 1 ) generates qualitatively state-of-the-art synthetic image that look almost real , ( 2 ) reveal the feature learn by each neuron in an interpretable way , ( 3 ) generalizes well to new datasets and somewhat well to different network architecture without require the prior to be relearn , and ( 4 ) can be consider a a high-quality generative method ( in this case , by generate novel , creative , interesting , recognizable image ) . ","http://arxiv.org/pdf/1605.09304v5","cs.NE	cs.AI	cs.CV	cs.LG","Synthesizing the preferred inputs for neurons in neural networks via   deep generator networks","anguyen8@uwyo.edu	dosovits@cs.uni-freiburg.de	jason@geometric.ai	brox@cs.uni-freiburg.de	jeffclune@uwyo.edu"
"Rathinakumar Appuswamy	Tapan Nayak	John Arthur	Steven Esser	Paul Merolla	Jeffrey Mckinstry	Timothy Melano	Myron Flickner	Dharmendra Modha","8","6","2016","We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.","We derive a relationship between network representation in energy-efficient neuromorphic architecture and block Toplitz convolutional matrix . Inspired by this connection , we develop deep convolutional network use a family of structured convolutional matrix and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition task . We also put forward a novel method to train binary convolutional network by utilise an exist connection between noisy-rectified linear unit and binary activation . ","http://arxiv.org/pdf/1606.02407v1","cs.NE	cs.AI	cs.CV	cs.LG","Structured Convolution Matrices for Energy-efficient Deep learning","rappusw@us.ibm.com	tknayak@us.ibm.com	arthurjo@us.ibm.com	sesser@us.ibm.com	pameroll@us.ibm.com	jlmckins@us.ibm.com	tmelano@us.ibm.com	mdflickner@us.ibm.com	dmodha@us.ibm.com"
"Baochen Sun	Kate Saenko","6","7","2016","Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.","Deep neural network be able to learn powerful representation from large quantity of labeled input data , however they can not always generalize well across change in input distribution . Domain adaptation algorithm have be propose to compensate for the degradation in performance due to domain shift . In this paper , we address the case when the target domain be unlabeled , require unsupervised adaptation . CORAL be a `` frustratingly easy '' unsupervised domain adaptation method that align the second-order statistic of the source and target distribution with a linear transformation . Here , we extend CORAL to learn a nonlinear transformation that aligns correlation of layer activation in deep neural network ( Deep CORAL ) . Experiments on standard benchmark datasets show state-of-the-art performance . ","http://arxiv.org/pdf/1607.01719v1","cs.CV	cs.AI	cs.LG	cs.NE","Deep CORAL: Correlation Alignment for Deep Domain Adaptation",""
"Jun Liu	Amir Shahroudy	Dong Xu	Gang Wang","24","7","2016","3D action recognition - analysis of human actions based on 3D skeleton data - becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.","3D action recognition - analysis of human action base on 3D skeleton data - becomes popular recently due to it succinctness , robustness , and view-invariant representation . Recent attempt on this problem suggest to develop RNN-based learn method to model the contextual dependency in the temporal domain . In this paper , we extend this idea to spatio-temporal domain to analyze the hidden source of action-related information within the input data over both domain concurrently . Inspired by the graphical structure of the human skeleton , we further propose a more powerful tree-structure base traversal method . To handle the noise and occlusion in 3D skeleton data , we introduce new gate mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust it effect on update the long-term context information store in the memory cell . Our method achieves state-of-the-art performance on 4 challenge benchmark datasets for 3D human action analysis . ","http://arxiv.org/pdf/1607.07043v1","cs.CV	cs.AI	cs.LG	cs.NE","Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition","jliu029@ntu.edu.sg	amir3@ntu.edu.sg	wanggang@ntu.edu.sg	dong.xu@sydney.edu.au"
"Suraj Srinivas	R. Venkatesh Babu","21","11","2016","Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.","Deep Neural Networks often require good regularizers to generalize well . Dropout be one such regularizer that be widely use among Deep Learning practitioner . Recent work have show that Dropout can also be view a perform Approximate Bayesian Inference over the network parameter . In this work , we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout . One set of method in this family , call Dropout++ , be a version of Dropout with trainable parameter . Classical Dropout emerge a a special case of this method . Another member of this family select the width of neural network layer . Experiments show that these method help in improve generalization performance over Dropout . ","http://arxiv.org/pdf/1611.06791v1","cs.LG	cs.AI	cs.CV	cs.NE","Generalized Dropout","surajsrinivas@grads.cds.iisc.ac.in	venky@cds.iisc.ac.in"
"I. Theodorakopoulos	V. Pothos	D. Kastaniotis	N. Fragoulis","18","1","2017","A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the introduction of a new structural element that can be inserted as an add-on to any contemporary CNN architecture, whilst preserving or even improving its recognition accuracy. Our approach formulates a systematic and data-driven method for developing CNNs that are trained to eventually change size and form in real-time during inference, targeting to the smaller possible computational footprint. Results are provided for the optimal implementation on a few modern, high-end mobile computing platforms indicating a significant speed-up of up to x3 times.","A new , radical CNN design approach be present in this paper , consider the reduction of the total computational load during inference . This be achieve by a new holistic intervention on both the CNN architecture and the training procedure , which target to the parsimonious inference by learn to exploit or remove the redundant capacity of a CNN architecture . This be accomplish , by the introduction of a new structural element that can be insert a an add-on to any contemporary CNN architecture , whilst preserving or even improve it recognition accuracy . Our approach formulate a systematic and data-driven method for develop CNNs that be train to eventually change size and form in real-time during inference , target to the small possible computational footprint . Results be provide for the optimal implementation on a few modern , high-end mobile compute platform indicate a significant speed-up of up to x3 time . ","http://arxiv.org/pdf/1701.05221v5","cs.CV	cs.AI	cs.LG	cs.NE	68T10, 62H30, 68Q32, 68T05, 68Q32, 91E40	I.5; F.1.1; F.4.1; K.3.2; I.4; I.4.8","Parsimonious Inference on Convolutional Neural Networks: Learning and   applying on-line kernel activation rules",""
"Chelsea Finn	Pieter Abbeel	Sergey Levine","9","3","2017","We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.","We propose an algorithm for meta-learning that be model-agnostic , in the sense that it be compatible with any model train with gradient descent and applicable to a variety of different learn problem , include classification , regression , and reinforcement learning . The goal of meta-learning be to train a model on a variety of learn task , such that it can solve new learn task use only a small number of training sample . In our approach , the parameter of the model be explicitly train such that a small number of gradient step with a small amount of train data from a new task will produce good generalization performance on that task . In effect , our method train the model to be easy to fine-tune . We demonstrate that this approach lead to state-of-the-art performance on two few-shot image classification benchmark , produce good result on few-shot regression , and accelerate fine-tuning for policy gradient reinforcement learn with neural network policy . ","http://arxiv.org/pdf/1703.03400v3","cs.LG	cs.AI	cs.CV	cs.NE","Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",""
"Asit Mishra	Jeffrey J Cook	Eriko Nurvitadhi	Debbie Marr","10","4","2017","For computer vision applications, prior works have shown the efficacy of reducing the numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.","For computer vision application , prior work have show the efficacy of reduce the numeric precision of model parameter ( network weight ) in deep neural network but also that reduce the precision of activation hurt model accuracy much more than reduce the precision of model parameter . We study scheme to train network from scratch use reduced-precision activation without hurt the model accuracy . We reduce the precision of activation map ( along with model parameter ) use a novel quantization scheme and increase the number of filter map in a layer , and find that this scheme compensate or surpass the accuracy of the baseline full-precision network . As a result , one can significantly reduce the dynamic memory footprint , memory bandwidth , computational energy and speed up the training and inference process with appropriate hardware support . We call our scheme WRPN - wide reduced-precision network . We report result use our propose scheme and show that our result be good than previously report accuracy on ILSVRC-12 dataset while be computationally less expensive compare to previously report reduced-precision network . ","http://arxiv.org/pdf/1704.03079v1","cs.LG	cs.AI	cs.CV	cs.NE","WRPN: Training and Inference using Wide Reduced-Precision Networks",""
"David Rolnick	Andreas Veit	Serge Belongie	Nir Shavit","30","5","2017","Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. We demonstrate remarkably high test performance after training on corrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain test accuracy above 90 percent even after each clean training example has been diluted with 100 randomly-labeled examples. Such behavior holds across multiple patterns of label noise, even when erroneous labels are biased towards confusing classes. We show that training in this regime requires a significant but manageable increase in dataset size that is related to the factor by which correct labels have been diluted. Finally, we provide an analysis of our results that shows how increasing noise decreases the effective batch size.","Deep neural network train on large supervise datasets have lead to impressive result in image classification and other task . However , well-annotated datasets can be time-consuming and expensive to collect , lend increase interest to large but noisy datasets that be more easily obtain . In this paper , we show that deep neural network be capable of generalize from train data for which true label be massively outnumber by incorrect label . We demonstrate remarkably high test performance after train on corrupt data from MNIST , CIFAR , and ImageNet . For example , on MNIST we obtain test accuracy above 90 percent even after each clean training example have be dilute with 100 randomly-labeled example . Such behavior hold across multiple pattern of label noise , even when erroneous label be bias towards confuse class . We show that training in this regime require a significant but manageable increase in dataset size that be relate to the factor by which correct label have be dilute . Finally , we provide an analysis of our result that show how increasing noise decrease the effective batch size . ","http://arxiv.org/pdf/1705.10694v3","cs.LG	cs.AI	cs.CV	cs.NE","Deep Learning is Robust to Massive Label Noise",""
"Stefan Lattner	Maarten Grachten","5","7","2017","Content-invariance in mapping codes learned by GAEs is a useful feature for various relation learning tasks. In this paper we show that the content-invariance of mapping codes for images of 2D and 3D rotated objects can be substantially improved by extending the standard GAE loss (symmetric reconstruction error) with a regularization term that penalizes the symmetric cross-reconstruction error. This error term involves reconstruction of pairs with mapping codes obtained from other pairs exhibiting similar transformations. Although this would principally require knowledge of the transformations exhibited by training pairs, our experiments show that a bootstrapping approach can sidestep this issue, and that the regularization term can effectively be used in an unsupervised setting.","Content-invariance in map code learn by GAEs be a useful feature for various relation learn task . In this paper we show that the content-invariance of map code for image of 2D and 3D rotated object can be substantially improve by extend the standard GAE loss ( symmetric reconstruction error ) with a regularization term that penalize the symmetric cross-reconstruction error . This error term involve reconstruction of pair with map code obtain from other pair exhibit similar transformation . Although this would principally require knowledge of the transformation exhibit by train pair , our experiment show that a bootstrapping approach can sidestep this issue , and that the regularization term can effectively be use in an unsupervised setting . ","http://arxiv.org/pdf/1707.01357v1","cs.CV	cs.AI	cs.LG	cs.NE","Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object   Rotation",""
"Jindong Wang	Yiqiang Chen	Shuji Hao	Xiaohui Peng	Lisha Hu","12","7","2017","Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction thus achieves promising performance in many areas. Since then, deep learning based methods have been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent advance of deep learning based sensor-based activity recognition. We summarize existing literature from three aspects: sensor modality, deep model, and application. We also present detailed insights on existing work and propose grand challenges for future research.","Sensor-based activity recognition seek the profound high-level knowledge about human activity from multitude of low-level sensor reading . Conventional pattern recognition approach have make tremendous progress in the past year . However , those method often heavily rely on heuristic hand-crafted feature extraction , which could hinder their generalization performance . Additionally , exist method be undermine for unsupervised and incremental learning task . Recently , the recent advancement of deep learning make it possible to perform automatic high-level feature extraction thus achieve promising performance in many area . Since then , deep learn base method have be widely adopt for the sensor-based activity recognition task . This paper survey the recent advance of deep learning base sensor-based activity recognition . We summarize exist literature from three aspect : sensor modality , deep model , and application . We also present detailed insight on exist work and propose grand challenge for future research . ","http://arxiv.org/pdf/1707.03502v2","cs.CV	cs.AI	cs.LG	cs.NE","Deep Learning for Sensor-based Activity Recognition: A Survey",""
"Chengxi Ye	Yezhou Yang	Cornelia Fermuller	Yiannis Aloimonos","2","8","2017","We explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. This paper describes our efforts in their analysis and treatment. The first issue is the training speed inconsistency in different layers. We propose to address it with an intuitive, simple-to-implement, low footprint second-order method. The second issue is the scale inconsistency between the layer inputs and the layer residuals. We explain how second-order information provides favorable convenience in removing this roadblock. The third and most challenging issue is the inconsistency in residual propagation. Based on the fundamental theorem of linear algebra, we provide a mathematical characterization of the famous vanishing gradient problem. Thus, an important design principle for future optimization and neural network design is derived. We conclude this paper with the construction of a novel contractive neural network.","We explain that the difficulty of train deep neural network come from a syndrome of three consistency issue . This paper describe our effort in their analysis and treatment . The first issue be the training speed inconsistency in different layer . We propose to address it with an intuitive , simple-to-implement , low footprint second-order method . The second issue be the scale inconsistency between the layer input and the layer residual . We explain how second-order information provide favorable convenience in remove this roadblock . The third and most challenging issue be the inconsistency in residual propagation . Based on the fundamental theorem of linear algebra , we provide a mathematical characterization of the famous vanish gradient problem . Thus , an important design principle for future optimization and neural network design be derive . We conclude this paper with the construction of a novel contractive neural network . ","http://arxiv.org/pdf/1708.00631v1","cs.LG	cs.AI	cs.CV	cs.NE","On the Importance of Consistency in Training Deep Neural Networks","cxy@umiacs.umd.edu	fer@umiacs.umd.edu	yiannis@umiacs.umd.edu	yz.yang@asu.edu"
"Mario Amrehn	Sven Gaube	Mathias Unberath	Frank Schebesch	Tim Horz	Maddalena Strumia	Stefan Steidl	Markus Kowarschik	Andreas Maier","11","9","2017","For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single patient. We propose a learning-based cooperative segmentation approach which includes the computing entity as well as the user into the task. Our system builds upon a state-of-the-art fully convolutional artificial neural network (FCN) as well as an active user model for training. During the segmentation process, a user of the trained system can iteratively add additional hints in form of pictorial scribbles as seed points into the FCN system to achieve an interactive and precise segmentation result. The segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches can yield superior results compared to networks without the user input channel component, due to a consistent improvement in segmentation quality after each interaction.","For complex segmentation task , fully automatic system be inherently limit in their achievable accuracy for extract relevant object . Especially in case where only few data set need to be process for a highly accurate result , semi-automatic segmentation technique exhibit a clear benefit for the user . One area of application be medical image processing during an intervention for a single patient . We propose a learning-based cooperative segmentation approach which include the compute entity as well a the user into the task . Our system build upon a state-of-the-art fully convolutional artificial neural network ( FCN ) as well a an active user model for training . During the segmentation process , a user of the trained system can iteratively add additional hint in form of pictorial scribble a seed point into the FCN system to achieve an interactive and precise segmentation result . The segmentation quality of interactive FCNs be evaluate . Iterative FCN approach can yield superior result compare to network without the user input channel component , due to a consistent improvement in segmentation quality after each interaction . ","http://arxiv.org/pdf/1709.03450v1","cs.CV	cs.AI	cs.LG	cs.NE	68T05, 68T45	I.2.6; I.4.6; I.5.5","UI-Net: Interactive Artificial Neural Networks for Iterative Image   Segmentation Based on a User Model",""
"Altaf H. Khan","15","12","2017","Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones are either +1 or -1. These universal approximators require approximately 1.1 bits/weight of storage, posses a quick forward pass and achieve classification accuracies similar to conventional continuous-weight networks. Their training regimen focuses on error reduction initially, but later emphasizes discretization of weights. They ignore insignificant inputs, remove unnecessary weights, and drop unneeded hidden neurons. We have successfully tested them on the MNIST, credit card fraud, and credit card defaults data sets using networks having 2 to 16 hidden layers and up to 4.4 million weights.","Most of the weight in a Lightweight Neural Network have a value of zero , while the remain one be either +1 or -1 . These universal approximators require approximately 1.1 bits/weight of storage , posses a quick forward pas and achieve classification accuracy similar to conventional continuous-weight network . Their training regimen focus on error reduction initially , but later emphasize discretization of weight . They ignore insignificant input , remove unnecessary weight , and drop unneeded hidden neuron . We have successfully test them on the MNIST , credit card fraud , and credit card default data set use network have 2 to 16 hidden layer and up to 4.4 million weight . ","http://arxiv.org/pdf/1712.05695v1","cs.LG	cs.AI	cs.CV	cs.NE","Lightweight Neural Networks","altaf@altafkhan.com"
"Nathaniel Thomas	Tess Smidt	Steven Kearnes	Lusann Yang	Li Li	Kai Kohlhoff	Patrick Riley","22","2","2018","We introduce tensor field networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantees as output) scalars, vectors, and higher-order tensors, in the geometric sense of these terms. We demonstrate how tensor field networks learn to model simple physics (Newtonian gravitation and moment of inertia), classify simple 3D shapes (trained on one orientation and tested on shapes in arbitrary orientations), and, given a small organic molecule with an atom removed, replace the correct element at the correct location in space.","We introduce tensor field network , which be locally equivariant to 3D rotation , translation , and permutation of point at every layer . 3D rotation equivariance remove the need for data augmentation to identify feature in arbitrary orientation . Our network use filter build from spherical harmonic ; due to the mathematical consequence of this filter choice , each layer accept a input ( and guarantee a output ) scalar , vector , and higher-order tensor , in the geometric sense of these term . We demonstrate how tensor field network learn to model simple physic ( Newtonian gravitation and moment of inertia ) , classify simple 3D shape ( train on one orientation and test on shape in arbitrary orientation ) , and , give a small organic molecule with an atom remove , replace the correct element at the correct location in space . ","http://arxiv.org/pdf/1802.08219v2","cs.LG	cs.AI	cs.CV	cs.NE","Tensor Field Networks: Rotation- and Translation-Equivariant Neural   Networks for 3D Point Clouds",""
"ÃaÄlar GÃ¼lÃ§ehre	Yoshua Bengio","17","1","2013","We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favor of this hypothesis. In our experiments, a two-tiered MLP architecture is trained on a dataset with 64x64 binary inputs images, each image with three sprites. The final task is to decide whether all the sprites are the same or one of them is different. Sprites are pentomino tetris shapes and they are placed in an image with different locations using scaling and rotation transformations. The first part of the two-tiered MLP is pre-trained with intermediate-level targets being the presence of sprites at each location, while the second part takes the output of the first part as input and predicts the final task's target binary event. The two-tiered MLP architecture, with a few tens of thousand examples, was able to learn the task perfectly, whereas all other algorithms (include unsupervised pre-training, but also traditional algorithms like SVMs, decision trees and boosting) all perform no better than chance. We hypothesize that the optimization difficulty involved when the intermediate pre-training is not performed is due to the {\em composition} of two highly non-linear tasks. Our findings are also consistent with hypotheses on cultural learning inspired by the observations of optimization problems with deep learning, presumably because of effective local minima.","We explore the effect of introduce prior information into the intermediate level of neural network for a learning task on which all the state-of-the-art machine learn algorithms test fail to learn . We motivate our work from the hypothesis that humans learn such intermediate concept from other individual via a form of supervision or guidance use a curriculum . The experiment we have conduct provide positive evidence in favor of this hypothesis . In our experiment , a two-tiered MLP architecture be train on a dataset with 64x64 binary input image , each image with three sprite . The final task be to decide whether all the sprite be the same or one of them be different . Sprites be pentomino tetris shape and they be place in an image with different location use scaling and rotation transformation . The first part of the two-tiered MLP be pre-trained with intermediate-level target be the presence of sprite at each location , while the second part take the output of the first part a input and predict the final task 's target binary event . The two-tiered MLP architecture , with a few ten of thousand example , be able to learn the task perfectly , whereas all other algorithm ( include unsupervised pre-training , but also traditional algorithm like SVMs , decision tree and boost ) all perform no good than chance . We hypothesize that the optimization difficulty involve when the intermediate pre-training be not perform be due to the { \em composition } of two highly non-linear task . Our finding be also consistent with hypothesis on cultural learning inspire by the observation of optimization problem with deep learning , presumably because of effective local minimum . ","http://arxiv.org/pdf/1301.4083v6","cs.LG	cs.CV	cs.NE	stat.ML","Knowledge Matters: Importance of Prior Information for Optimization","gulcehrc@iro.umontreal.ca	bengioy@iro.umontreal.ca"
"Kishore Konda	Roland Memisevic	David Krueger","13","2","2014","Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization.","Regularized training of an autoencoder typically result in hidden unit bias that take on large negative value . We show that negative bias be a natural result of use a hidden layer whose responsibility be to both represent the input data and act a a selection mechanism that ensure sparsity of the representation . We then show that negative bias impede the learning of data distribution whose intrinsic dimensionality be high . We also propose a new activation function that decouple the two role of the hidden layer and that allow u to learn representation on data with very high intrinsic dimensionality , where standard autoencoders typically fail . Since the decoupled activation function act like an implicit regularizer , the model can be train by minimize the reconstruction error of training data , without require any additional regularization . ","http://arxiv.org/pdf/1402.3337v5","stat.ML	cs.CV	cs.LG	cs.NE","Zero-bias autoencoders and the benefits of co-adapting features","konda.kishorereddy@gmail.com	roland.memisevic@umontreal.ca	david.krueger@umontreal.ca"
"Bodo Rueckauer	Iulia-Alexandra Lungu	Yuhuang Hu	Michael Pfeiffer","13","12","2016","Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their data-driven, event-based style of computing. Here we provide a novel theory that explains why this conversion is successful, and derive from it several new tools to convert a larger and more powerful class of deep networks into SNNs. We identify the main sources of approximation errors in previous conversion methods, and propose simple mechanisms to fix these issues. Furthermore, we develop spiking implementations of common CNN operations such as max-pooling, softmax, and batch-normalization, which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain. Empirical evaluation of different network architectures on the MNIST and CIFAR10 benchmarks leads to the best SNN results reported to date.","Deep convolutional neural network ( CNNs ) have show great potential for numerous real-world machine learning application , but perform inference in large CNNs in real-time remain a challenge . We have previously demonstrate that traditional CNNs can be convert into deep spike neural network ( SNNs ) , which exhibit similar accuracy while reduce both latency and computational load a a consequence of their data-driven , event-based style of compute . Here we provide a novel theory that explain why this conversion be successful , and derive from it several new tool to convert a large and more powerful class of deep network into SNNs . We identify the main source of approximation error in previous conversion method , and propose simple mechanism to fix these issue . Furthermore , we develop spike implementation of common CNN operation such a max-pooling , softmax , and batch-normalization , which allow almost loss-less conversion of arbitrary CNN architectures into the spiking domain . Empirical evaluation of different network architecture on the MNIST and CIFAR10 benchmarks lead to the best SNN result report to date . ","http://arxiv.org/pdf/1612.04052v1","stat.ML	cs.CV	cs.LG	cs.NE","Theory and Tools for the Conversion of Analog to Spiking Convolutional   Neural Networks","rbodo@ini.uzh.ch	iulialexandra@ini.uzh.ch	yuhu@ini.uzh.ch	pfeiffer@ini.uzh.ch"
"Xun Huang	Yixuan Li	Omid Poursaeed	John Hopcroft	Serge Belongie","13","12","2016","In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.","In this paper , we propose a novel generative model name Stacked Generative Adversarial Networks ( SGAN ) , which be train to invert the hierarchical representation of a bottom-up discriminative network . Our model consist of a top-down stack of GANs , each learn to generate lower-level representation condition on higher-level representation . A representation discriminator be introduce at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network , leverage the powerful discriminative representation to guide the generative model . In addition , we introduce a conditional loss that encourage the use of conditional information from the layer above , and a novel entropy loss that maximize a variational low bound on the conditional entropy of generator output . We first train each stack independently , and then train the whole model end-to-end . Unlike the original GAN that use a single noise vector to represent all the variation , our SGAN decompose variation into multiple level and gradually resolve uncertainty in the top-down generative process . Based on visual inspection , Inception score and visual Turing test , we demonstrate that SGAN be able to generate image of much high quality than GANs without stack . ","http://arxiv.org/pdf/1612.04357v4","cs.CV	cs.LG	cs.NE	stat.ML","Stacked Generative Adversarial Networks","xh258@cornell.edu	yl2363@cornell.edu	op63@cornell.edu	sjb344@cornell.edu	jeh@cs.cornell.edu"
"David Warde-Farley	Andrew Rabinovich	Dragomir Anguelov","20","12","2014","We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%.","We study the problem of large scale , multi-label visual recognition with a large number of possible class . We propose a method for augment a trained neural network classifier with auxiliary capacity in a manner design to significantly improve upon an already well-performing model , while minimally impact it computational footprint . Using the prediction of the network itself a a descriptor for assess visual similarity , we define a partitioning of the label space into group of visually similar entity . We then augment the network with auxilliary hidden layer pathway with connectivity only to these group of label unit . We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model , while increase the number of multiply-adds by less than 3 % . ","http://arxiv.org/pdf/1412.6563v2","stat.ML	cs.CV	cs.LG	cs.NE","Self-informed neural network structure learning","wardefar@iro.umontreal.ca	amrabino@google.com	dragomir@google.com"
"Forest Agostinelli	Matthew Hoffman	Peter Sadowski	Pierre Baldi","21","12","2014","Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.","Artificial neural network typically have a fix , non-linear activation function at each neuron . We have design a novel form of piecewise linear activation function that be learn independently for each neuron use gradient descent . With this adaptive activation function , we be able to improve upon deep neural network architecture compose of static rectified linear unit , achieve state-of-the-art performance on CIFAR-10 ( 7.51 % ) , CIFAR-100 ( 30.83 % ) , and a benchmark from high-energy physic involve Higgs boson decay mode . ","http://arxiv.org/pdf/1412.6830v3","cs.NE	cs.CV	cs.LG	stat.ML","Learning Activation Functions to Improve Deep Neural Networks","fagostin@uci.edu	mathoffm@adobe.com	peter.j.sadowski@uci.edu	pfbaldi@uci.edu"
"Antti Rasmus	Tapani Raiko	Harri Valpola","22","12","2014","Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.","Suitable lateral connection between encoder and decoder be show to allow high layer of a denoising autoencoder ( dAE ) to focus on invariant representation . In regular autoencoders , detailed information need to be carry through the high layer but lateral connection from encoder to decoder relieve this pressure . It be show that abstract invariant feature can be translate to detail reconstruction when invariant feature be allow to modulate the strength of the lateral connection . Three dAE structure with modulated and additive lateral connection , and without lateral connection be compare in experiment use real-world image . The experiment verify that add modulated lateral connection to the model 1 ) improve the accuracy of the probability model for input , a measure by denoising performance ; 2 ) result in representation whose degree of invariance grows faster towards the high layer ; and 3 ) support the formation of diverse invariant poolings . ","http://arxiv.org/pdf/1412.7210v4","cs.NE	cs.CV	cs.LG	stat.ML","Denoising autoencoder with modulated lateral connections learns   invariant representations of natural images","antti.rasmus@aalto.fi	tapani.raiko@aalto.fi	harri@zenrobotics.com"
"Ankit B. Patel	Tan Nguyen	Richard G. Baraniuk","2","4","2015","A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement.","A grand challenge in machine learning be the development of computational algorithm that match or outperform human in perceptual inference task that be complicate by nuisance variation . For instance , visual object recognition involve the unknown object position , orientation , and scale in object recognition while speech recognition involve the unknown voice pronunciation , pitch , and speed . Recently , a new breed of deep learning algorithm have emerge for high-nuisance inference task that routinely yield pattern recognition system with near- or super-human capability . But a fundamental question remain : Why do they work ? Intuitions abound , but a coherent framework for understand , analyze , and synthesize deep learning architecture have remain elusive . We answer this question by develop a new probabilistic framework for deep learning base on the Deep Rendering Model : a generative probabilistic model that explicitly capture latent nuisance variation . By relax the generative model to a discriminative one , we can recover two of the current leading deep learning system , deep convolutional neural network and random decision forest , provide insight into their success and shortcoming , as well a a principled route to their improvement . ","http://arxiv.org/pdf/1504.00641v1","stat.ML	cs.CV	cs.LG	cs.NE","A Probabilistic Theory of Deep Learning","abp4@rice.edu	mn15@rice.edu	richb@rice.edu"
"Rein Houthooft	Filip De Turck","3","8","2015","Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are nonprobabilistic models that optimize a joint input-output function through margin-based learning. Because SSVMs generally disregard the interplay between unary and interaction factors during the training phase, final parameters are suboptimal. Moreover, its factors are often restricted to linear combinations of input features, limiting its generalization power. To improve prediction accuracy, this paper proposes: (i) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM factors to neural networks that form highly nonlinear functions of input features. Image segmentation benchmark results demonstrate improvements over conventional SSVM training methods in terms of accuracy, highlighting the feasibility of end-to-end SSVM training with neural factors.","Tackling pattern recognition problem in area such a computer vision , bioinformatics , speech or text recognition be often do best by take into account task-specific statistical relation between output variable . In structured prediction , this internal structure be use to predict multiple output simultaneously , lead to more accurate and coherent prediction . Structural support vector machine ( SSVMs ) be nonprobabilistic model that optimize a joint input-output function through margin-based learning . Because SSVMs generally disregard the interplay between unary and interaction factor during the training phase , final parameter be suboptimal . Moreover , it factor be often restrict to linear combination of input feature , limit it generalization power . To improve prediction accuracy , this paper propose : ( i ) Joint inference and learning by integration of back-propagation and loss-augmented inference in SSVM subgradient descent ; ( ii ) Extending SSVM factor to neural network that form highly nonlinear function of input feature . Image segmentation benchmark result demonstrate improvement over conventional SSVM training method in term of accuracy , highlight the feasibility of end-to-end SSVM training with neural factor . ","http://arxiv.org/pdf/1508.00451v4","stat.ML	cs.CV	cs.LG	cs.NE","Integrated Inference and Learning of Neural Factors in Structural   Support Vector Machines",""
"Patrick W. Gallagher	Shuai Tang	Zhuowen Tu","23","11","2015","Top-down information plays a central role in human perception, but plays relatively little role in many current state-of-the-art deep networks, such as Convolutional Neural Networks (CNNs). This work seeks to explore a path by which top-down information can have a direct impact within current deep networks. We explore this path by learning and using "generators" corresponding to the network internal effects of three types of transformation (each a restriction of a general affine transformation): rotation, scaling, and translation. We demonstrate how these learned generators can be used to transfer top-down information to novel settings, as mediated by the "feature flows" that the transformations (and the associated generators) correspond to inside the network. Specifically, we explore three aspects: 1) using generators as part of a method for synthesizing transformed images --- given a previously unseen image, produce versions of that image corresponding to one or more specified transformations, 2) "zero-shot learning" --- when provided with a feature flow corresponding to the effect of a transformation of unknown amount, leverage learned generators as part of a method by which to perform an accurate categorization of the amount of transformation, even for amounts never observed during training, and 3) (inside-CNN) "data augmentation" --- improve the classification performance of an existing network by using the learned generators to directly provide additional training "inside the CNN".","Top-down information play a central role in human perception , but play relatively little role in many current state-of-the-art deep network , such a Convolutional Neural Networks ( CNNs ) . This work seek to explore a path by which top-down information can have a direct impact within current deep network . We explore this path by learn and use `` generator '' correspond to the network internal effect of three type of transformation ( each a restriction of a general affine transformation ) : rotation , scaling , and translation . We demonstrate how these learn generator can be use to transfer top-down information to novel setting , a mediate by the `` feature flow '' that the transformation ( and the associated generator ) correspond to inside the network . Specifically , we explore three aspect : 1 ) use generator a part of a method for synthesize transformed image -- - give a previously unseen image , produce version of that image correspond to one or more specified transformation , 2 ) `` zero-shot learning '' -- - when provide with a feature flow correspond to the effect of a transformation of unknown amount , leverage learn generator a part of a method by which to perform an accurate categorization of the amount of transformation , even for amount never observe during training , and 3 ) ( inside-CNN ) `` data augmentation '' -- - improve the classification performance of an exist network by use the learned generator to directly provide additional training `` inside the CNN '' . ","http://arxiv.org/pdf/1511.07125v1","cs.NE	cs.CV	cs.LG	stat.ML","What Happened to My Dog in That Network: Unraveling Top-down Generators   in Convolutional Neural Networks","pwgallag@ucsd.edu	shuaitang93@ucsd.edu	ztu@ucsd.edu"
"Adrien Gaidon	Qiao Wang	Yohann Cabon	Eleonora Vig","20","5","2016","Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.","Modern computer vision algorithm typically require expensive data acquisition and accurate manual labeling . In this work , we instead leverage the recent progress in computer graphic to generate fully label , dynamic , and photo-realistic proxy virtual world . We propose an efficient real-to-virtual world clone method , and validate our approach by building and publicly release a new video dataset , call Virtual KITTI ( see http : //www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds ) , automatically label with accurate ground truth for object detection , track , scene and instance segmentation , depth , and optical flow . We provide quantitative experimental evidence suggest that ( i ) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual world , and ( ii ) pre-training on virtual data improves performance . As the gap between real and virtual world be small , virtual world enable measure the impact of various weather and imaging condition on recognition performance , all other thing be equal . We show these factor may affect drastically otherwise high-performing deep model for track . ","http://arxiv.org/pdf/1605.06457v1","cs.CV	cs.LG	cs.NE	stat.ML","Virtual Worlds as Proxy for Multi-Object Tracking Analysis","adrien.gaidon@xrce.xerox.com	yohann.cabon@xrce.xerox.com	qiao.wang@asu.edu	eleonora.vig@dlr.de"
"Jianwen Xie	Song-Chun Zhu	Ying Nian Wu","3","6","2016","Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal ConvNet that consists of multiple layers of spatial-temporal filters to capture spatial-temporal patterns of different scales. The model can be learned from the training video sequences by an "analysis by synthesis" learning algorithm that iterates the following two steps. Step 1 synthesizes video sequences from the currently learned model. Step 2 then updates the model parameters based on the difference between the synthesized video sequences and the observed training sequences. We show that the learning algorithm can synthesize realistic dynamic patterns.","Video sequence contain rich dynamic pattern , such a dynamic texture pattern that exhibit stationarity in the temporal domain , and action pattern that be non-stationary in either spatial or temporal domain . We show that a spatial-temporal generative ConvNet can be use to model and synthesize dynamic pattern . The model define a probability distribution on the video sequence , and the log probability be define by a spatial-temporal ConvNet that consist of multiple layer of spatial-temporal filter to capture spatial-temporal pattern of different scale . The model can be learn from the training video sequence by an `` analysis by synthesis '' learn algorithm that iterate the following two step . Step 1 synthesizes video sequence from the currently learn model . Step 2 then update the model parameter base on the difference between the synthesized video sequence and the observed training sequence . We show that the learn algorithm can synthesize realistic dynamic pattern . ","http://arxiv.org/pdf/1606.00972v2","stat.ML	cs.CV	cs.LG	cs.NE","Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet","jianwen@ucla.edu,	sczhu@stat.ucla.edu,	ywu@stat.ucla.edu"
"Mohammad Javad Shafiee	Akshaya Mishra	Alexander Wong","14","6","2016","Taking inspiration from biological evolution, we explore the idea of "Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks are encoded using synaptic probability models, which can be viewed as the `DNA' of these networks. New descendant networks with differing network architectures are synthesized based on these synaptic probability models from the ancestor networks and computational environmental factor models, in a random manner to mimic heredity, natural selection, and random mutation. These offspring networks are then trained into fully functional networks, like one would train a newborn, and have more efficient, more diverse network architectures than their ancestor networks, while achieving powerful modeling capabilities. Experimental results for the task of visual saliency demonstrated that the synthesized `evolved' offspring networks can achieve state-of-the-art performance while having network architectures that are significantly more efficient (with a staggering $\sim$48-fold decrease in synapses by the fourth generation) compared to the original ancestor network.","Taking inspiration from biological evolution , we explore the idea of `` Can deep neural network evolve naturally over successive generation into highly efficient deep neural network ? '' by introduce the notion of synthesize new highly efficient , yet powerful deep neural network over successive generation via an evolutionary process from ancestor deep neural network . The architectural trait of ancestor deep neural network be encode use synaptic probability model , which can be view a the ` DNA ' of these network . New descendant network with differ network architecture be synthesize base on these synaptic probability model from the ancestor network and computational environmental factor model , in a random manner to mimic heredity , natural selection , and random mutation . These offspring network be then train into fully functional network , like one would train a newborn , and have more efficient , more diverse network architecture than their ancestor network , while achieve powerful model capability . Experimental result for the task of visual saliency demonstrate that the synthesized ` evolve ' offspring network can achieve state-of-the-art performance while have network architecture that be significantly more efficient ( with a staggering $ \sim $ 48-fold decrease in synapsis by the fourth generation ) compare to the original ancestor network . ","http://arxiv.org/pdf/1606.04393v3","cs.CV	cs.LG	cs.NE	stat.ML","Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural   Networks",""
"Tian Han	Yang Lu	Song-Chun Zhu	Ying Nian Wu","28","6","2016","This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.","This paper propose an alternate back-propagation algorithm for learn the generator network model . The model be a non-linear generalization of factor analysis . In this model , the mapping from the continuous latent factor to the observed signal be parametrized by a convolutional neural network . The alternate back-propagation algorithm iterate the following two step : ( 1 ) Inferential back-propagation , which infer the latent factor by Langevin dynamic or gradient descent . ( 2 ) Learning back-propagation , which update the parameter give the inferred latent factor by gradient descent . The gradient computation in both step be power by back-propagation , and they share most of their code in common . We show that the alternate back-propagation algorithm can learn realistic generator model of natural image , video sequence , and sound . Moreover , it can also be use to learn from incomplete or indirect training data . ","http://arxiv.org/pdf/1606.08571v4","stat.ML	cs.CV	cs.LG	cs.NE","Alternating Back-Propagation for Generator Network",""
"Ilija Ilievski	Jiashi Feng","31","7","2016","Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately model the distribution. To address this challenging issue, we propose a method that learns to transfer optimal hyperparameter values for a small source dataset to hyperparameter values with comparable performance on a dataset of interest. As opposed to existing transfer learning methods, our proposed method does not use hand-designed features. Instead, it uses surrogates to model the hyperparameter-error distributions of the two datasets and trains a neural network to learn the transfer function. Extensive experiments on three CV benchmark datasets clearly demonstrate the efficiency of our method.","Recently , several optimization method have be successfully apply to the hyperparameter optimization of deep neural network ( DNNs ) . The method work by model the joint distribution of hyperparameter value and correspond error . Those method become less practical when apply to modern DNNs whose training may take a few day and thus one can not collect sufficient observation to accurately model the distribution . To address this challenging issue , we propose a method that learn to transfer optimal hyperparameter value for a small source dataset to hyperparameter value with comparable performance on a dataset of interest . As oppose to exist transfer learn method , our propose method do not use hand-designed feature . Instead , it use surrogate to model the hyperparameter-error distribution of the two datasets and train a neural network to learn the transfer function . Extensive experiment on three CV benchmark datasets clearly demonstrate the efficiency of our method . ","http://arxiv.org/pdf/1608.00218v1","cs.LG	cs.CV	cs.NE	stat.ML","Hyperparameter Transfer Learning through Surrogate Alignment for   Efficient Deep Neural Network Training","ilija.ilievski@u.nus.edu,	elefjia@nus.edu.sg"
"Hao Wang	Dit-Yan Yeung","24","8","2016","While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics like Bayesian treatment of neural networks.","While perception task such a visual object recognition and text understanding play an important role in human intelligence , the subsequent task that involve inference , reason and plan require an even high level of intelligence . The past few year have see major advance in many perception task use deep learning model . For higher-level inference , however , probabilistic graphical model with their Bayesian nature be still more powerful and flexible . To achieve integrated intelligence that involve both perception and inference , it be naturally desirable to tightly integrate deep learning and Bayesian model within a principled probabilistic framework , which we call Bayesian deep learning . In this unified framework , the perception of text or image use deep learning can boost the performance of higher-level inference and in return , the feedback from the inference process be able to enhance the perception of text or image . This paper propose a general framework for Bayesian deep learning and review it recent application on recommender system , topic model , and control . In this paper , we also discuss the relationship and difference between Bayesian deep learning and other related topic like Bayesian treatment of neural network . ","http://arxiv.org/pdf/1608.06884v2","stat.ML	cs.CV	cs.LG	cs.NE","Towards Bayesian Deep Learning: A Framework and Some Existing Methods",""
"Mason McGill	Pietro Perona","17","3","2017","We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.","We propose and systematically evaluate three strategy for train dynamically-routed artificial neural network : graph of learned transformation through which different input signal may take different path . Though some approach have advantage over others , the result network be often qualitatively similar . We find that , in dynamically-routed network train to classify image , layer and branch become specialized to process distinct category of image . Additionally , give a fixed computational budget , dynamically-routed network tend to perform good than comparable statically-routed network . ","http://arxiv.org/pdf/1703.06217v2","stat.ML	cs.CV	cs.LG	cs.NE","Deciding How to Decide: Dynamic Routing in Artificial Neural Networks",""
"Hongyang Gao	Hao Yuan	Zhengyang Wang	Shuiwang Ji","18","5","2017","Deconvolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. Our method is based on a fresh interpretation of the regular deconvolution operation. The resulting PixelDCL can be used to replace any deconvolutional layer in a plug-and-play manner without compromising the fully trainable capabilities of original models. The proposed PixelDCL may result in slight decrease in efficiency, but this can be overcome by an implementation trick. Experimental results on semantic segmentation demonstrate that PixelDCL can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than deconvolutional layers. When used in image generation tasks, our PixelDCL can largely overcome the checkerboard problem suffered by regular deconvolution operations.","Deconvolutional layer have be widely use in a variety of deep model for up-sampling , include encoder-decoder network for semantic segmentation and deep generative model for unsupervised learning . One of the key limitation of deconvolutional operation be that they result in the so-called checkerboard problem . This be cause by the fact that no direct relationship exist among adjacent pixel on the output feature map . To address this problem , we propose the pixel deconvolutional layer ( PixelDCL ) to establish direct relationship among adjacent pixel on the up-sampled feature map . Our method be base on a fresh interpretation of the regular deconvolution operation . The result PixelDCL can be use to replace any deconvolutional layer in a plug-and-play manner without compromise the fully trainable capability of original model . The propose PixelDCL may result in slight decrease in efficiency , but this can be overcome by an implementation trick . Experimental result on semantic segmentation demonstrate that PixelDCL can consider spatial feature such a edge and shape and yield more accurate segmentation output than deconvolutional layer . When use in image generation task , our PixelDCL can largely overcome the checkerboard problem suffer by regular deconvolution operation . ","http://arxiv.org/pdf/1705.06820v4","cs.LG	cs.CV	cs.NE	stat.ML","Pixel Deconvolutional Networks","hongyang.gao@wsu.edu	hao.yuan@wsu.edu	zwang6@eecs.wsu.edu	sji@eecs.wsu.edu"
"Stanislav Fort","9","8","2017","We propose a novel architecture for $k$-shot classification on the Omniglot dataset. Building on prototypical networks, we extend their architecture to what we call Gaussian prototypical networks. Prototypical networks learn a map between images and embedding vectors, and use their clustering for classification. In our model, a part of the encoder output is interpreted as a confidence region estimate about the embedding point, and expressed as a Gaussian covariance matrix. Our network then constructs a direction and class dependent distance metric on the embedding space, using uncertainties of individual data points as weights. We show that Gaussian prototypical networks are a preferred architecture over vanilla prototypical networks with an equivalent number of parameters. We report state-of-the-art performance in 1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot 5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset. We explore artificially down-sampling a fraction of images in the training set, which improves our performance even further. We therefore hypothesize that Gaussian prototypical networks might perform better in less homogeneous, noisier datasets, which are commonplace in real world applications.","We propose a novel architecture for $ k $ -shot classification on the Omniglot dataset . Building on prototypical network , we extend their architecture to what we call Gaussian prototypical network . Prototypical network learn a map between image and embed vector , and use their clustering for classification . In our model , a part of the encoder output be interpret a a confidence region estimate about the embedding point , and express a a Gaussian covariance matrix . Our network then construct a direction and class dependent distance metric on the embed space , use uncertainty of individual data point a weight . We show that Gaussian prototypical network be a preferred architecture over vanilla prototypical network with an equivalent number of parameter . We report state-of-the-art performance in 1-shot and 5-shot classification both in 5-way and 20-way regime ( for 5-shot 5-way , we be comparable to previous state-of-the-art ) on the Omniglot dataset . We explore artificially down-sampling a fraction of image in the training set , which improve our performance even far . We therefore hypothesize that Gaussian prototypical network might perform good in less homogeneous , noisy datasets , which be commonplace in real world application . ","http://arxiv.org/pdf/1708.02735v1","cs.LG	cs.CV	cs.NE	stat.ML","Gaussian Prototypical Networks for Few-Shot Learning on Omniglot",""
"Leslie N. Smith	Nicholay Topin","23","8","2017","In this paper, we show a phenomenon, which we named "super-convergence", where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate. Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence.","In this paper , we show a phenomenon , which we name `` super-convergence '' , where residual network can be train use an order of magnitude few iteration than be use with standard training method . The existence of super-convergence be relevant to understand why deep network generalize well . One of the key element of super-convergence be train with cyclical learn rate and a large maximum learning rate . Furthermore , we present evidence that train with large learn rate improves performance by regularize the network . In addition , we show that super-convergence provide a great boost in performance relative to standard training when the amount of labeled training data be limited . We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate . The architecture and code to replicate the figure in this paper be available at github.com/lnsmith54/super-convergence . ","http://arxiv.org/pdf/1708.07120v2","cs.LG	cs.CV	cs.NE	stat.ML","Super-Convergence: Very Fast Training of Residual Networks Using Large   Learning Rates","leslie.smith@nrl.navy.mil	ntopin1@umbc.edu"
"Boris Flach	Alexander Shekhovtsov	Ondrej Fikar","25","9","2017","Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we show that forward computation in DNNs with logistic sigmoid activations corresponds to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model. This connection allows to interpret DNN as a probabilistic model of the output and all hidden units given the input. Second, we propose that in order for the recognition and generation networks to be more consistent with the joint model of the data, weights of the recognition and generator network should be related by transposition. We demonstrate in a tentative experiment that such a coupled pair can be learned generatively, modelling the full distribution of the data, and has enough capacity to perform well in both recognition and generation.","Learning , take into account full distribution of the data , refer to a generative , be not feasible with deep neural network ( DNNs ) because they model only the conditional distribution of the output give the input . Current solution be either base on joint probability model face difficult estimation problem or learn two separate network , map input to output ( recognition ) and vice-versa ( generation ) . We propose an intermediate approach . First , we show that forward computation in DNNs with logistic sigmoid activation correspond to a simplified approximate Bayesian inference in a directed probabilistic multi-layer model . This connection allow to interpret DNN a a probabilistic model of the output and all hidden unit give the input . Second , we propose that in order for the recognition and generation network to be more consistent with the joint model of the data , weight of the recognition and generator network should be relate by transposition . We demonstrate in a tentative experiment that such a coupled pair can be learn generatively , model the full distribution of the data , and have enough capacity to perform well in both recognition and generation . ","http://arxiv.org/pdf/1709.08524v1","cs.LG	cs.CV	cs.NE	stat.ML","Generative learning for deep networks","flachbor@cmp.felk.cvut.cz	shekhovtsov@gmail.com	fikarond@fel.cvut.cz"
"Hanxiao Liu	Karen Simonyan	Oriol Vinyals	Chrisantha Fernando	Koray Kavukcuoglu","1","11","2017","We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.","We explore efficient neural architecture search method and show that a simple yet powerful evolutionary algorithm can discover new architecture with excellent performance . Our approach combine a novel hierarchical genetic representation scheme that imitate the modularized design pattern commonly adopt by human expert , and an expressive search space that support complex topology . Our algorithm efficiently discover architecture that outperform a large number of manually design model for image classification , obtain top-1 error of 3.6 % on CIFAR-10 and 20.3 % when transfer to ImageNet , which be competitive with the best exist neural architecture search approach . We also present result use random search , achieve 0.3 % less top-1 accuracy on CIFAR-10 and 0.1 % less on ImageNet whilst reduce the search time from 36 hour down to 1 hour . ","http://arxiv.org/pdf/1711.00436v2","cs.LG	cs.CV	cs.NE	stat.ML","Hierarchical Representations for Efficient Architecture Search","hanxiaol@cs.cmu.edu	simonyan@google.com	vinyals@google.com	chrisantha@google.com	korayk@google.com"
"Antreas Antoniou	Amos Storkey	Harrison Edwards","12","11","2017","Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).","Effective training of neural network require much data . In the low-data regime , parameter be underdetermined , and learnt network generalise poorly . Data Augmentation alleviate this by use exist data more effectively . However standard data augmentation produce only limited plausible alternative data . Given there be potential to generate a much broad set of augmentation , we design and train a generative model to do data augmentation . The model , base on image conditional Generative Adversarial Networks , take data from a source domain and learn to take any data item and generalise it to generate other within-class data item . As this generative process do not depend on the class themselves , it can be apply to novel unseen class of data . We show that a Data Augmentation Generative Adversarial Network ( DAGAN ) augment standard vanilla classifier well . We also show a DAGAN can enhance few-shot learn system such a Matching Networks . We demonstrate these approach on Omniglot , on EMNIST have learn the DAGAN on Omniglot , and VGG-Face data . In our experiment we can see over 13 % increase in accuracy in the low-data regime experiment in Omniglot ( from 69 % to 82 % ) , EMNIST ( 73.9 % to 76 % ) and VGG-Face ( 4.5 % to 12 % ) ; in Matching Networks for Omniglot we observe an increase of 0.5 % ( from 96.9 % to 97.4 % ) and an increase of 1.8 % in EMNIST ( from 59.5 % to 61.3 % ) . ","http://arxiv.org/pdf/1711.04340v3","stat.ML	cs.CV	cs.LG	cs.NE","Data Augmentation Generative Adversarial Networks","a.antoniou@sms.ed.ac.uk	a.storkey@ed.ac.uk	h.l.edwards@sms.ed.ac.uk"
"Dror Sholomon	Eli David	Nathan S. Netanyahu","23","11","2017","This paper introduces the first deep neural network-based estimation metric for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle, using nothing but the pixels of each piece. The proposed metric exhibits an extremely high precision even though no manual feature extraction is performed. When incorporated into an existing puzzle solver, the solution's accuracy increases significantly, achieving thereby a new state-of-the-art standard.","This paper introduce the first deep neural network-based estimation metric for the jigsaw puzzle problem . Given two puzzle piece edge , the neural network predict whether or not they should be adjacent in the correct assembly of the puzzle , use nothing but the pixel of each piece . The propose metric exhibit an extremely high precision even though no manual feature extraction be perform . When incorporate into an exist puzzle solver , the solution 's accuracy increase significantly , achieve thereby a new state-of-the-art standard . ","http://arxiv.org/pdf/1711.08762v1","cs.CV	cs.LG	cs.NE	stat.ML","DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the   Jigsaw Puzzle Problem","dror.sholomon@gmail.com,	mail@elidavid.com,	nathan@cs.biu.ac.il	nathan@cfar.umd.edu"
"Eli David	Nathan S. Netanyahu","23","11","2017","In this paper we describe the problem of painter classification, and propose a novel approach based on deep convolutional autoencoder neural networks. While previous approaches relied on image processing and manual feature extraction from paintings, our approach operates on the raw pixel level, without any preprocessing or manual feature extraction. We first train a deep convolutional autoencoder on a dataset of paintings, and subsequently use it to initialize a supervised convolutional neural network for the classification phase.   The proposed approach substantially outperforms previous methods, improving the previous state-of-the-art for the 3-painter classification problem from 90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63% reduction in error rate.","In this paper we describe the problem of painter classification , and propose a novel approach base on deep convolutional autoencoder neural network . While previous approach rely on image processing and manual feature extraction from painting , our approach operate on the raw pixel level , without any preprocessing or manual feature extraction . We first train a deep convolutional autoencoder on a dataset of painting , and subsequently use it to initialize a supervised convolutional neural network for the classification phase . The propose approach substantially outperform previous method , improve the previous state-of-the-art for the 3-painter classification problem from 90.44 % accuracy ( previous state-of-the-art ) to 96.52 % accuracy , i.e. , a 63 % reduction in error rate . ","http://arxiv.org/pdf/1711.08763v1","cs.CV	cs.LG	cs.NE	stat.ML","DeepPainter: Painter Classification Using Deep Convolutional   Autoencoders","mail@elidavid.com,	nathan@cs.biu.ac.il	nathan@cfar.umd.edu"
"Ido Cohen	Eli David	Nathan S. Netanyahu	Noa Liscovitch	Gal Chechik","27","11","2017","This paper presents a novel deep learning-based method for learning a functional representation of mammalian neural images. The method uses a deep convolutional denoising autoencoder (CDAE) for generating an invariant, compact representation of in situ hybridization (ISH) images. While most existing methods for bio-imaging analysis were not developed to handle images with highly complex anatomical structures, the results presented in this paper show that functional representation extracted by CDAE can help learn features of functional gene ontology categories for their classification in a highly accurate manner. Using this CDAE representation, our method outperforms the previous state-of-the-art classification rate, by improving the average AUC from 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates on input images that were downsampled significantly with respect to the original ones to make it computationally feasible.","This paper present a novel deep learning-based method for learn a functional representation of mammalian neural image . The method use a deep convolutional denoising autoencoder ( CDAE ) for generate an invariant , compact representation of in situ hybridization ( ISH ) image . While most exist method for bio-imaging analysis be not develop to handle image with highly complex anatomical structure , the result present in this paper show that functional representation extract by CDAE can help learn feature of functional gene ontology category for their classification in a highly accurate manner . Using this CDAE representation , our method outperform the previous state-of-the-art classification rate , by improve the average AUC from 0.92 to 0.98 , i.e. , achieve 75 % reduction in error . The method operate on input image that be downsampled significantly with respect to the original one to make it computationally feasible . ","http://arxiv.org/pdf/1711.09663v1","cs.CV	cs.LG	cs.NE	stat.ML","DeepBrain: Functional Representation of Neural In-Situ Hybridization   Images for Gene Ontology Classification Using Deep Convolutional Autoencoders","cido15@gmail.com,	mail@elidavid.com,	nathan@cs.biu.ac.il	nathan@cfar.umd.edu	noalis@gmail.com,	gal.chechik@mail.biu.ac.il"
"Omid Poursaeed	Isay Katsman	Bicheng Gao	Serge Belongie","6","12","2017","In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.","In this paper , we propose novel generative model for create adversarial example , slightly perturb image resemble natural image but maliciously craft to fool pre-trained model . We present trainable deep neural network for transform image to adversarial perturbation . Our propose model can produce image-agnostic and image-dependent perturbation for both targeted and non-targeted attack . We also demonstrate that similar architecture can achieve impressive result in fool classification and semantic segmentation model , obviate the need for hand-crafting attack method for each task . Using extensive experiment on challenge high-resolution datasets such a ImageNet and Cityscapes , we show that our perturbation achieve high fooling rate with small perturbation norm . Moreover , our attack be considerably faster than current iterative method at inference time . ","http://arxiv.org/pdf/1712.02328v1","cs.CV	cs.LG	cs.NE	stat.ML","Generative Adversarial Perturbations","op63@cornell.edu	isk22@cornell.edu	bg455@cornell.edu	sjb344@cornell.edu"
"Logan Engstrom	Brandon Tran	Dimitris Tsipras	Ludwig Schmidt	Aleksander Madry","7","12","2017","We show that simple transformations, namely translations and rotations alone, are sufficient to fool neural network-based vision models on a significant fraction of inputs. This is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. Moreover, fooling rotations and translations are easy to find and require only a few black-box queries to the target model. Overall, our findings emphasize the need for designing robust classifiers even in natural, benign contexts.","We show that simple transformation , namely translation and rotation alone , be sufficient to fool neural network-based vision model on a significant fraction of input . This be in sharp contrast to previous work that rely on more complicated optimization approach that be unlikely to appear outside of a truly adversarial setting . Moreover , fool rotation and translation be easy to find and require only a few black-box query to the target model . Overall , our finding emphasize the need for design robust classifier even in natural , benign context . ","http://arxiv.org/pdf/1712.02779v3","cs.LG	cs.CV	cs.NE	stat.ML","A Rotation and a Translation Suffice: Fooling CNNs with Simple   Transformations",""
"Boyang Deng	Junjie Yan	Dahua Lin","9","12","2017","The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, based on its architecture. Specifically, we develop a unified way to encode individual layers into vectors and bring them together to form an integrated description via LSTM. Taking advantage of the recurrent network's strong expressive power, this method can reliably predict the performances of various network architectures. Our empirical studies showed that it not only achieved accurate predictions but also produced consistent rankings across datasets -- a key desideratum in performance prediction.","The quest for performant network have be a significant force that drive the advancement of deep learning in recent year . While reward , improve network design have never be an easy journey . The large design space combine with the tremendous cost require for network training pose a major obstacle to this endeavor . In this work , we propose a new approach to this problem , namely , predict the performance of a network before training , base on it architecture . Specifically , we develop a unified way to encode individual layer into vector and bring them together to form an integrated description via LSTM . Taking advantage of the recurrent network 's strong expressive power , this method can reliably predict the performance of various network architecture . Our empirical study show that it not only achieved accurate prediction but also produce consistent ranking across datasets -- a key desideratum in performance prediction . ","http://arxiv.org/pdf/1712.03351v1","cs.LG	cs.CV	cs.NE	stat.ML","Peephole: Predicting Network Performance Before Training","billydeng@buaa.edu.cn	yanjunjie@sensetime.com	dhlin@ie.cuhk.edu.hk"
"Abien Fred Agarap","10","12","2017","Convolutional neural networks (CNNs) are similar to "ordinary" neural networks in the sense that they are made up of hidden layers consisting of neurons with "learnable" parameters. These neurons receive inputs, performs a dot product, and then follows it with a non-linearity. The whole network expresses the mapping between raw image pixels and their class scores. Conventionally, the Softmax function is the classifier used at the last layer of this network. However, there have been studies (Alalshekmubarak and Smith, 2013; Agarap, 2017; Tang, 2013) conducted to challenge this norm. The cited studies introduce the usage of linear support vector machine (SVM) in an artificial neural network architecture. This project is yet another take on the subject, and is inspired by (Tang, 2013). Empirical data has shown that the CNN-SVM model was able to achieve a test accuracy of ~99.04% using the MNIST dataset (LeCun, Cortes, and Burges, 2010). On the other hand, the CNN-Softmax was able to achieve a test accuracy of ~99.23% using the same dataset. Both models were also tested on the recently-published Fashion-MNIST dataset (Xiao, Rasul, and Vollgraf, 2017), which is suppose to be a more difficult image classification dataset than MNIST (Zalandoresearch, 2017). This proved to be the case as CNN-SVM reached a test accuracy of ~90.72%, while the CNN-Softmax reached a test accuracy of ~91.86%. The said results may be improved if data preprocessing techniques were employed on the datasets, and if the base CNN model was a relatively more sophisticated than the one used in this study.","Convolutional neural network ( CNNs ) be similar to `` ordinary '' neural network in the sense that they be make up of hidden layer consist of neuron with `` learnable '' parameter . These neuron receive input , perform a dot product , and then follow it with a non-linearity . The whole network express the mapping between raw image pixel and their class score . Conventionally , the Softmax function be the classifier use at the last layer of this network . However , there have be study ( Alalshekmubarak and Smith , 2013 ; Agarap , 2017 ; Tang , 2013 ) conduct to challenge this norm . The cited study introduce the usage of linear support vector machine ( SVM ) in an artificial neural network architecture . This project be yet another take on the subject , and be inspire by ( Tang , 2013 ) . Empirical data have show that the CNN-SVM model be able to achieve a test accuracy of ~99.04 % use the MNIST dataset ( LeCun , Cortes , and Burges , 2010 ) . On the other hand , the CNN-Softmax be able to achieve a test accuracy of ~99.23 % use the same dataset . Both model be also test on the recently-published Fashion-MNIST dataset ( Xiao , Rasul , and Vollgraf , 2017 ) , which be suppose to be a more difficult image classification dataset than MNIST ( Zalandoresearch , 2017 ) . This prove to be the case a CNN-SVM reach a test accuracy of ~90.72 % , while the CNN-Softmax reach a test accuracy of ~91.86 % . The say result may be improve if data preprocessing technique be employ on the datasets , and if the base CNN model be a relatively more sophisticated than the one use in this study . ","http://arxiv.org/pdf/1712.03541v1","cs.CV	cs.LG	cs.NE	stat.ML","An Architecture Combining Convolutional Neural Network (CNN) and Support   Vector Machine (SVM) for Image Classification","abien.fred.agarap@adamson.edu.ph"
"Ekaba Bisong","22","12","2017","Artifical Neural Networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for Hebbian learning, that is "neurons that wire together, fire together". However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred. Such a constraint though may have sufficed for a while, is now no longer feasible in the era of very-large-scale machine learning, coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. To solve this problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are introduced as a viable alternative to the backpropagation algorithm. This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as opposed to a standard neural interface using multilayer perceptron MLP. SG-DNI shows good promise, in that it not only captures the learning problem, it is also over 3-fold faster due to it asynchronous learning capabilities.","Artifical Neural Networks be a particular class of learn system model after biological neural function with an interesting penchant for Hebbian learning , that be `` neuron that wire together , fire together '' . However , unlike their natural counterpart , artificial neural network have a close and stringent coupling between the module of neuron in the network . This coupling or lock imposes upon the network a strict and inflexible structure that prevent layer in the network from update their weight until a full feed-forward and backward pas have occur . Such a constraint though may have suffice for a while , be now no longer feasible in the era of very-large-scale machine learning , couple with the increase desire for parallelization of the learning process across multiple computing infrastructure . To solve this problem , synthetic gradient ( SG ) with decoupled neural interface ( DNI ) be introduce a a viable alternative to the backpropagation algorithm . This paper perform a speed benchmark to compare the speed and accuracy capability of SG-DNI a oppose to a standard neural interface use multilayer perceptron MLP . SG-DNI show good promise , in that it not only capture the learning problem , it be also over 3-fold faster due to it asynchronous learn capability . ","http://arxiv.org/pdf/1712.08314v2","cs.LG	cs.CV	cs.NE	stat.ML","Benchmarking Decoupled Neural Interfaces with Synthetic Gradients","ekaba.bisong@carleton.ca"
"Amin Fehri	Santiago Velasco-Forero	Fernand Meyer","20","2","2018","Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that takes into account any prior spatial information and outputs a hierarchical segmentation that emphasizes the contours or regions of interest while preserving the important structures in the image. An application of this method to the weakly-supervised segmentation problem is presented.","Image segmentation be the process of partition an image into a set of meaningful region accord to some criterion . Hierarchical segmentation have emerge a a major trend in this regard a it favor the emergence of important region at different scale . On the other hand , many method allow u to have prior information on the position of structure of interest in the image . In this paper , we present a versatile hierarchical segmentation method that take into account any prior spatial information and output a hierarchical segmentation that emphasize the contour or region of interest while preserve the important structure in the image . An application of this method to the weakly-supervised segmentation problem be present . ","http://arxiv.org/pdf/1802.07008v1","stat.ML	cs.CV	cs.LG	cs.NE","Segmentation hiÃ©rarchique faiblement supervisÃ©e","amin.fehri@mines-paristech.fr}	santiago.velasco@mines-paristech.fr}	fernand.meyer@mines-paristech.fr@mines-paristech.fr}"
"Mark D. McDonnell","23","2","2018","For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide residual networks as our main baseline, our approach simplifies existing methods that binarize weights by applying the sign function in training; we apply scaling factors for each layer with constant unlearned values equal to the layer-specific standard deviations used for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with 1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve error rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We also considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test results of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error rates halve previously reported values, and are within about 1% of our error-rates for the same network with full-precision weights. For networks that overfit, we also show significant improvements in error rate by not learning batch normalization scale and offset parameters. This applies to both full precision and 1-bit-per-weight networks. Using a warm-restart learning-rate schedule, we found that training for 1-bit-per-weight is just as fast as full-precision networks, with better accuracy than standard schedules, and achieved about 98%-99% of peak performance in just 62 training epochs for CIFAR-10/100. For full training code and trained models in MATLAB, Keras and PyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .","For fast and energy-efficient deployment of trained deep neural network on resource-constrained embed hardware , each learn weight parameter should ideally be represent and store use a single bit . Error-rates usually increase when this requirement be impose . Here , we report large improvement in error rate on multiple datasets , for deep convolutional neural network deploy with 1-bit-per-weight . Using wide residual network a our main baseline , our approach simplify exist method that binarize weight by apply the sign function in training ; we apply scale factor for each layer with constant unlearned value equal to the layer-specific standard deviation use for initialization . For CIFAR-10 , CIFAR-100 and ImageNet , and model with 1-bit-per-weight require less than 10 MB of parameter memory , we achieve error rate of 3.9 % , 18.5 % and 26.0 % / 8.5 % ( Top-1 / Top-5 ) respectively . We also consider MNIST , SVHN and ImageNet32 , achieve 1-bit-per-weight test result of 0.27 % , 1.9 % , and 41.3 % / 19.1 % respectively . For CIFAR , our error rate halve previously report value , and be within about 1 % of our error-rates for the same network with full-precision weight . For network that overfit , we also show significant improvement in error rate by not learn batch normalization scale and offset parameter . This apply to both full precision and 1-bit-per-weight network . Using a warm-restart learning-rate schedule , we find that train for 1-bit-per-weight be just as fast a full-precision network , with good accuracy than standard schedule , and achieve about 98 % -99 % of peak performance in just 62 train epoch for CIFAR-10/100 . For full training code and trained model in MATLAB , Keras and PyTorch see https : //github.com/McDonnell-Lab/1-bit-per-weight/ . ","http://arxiv.org/pdf/1802.08530v1","cs.LG	cs.CV	cs.NE	stat.ML","Training wide residual networks for deployment using a single bit for   each weight","mark.mcdonnell@unisa.edu.au"
"Abien Fred Agarap","22","3","2018","We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.","We introduce the use of rectified linear unit ( ReLU ) a the classification function in a deep neural network ( DNN ) . Conventionally , ReLU be use a an activation function in DNNs , with Softmax function a their classification function . However , there have be several study on use a classification function other than Softmax , and this study be an addition to those . We accomplish this by take the activation of the penultimate layer $ h_ { n - 1 } $ in a neural network , then multiply it by weight parameter $ \theta $ to get the raw score $ o_ { i } $ . Afterwards , we threshold the raw score $ o_ { i } $ by $ 0 $ , i.e . $ f ( o ) = \max ( 0 , o_ { i } ) $ , where $ f ( o ) $ be the ReLU function . We provide class prediction $ \hat { y } $ through argmax function , i.e . argmax $ f ( x ) $ . ","http://arxiv.org/pdf/1803.08375v1","cs.NE	cs.CV	cs.LG	stat.ML","Deep Learning using Rectified Linear Units (ReLU)","abien.fred.agarap@adamson.edu.ph"
"Djork-ArnÃ© Clevert	Andreas Mayr	Thomas Unterthiner	Sepp Hochreiter","23","2","2015","We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.","We propose rectified factor network ( RFNs ) to efficiently construct very sparse , non-linear , high-dimensional representation of the input . RFN model identify rare and small event in the input , have a low interference between code unit , have a small reconstruction error , and explain the data covariance structure . RFN learning be a generalized alternating minimization algorithm derive from the posterior regularization method which enforce non-negative and normalized posterior mean . We proof convergence and correctness of the RFN learning algorithm . On benchmark , RFNs be compare to other unsupervised method like autoencoders , RBMs , factor analysis , ICA , and PCA . In contrast to previous sparse cod method , RFNs yield sparser code , capture the data 's covariance structure more precisely , and have a significantly small reconstruction error . We test RFNs a pretraining technique for deep network on different vision datasets , where RFNs be superior to RBMs and autoencoders . On gene expression data from two pharmaceutical drug discovery study , RFNs detect small and rare gene module that reveal highly relevant new biological insight which be so far miss by other unsupervised method . ","http://arxiv.org/pdf/1502.06464v2","cs.LG	cs.CV	cs.NE	stat.ML","Rectified Factor Networks","okko@bioinf.jku.at	mayr@bioinf.jku.at	unterthiner@bioinf.jku.at	hochreit@bioinf.jku.at"
"Qi Wang	Joseph JaJa","18","11","2013","Motivated by an important insight from neural science, we propose a new framework for understanding the success of the recently proposed "maxout" networks. The framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time. Elaborating further on this insight, we propose a novel deep network architecture, called "channel-out" network, which takes a much better advantage of sparse pathway encoding. In channel-out networks, pathways are not only formed a posteriori, but they are also actively selected according to the inference outputs from the lower layers. From a mathematical perspective, channel-out networks can represent a wider class of piece-wise continuous functions, thereby endowing the network with more expressive power than that of maxout networks. We test our channel-out networks on several well-known image classification benchmarks, setting new state-of-the-art performance on CIFAR-100 and STL-10, which represent some of the "harder" image classification benchmarks.","Motivated by an important insight from neural science , we propose a new framework for understand the success of the recently propose `` maxout '' network . The framework be base on encode information on sparse pathway and recognize the correct pathway at inference time . Elaborating far on this insight , we propose a novel deep network architecture , call `` channel-out '' network , which take a much good advantage of sparse pathway encode . In channel-out network , pathway be not only form a posteriori , but they be also actively select accord to the inference output from the low layer . From a mathematical perspective , channel-out network can represent a wide class of piece-wise continuous function , thereby endow the network with more expressive power than that of maxout network . We test our channel-out network on several well-known image classification benchmark , set new state-of-the-art performance on CIFAR-100 and STL-10 , which represent some of the `` harder '' image classification benchmark . ","http://arxiv.org/pdf/1312.1909v1","cs.NE	cs.CV	cs.LG	stat.ML","From Maxout to Channel-Out: Encoding Information on Sparse Pathways","qwang37@umiacs.umd.edu	joseph@umiacs.umd.edu"
"Takashi Shinozaki	Yasushi Naruse","20","12","2013","We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer for robust leaning and revising internal representation in a multilayer neural network.","We propose a novel learn method for multilayered neural network which use feedforward supervisory signal and associate classification of a new input with that of pre-trained input . The propose method effectively use rich input information in the early layer for robust leaning and revise internal representation in a multilayer neural network . ","http://arxiv.org/pdf/1312.5845v7","cs.NE	cs.CV	cs.LG	stat.ML","Competitive Learning with Feedforward Supervisory Signal for Pre-trained   Multilayered Networks",""
"Chen-Yu Lee	Saining Xie	Patrick Gallagher	Zhengyou Zhang	Zhuowen Tu","18","9","2014","Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce "companion objective" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).","Our propose deeply-supervised net ( DSN ) method simultaneously minimize classification error while make the learning process of hidden layer direct and transparent . We make an attempt to boost the classification performance by study a new formulation in deep network . Three aspect in convolutional neural network ( CNN ) style architecture be be look at : ( 1 ) transparency of the intermediate layer to the overall classification ; ( 2 ) discriminativeness and robustness of learned feature , especially in the early layer ; ( 3 ) effectiveness in train due to the presence of the exploding and vanish gradient . We introduce `` companion objective '' to the individual hidden layer , in addition to the overall objective at the output layer ( a different strategy to layer-wise pre-training ) . We extend technique from stochastic gradient method to analyze our algorithm . The advantage of our method be evident and our experimental result on benchmark datasets show significant performance gain over exist method ( e.g . all state-of-the-art result on MNIST , CIFAR-10 , CIFAR-100 , and SVHN ) . ","http://arxiv.org/pdf/1409.5185v2","stat.ML	cs.CV	cs.LG	cs.NE","Deeply-Supervised Nets","chl260@ucsd.edu	s9xie@ucsd.edu	rexaran@gmail.com	zhang@microsoft.com	ztu@ucsd.edu"
"Behnam Neyshabur	Ruslan Salakhutdinov	Nathan Srebro","8","6","2015","We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.","We revisit the choice of SGD for train deep neural network by reconsider the appropriate geometry in which to optimize the weight . We argue for a geometry invariant to rescaling of weight that do not affect the output of the network , and suggest Path-SGD , which be an approximate steepest descent method with respect to a path-wise regularizer relate to max-norm regularization . Path-SGD be easy and efficient to implement and lead to empirical gain over SGD and AdaGrad . ","http://arxiv.org/pdf/1506.02617v1","cs.LG	cs.CV	cs.NE	stat.ML","Path-SGD: Path-Normalized Optimization in Deep Neural Networks",""
"Alan Mosca	George D. Magoulas","15","9","2015","The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method for training Deep Neural Networks as standalone components and in ensemble formulations. Results on the MNIST dataset show that the proposed modification alleviates standard Rprop's problems demonstrating improved learning speed and accuracy.","The Resilient Propagation ( Rprop ) algorithm have be very popular for backpropagation training of multilayer feed-forward neural network in various application . The standard Rprop however encounter difficulty in the context of deep neural network a typically happen with gradient-based learn algorithm . In this paper , we propose a modification of the Rprop that combine standard Rprop step with a special drop out technique . We apply the method for train Deep Neural Networks a standalone component and in ensemble formulation . Results on the MNIST dataset show that the propose modification alleviate standard Rprop 's problem demonstrate improve learn speed and accuracy . ","http://arxiv.org/pdf/1509.04612v2","cs.NE	cs.CV	cs.LG	stat.ML","Adapting Resilient Propagation for Deep Learning","a.mosca@dcs.bbk.ac.uk	gmagoulas@dcs.bbk.ac.uk"
"Nastaran Mohammadian Rad	Andrea Bizzego	Seyed Mostafa Kia	Giuseppe Jurman	Paola Venuti	Cesare Furlanello","5","11","2015","Autism Spectrum Disorders (ASDs) are often associated with specific atypical postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have a specific visibility. While the identification and the quantification of SMM patterns remain complex, its automation would provide support to accurate tuning of the intervention in the therapy of autism. Therefore, it is essential to develop automatic SMM detection systems in a real world setting, taking care of strong inter-subject and intra-subject variability. Wireless accelerometer sensing technology can provide a valid infrastructure for real-time SMM detection, however such variability remains a problem also for machine learning methods, in particular whenever handcrafted features extracted from accelerometer signal are considered. Here, we propose to employ the deep learning paradigm in order to learn discriminating features from multi-sensor accelerometer signals. Our results provide preliminary evidence that feature learning and transfer learning embedded in the deep architecture achieve higher accurate SMM detectors in longitudinal scenarios.","Autism Spectrum Disorders ( ASDs ) be often associate with specific atypical postural or motor behavior , of which Stereotypical Motor Movements ( SMMs ) have a specific visibility . While the identification and the quantification of SMM pattern remain complex , it automation would provide support to accurate tuning of the intervention in the therapy of autism . Therefore , it be essential to develop automatic SMM detection system in a real world setting , take care of strong inter-subject and intra-subject variability . Wireless accelerometer sense technology can provide a valid infrastructure for real-time SMM detection , however such variability remain a problem also for machine learning method , in particular whenever handcraft feature extract from accelerometer signal be consider . Here , we propose to employ the deep learning paradigm in order to learn discriminate feature from multi-sensor accelerometer signal . Our result provide preliminary evidence that feature learning and transfer learning embed in the deep architecture achieve high accurate SMM detector in longitudinal scenario . ","http://arxiv.org/pdf/1511.01865v3","cs.NE	cs.CV	cs.LG	stat.ML","Convolutional Neural Network for Stereotypical Motor Movement Detection   in Autism",""
"Sasha Targ	Diogo Almeida	Kevin Lyman","25","3","2016","Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.","Residual network ( ResNets ) have recently achieve state-of-the-art on challenge computer vision task . We introduce Resnet in Resnet ( RiR ) : a deep dual-stream architecture that generalize ResNets and standard CNNs and be easily implement with no computational overhead . RiR consistently improve performance over ResNets , outperform architecture with similar amount of augmentation on CIFAR-10 , and establish a new state-of-the-art on CIFAR-100 . ","http://arxiv.org/pdf/1603.08029v1","cs.LG	cs.CV	cs.NE	stat.ML","Resnet in Resnet: Generalizing Residual Architectures","diogo@enlitic.com	kevin@enlitic.com"
"Mohammad Javad Shafiee	Alexander Wong","6","9","2016","There has been significant recent interest towards achieving highly efficient deep neural network architectures. A promising paradigm for achieving this is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations. An important aspect of evolutionary deep intelligence is the genetic encoding scheme used to mimic heredity, which can have a significant impact on the quality of offspring deep neural networks. Motivated by the neurobiological phenomenon of synaptic clustering, we introduce a new genetic encoding scheme where synaptic probability is driven towards the formation of a highly sparse set of synaptic clusters. Experimental results for the task of image classification demonstrated that the synthesized offspring networks using this synaptic cluster-driven genetic encoding scheme can achieve state-of-the-art performance while having network architectures that are not only significantly more efficient (with a ~125-fold decrease in synapses for MNIST) compared to the original ancestor network, but also tailored for GPU-accelerated machine learning applications.","There have be significant recent interest towards achieve highly efficient deep neural network architecture . A promising paradigm for achieve this be the concept of evolutionary deep intelligence , which attempt to mimic biological evolution process to synthesize highly-efficient deep neural network over successive generation . An important aspect of evolutionary deep intelligence be the genetic encoding scheme use to mimic heredity , which can have a significant impact on the quality of offspring deep neural network . Motivated by the neurobiological phenomenon of synaptic clustering , we introduce a new genetic encoding scheme where synaptic probability be drive towards the formation of a highly sparse set of synaptic cluster . Experimental result for the task of image classification demonstrate that the synthesized offspring network use this synaptic cluster-driven genetic encode scheme can achieve state-of-the-art performance while have network architecture that be not only significantly more efficient ( with a ~125-fold decrease in synapsis for MNIST ) compare to the original ancestor network , but also tailor for GPU-accelerated machine learning application . ","http://arxiv.org/pdf/1609.01360v2","cs.LG	cs.CV	cs.NE	stat.ML","Evolutionary Synthesis of Deep Neural Networks via Synaptic   Cluster-driven Genetic Encoding","mjshafiee@uwaterloo.ca	a28wong@uwaterloo.ca"
"Andrew Brock	Theodore Lim	J. M. Ritchie	Nick Weston","22","9","2016","The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.","The increasingly photorealistic sample quality of generative image model suggest their feasibility in application beyond image generation . We present the Neural Photo Editor , an interface that leverage the power of generative neural network to make large , semantically coherent change to exist image . To tackle the challenge of achieve accurate reconstruction without loss of feature quality , we introduce the Introspective Adversarial Network , a novel hybridization of the VAE and GAN . Our model efficiently capture long-range dependency through use of a computational block base on weight-shared dilated convolution , and improve generalization performance with Orthogonal Regularization , a novel weight regularization method . We validate our contribution on CelebA , SVHN , and CIFAR-100 , and produce sample and reconstruction with high visual fidelity . ","http://arxiv.org/pdf/1609.07093v3","cs.LG	cs.CV	cs.NE	stat.ML","Neural Photo Editing with Introspective Adversarial Networks","ajb5@hw.ac.uk	t.lim@hw.ac.uk	j.m.ritchie@hw.ac.uk	Nick.Weston@renishaw.com"
"Tolga Bolukbasi	Joseph Wang	Ofer Dekel	Venkatesh Saligrama","25","2","2017","We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1%) loss of top5 accuracy.","We present an approach to adaptively utilize deep neural network in order to reduce the evaluation time on new example without loss of accuracy . Rather than attempt to redesign or approximate exist network , we propose two scheme that adaptively utilize network . We first pose an adaptive network evaluation scheme , where we learn a system to adaptively choose the component of a deep network to be evaluate for each example . By allow example correctly classify use early layer of the system to exit , we avoid the computational time associate with full evaluation of the network . We extend this to learn a network selection system that adaptively select the network to be evaluate for each example . We show that computational time can be dramatically reduce by exploit the fact that many example can be correctly classify use relatively efficient network and that complex , computationally costly network be only necessary for a small fraction of example . We pose a global objective for learn an adaptive early exit or network selection policy and solve it by reduce the policy learn problem to a layer-by-layer weighted binary classification problem . Empirically , these approach yield dramatic reduction in computational cost , with up to a 2.8x speedup on state-of-the-art network from the ImageNet image recognition challenge with minimal ( < 1 % ) loss of top5 accuracy . ","http://arxiv.org/pdf/1702.07811v2","cs.LG	cs.CV	cs.NE	stat.ML","Adaptive Neural Networks for Efficient Inference",""
"Zhengyang Wang	Hao Yuan	Shuiwang Ji","18","5","2017","The key idea of variational auto-encoders (VAEs) resembles that of traditional auto-encoder models in which spatial information is supposed to be explicitly encoded in the latent space. However, the latent variables in VAEs are vectors, which are commonly interpreted as multiple feature maps of size 1x1. Such representations can only convey spatial information implicitly when coupled with powerful decoders. In this work, we propose spatial VAEs that use latent variables as feature maps of larger size to explicitly capture spatial information. This is achieved by allowing the latent variables to be sampled from matrix-variate normal (MVN) distributions whose parameters are computed from the encoder network. To increase dependencies among locations on latent feature maps and reduce the number of parameters, we further propose spatial VAEs via low-rank MVN distributions. Experimental results show that the proposed spatial VAEs outperform original VAEs in capturing rich structural and spatial information.","The key idea of variational auto-encoders ( VAEs ) resemble that of traditional auto-encoder model in which spatial information be suppose to be explicitly encode in the latent space . However , the latent variable in VAEs be vector , which be commonly interpret a multiple feature map of size 1x1 . Such representation can only convey spatial information implicitly when couple with powerful decoder . In this work , we propose spatial VAEs that use latent variable a feature map of large size to explicitly capture spatial information . This be achieve by allow the latent variable to be sample from matrix-variate normal ( MVN ) distribution whose parameter be compute from the encoder network . To increase dependency among location on latent feature map and reduce the number of parameter , we further propose spatial VAEs via low-rank MVN distribution . Experimental result show that the propose spatial VAEs outperform original VAEs in capture rich structural and spatial information . ","http://arxiv.org/pdf/1705.06821v1","cs.LG	cs.CV	cs.NE	stat.ML","Spatial Variational Auto-Encoding via Matrix-Variate Normal   Distributions","zwang6@eecs.wsu.edu	hao.yuan@wsu.edu	sji@eecs.wsu.edu"
"Jun Li	Yongjun Chen	Lei Cai	Ian Davidson	Shuiwang Ji","24","5","2017","The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on natural and biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.","The key idea of current deep learning method for dense prediction be to apply a model on a regular patch center on each pixel to make pixel-wise prediction . These method be limit in the sense that the patch be determine by network architecture instead of learn from data . In this work , we propose the dense transformer network , which can learn the shape and size of patch from data . The dense transformer network employ an encoder-decoder architecture , and a pair of dense transformer module be insert into each of the encoder and decoder path . The novelty of this work be that we provide technical solution for learn the shape and size of patch from data and efficiently restore the spatial correspondence require for dense prediction . The propose dense transformer module be differentiable , thus the entire network can be train . We apply the propose network on natural and biological image segmentation task and show superior performance be achieve in comparison to baseline method . ","http://arxiv.org/pdf/1705.08881v2","cs.CV	cs.LG	cs.NE	stat.ML","Dense Transformer Networks","jun.li3@wsu.edu	yongjun.chen@wsu.edu	davidson@cs.ucdavis.edu	lei.cai@wsu.edu	sji@eecs.wsu.edu"
"Saikat Chatterjee	Alireza M. Javid	Mostafa Sadeghi	Partha P. Mitra	Mikael Skoglund","23","10","2017","We develop an algorithm for systematic design of a large artificial neural network using a progression property. We find that some non-linear functions, such as the rectifier linear unit and its derivatives, hold the property. The systematic design addresses the choice of network size and regularization of parameters. The number of nodes and layers in network increases in progression with the objective of consistently reducing an appropriate cost. Each layer is optimized at a time, where appropriate parameters are learned using convex optimization. Regularization parameters for convex optimization do not need a significant manual effort for tuning. We also use random instances for some weight matrices, and that helps to reduce the number of parameters we learn. The developed network is expected to show good generalization power due to appropriate regularization and use of random weights in the layers. This expectation is verified by extensive experiments for classification and regression problems, using standard databases.","We develop an algorithm for systematic design of a large artificial neural network use a progression property . We find that some non-linear function , such a the rectifier linear unit and it derivative , hold the property . The systematic design address the choice of network size and regularization of parameter . The number of node and layer in network increase in progression with the objective of consistently reduce an appropriate cost . Each layer be optimize at a time , where appropriate parameter be learn use convex optimization . Regularization parameter for convex optimization do not need a significant manual effort for tune . We also use random instance for some weight matrix , and that help to reduce the number of parameter we learn . The developed network be expect to show good generalization power due to appropriate regularization and use of random weight in the layer . This expectation be verify by extensive experiment for classification and regression problem , use standard database . ","http://arxiv.org/pdf/1710.08177v1","cs.NE	cs.CV	cs.LG	stat.ML","Progressive Learning for Systematic Design of Large Neural Networks",""
"Shibani Santurkar	Ludwig Schmidt	Aleksander MÄdry","2","11","2017","A fundamental, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether GANs are actually able to capture the key characteristics of the datasets they are trained on. The current approaches to examining this issue require significant human supervision, such as visual inspection of sampled images, and often offer only fairly limited scalability. In this paper, we propose new techniques that employ a classification-based perspective to evaluate synthetic GAN distributions and their capability to accurately reflect the essential properties of the training data. These techniques require only minimal human supervision and can easily be scaled and adapted to evaluate a variety of state-of-the-art GANs on large, popular datasets. Our analysis indicates that GANs have significant problems in reproducing the more distributional properties of the training dataset. In particular, when seen through the lens of classification, the diversity of GAN data is orders of magnitude less than that of the original data.","A fundamental , and still largely unanswered , question in the context of Generative Adversarial Networks ( GANs ) be whether GANs be actually able to capture the key characteristic of the datasets they be train on . The current approach to examine this issue require significant human supervision , such a visual inspection of sampled image , and often offer only fairly limited scalability . In this paper , we propose new technique that employ a classification-based perspective to evaluate synthetic GAN distribution and their capability to accurately reflect the essential property of the training data . These technique require only minimal human supervision and can easily be scale and adapt to evaluate a variety of state-of-the-art GANs on large , popular datasets . Our analysis indicate that GANs have significant problem in reproduce the more distributional property of the training dataset . In particular , when see through the lens of classification , the diversity of GAN data be order of magnitude less than that of the original data . ","http://arxiv.org/pdf/1711.00970v3","cs.LG	cs.CV	cs.NE	stat.ML","A Classification-Based Perspective on GAN Distributions","shibani@mit.edu	ludwigs@mit.edu	madry@mit.edu"
"Ethan Perez	Harm de Vries	Florian Strub	Vincent Dumoulin	Aaron Courville","10","7","2017","Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to visually reason successfully but are hand-crafted for reasoning. We show that a general-purpose, Conditional Batch Normalization approach achieves state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4% error rate. We outperform the next best end-to-end method (4.5%) and even methods that use extra supervision (3.1%). We probe our model to shed light on how it reasons, showing it has learned a question-dependent, multi-step process. Previous work has operated under the assumption that visual reasoning calls for a specialized architecture, but we show that a general architecture with proper conditioning can learn to visually reason effectively.","Achieving artificial visual reason - the ability to answer image-related question which require a multi-step , high-level process - be an important step towards artificial general intelligence . This multi-modal task require learn a question-dependent , structure reason process over image from language . Standard deep learning approach tend to exploit bias in the data rather than learn this underlying structure , while lead method learn to visually reason successfully but be hand-crafted for reason . We show that a general-purpose , Conditional Batch Normalization approach achieve state-of-the-art result on the CLEVR Visual Reasoning benchmark with a 2.4 % error rate . We outperform the next best end-to-end method ( 4.5 % ) and even method that use extra supervision ( 3.1 % ) . We probe our model to shed light on how it reason , show it have learn a question-dependent , multi-step process . Previous work have operate under the assumption that visual reasoning call for a specialized architecture , but we show that a general architecture with proper conditioning can learn to visually reason effectively . ","http://arxiv.org/pdf/1707.03017v5","cs.CV	cs.AI	cs.CL	stat.ML","Learning Visual Reasoning Without Strong Priors","ethanperez@rice.edu,	mail@harmdevries.com,	florian.strub@inria.fr	dumouliv@iro.umontreal.ca,	courvila@iro.umontreal.ca"
"Jieyu Zhao	Tianlu Wang	Mark Yatskar	Vicente Ordonez	Kai-Wei Chang","29","7","2017","Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classification and visual semantic role labeling. We find that (a) datasets for these tasks contain significant gender bias and (b) models trained on these datasets further amplify existing bias. For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. Our method results in almost no performance loss for the underlying recognition task but decreases the magnitude of bias amplification by 47.5% and 40.5% for multilabel classification and visual semantic role labeling, respectively.","Language be increasingly be use to define rich visual recognition problem with support image collection source from the web . Structured prediction model be use in these task to take advantage of correlation between co-occurring label and visual input but risk inadvertently encode social bias find in web corpus . In this work , we study data and model associate with multilabel object classification and visual semantic role label . We find that ( a ) datasets for these task contain significant gender bias and ( b ) model train on these datasets far amplify exist bias . For example , the activity cooking be over 33 % more likely to involve female than male in a training set , and a trained model further amplify the disparity to 68 % at test time . We propose to inject corpus-level constraint for calibrate exist structure prediction model and design an algorithm base on Lagrangian relaxation for collective inference . Our method result in almost no performance loss for the underlying recognition task but decrease the magnitude of bias amplification by 47.5 % and 40.5 % for multilabel classification and visual semantic role labeling , respectively . ","http://arxiv.org/pdf/1707.09457v1","cs.AI	cs.CL	cs.CV	stat.ML","Men Also Like Shopping: Reducing Gender Bias Amplification using   Corpus-level Constraints","jz4fu@virginia.edu	tw8cb@virginia.edu	vicente@virginia.edu	kc2wc@virginia.edu	my89@cs.washington.edu"
"Guillem Collell	Luc Van Gool	Marie-Francine Moens","18","11","2017","Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., "on", "below", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., "glass on table"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., "man riding horse"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output ("where is the man w.r.t. a horse when the man is walking the horse?"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,"man walking dog") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., "dog"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.","Spatial understanding be a fundamental problem with wide-reaching real-world application . The representation of spatial knowledge be often model with spatial template , i.e. , region of acceptability of two object under an explicit spatial relationship ( e.g. , `` on '' , `` below '' , etc. ) . In contrast with prior work that restrict spatial template to explicit spatial preposition ( e.g. , `` glass on table '' ) , here we extend this concept to implicit spatial language , i.e. , those relationship ( generally action ) for which the spatial arrangement of the object be only implicitly imply ( e.g. , `` man rid horse '' ) . In contrast with explicit relationship , predict spatial arrangement from implicit spatial language require significant common sense spatial understanding . Here , we introduce the task of predict spatial template for two object under a relationship , which can be see a a spatial question-answering task with a ( 2D ) continuous output ( `` where be the man w.r.t . a horse when the man be walk the horse ? '' ) . We present two simple neural-based model that leverage annotate image and structure text to learn this task . The good performance of these model reveals that spatial location be to a large extent predictable from implicit spatial language . Crucially , the model attain similar performance in a challenging generalized setting , where the object-relation-object combination ( e.g. , '' man walk dog '' ) have never be see before . Next , we go one step far by present the model with unseen object ( e.g. , `` dog '' ) . In this scenario , we show that leverage word embeddings enable the model to output accurate spatial prediction , prove that the model acquire solid common sense spatial knowledge allow for such generalization . ","http://arxiv.org/pdf/1711.06821v2","cs.AI	cs.CL	cs.CV	stat.ML","Acquiring Common Sense Spatial Knowledge through Implicit Spatial   Templates","gcollell@kuleuven.be	vangool@vision.ee.ethz.ch	sien.moens@cs.kuleuven.be"
"Ethan Perez	Florian Strub	Harm de Vries	Vincent Dumoulin	Aaron Courville","22","9","2017","We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.","We introduce a general-purpose conditioning method for neural network call FiLM : Feature-wise Linear Modulation . FiLM layer influence neural network computation via a simple , feature-wise affine transformation base on condition information . We show that FiLM layer be highly effective for visual reason - answer image-related question which require a multi-step , high-level process - a task which have prove difficult for standard deep learning method that do not explicitly model reason . Specifically , we show on visual reasoning task that FiLM layer 1 ) halve state-of-the-art error for the CLEVR benchmark , 2 ) modulate feature in a coherent manner , 3 ) be robust to ablation and architectural modification , and 4 ) generalize well to challenge , new data from few example or even zero-shot . ","http://arxiv.org/pdf/1709.07871v2","cs.CV	cs.AI	cs.CL	stat.ML","FiLM: Visual Reasoning with a General Conditioning Layer","ethanperez@rice.edu,	florian.strub@inria.fr,	dumouliv@iro.umontreal.ca	courvila@iro.umontreal.ca"
"Ivan Titov	Ehsan Khoddam","8","12","2014","We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.","We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling model . Our model consist of two component : ( 1 ) an encoding component : a semantic role label model which predict role give a rich set of syntactic and lexical feature ; ( 2 ) a reconstruction component : a tensor factorization model which rely on role to predict argument filler . When the component be estimate jointly to minimize error in argument reconstruction , the induced role largely correspond to role define in annotated resource . Our method performs on par with most accurate role induction method on English and German , even though , unlike these previous approach , we do not incorporate any prior linguistic knowledge about the language . ","http://arxiv.org/pdf/1412.2812v1","cs.CL	cs.AI	cs.LG	stat.ML","Unsupervised Induction of Semantic Roles within a Reconstruction-Error   Minimization Framework","titov@uva.nl	e.khoddammohammadi@uva.nl"
"Tolga Bolukbasi	Kai-Wei Chang	James Zou	Venkatesh Saligrama	Adam Kalai","21","7","2016","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","The blind application of machine learning run the risk of amplify bias present in data . Such a danger be face u with word embedding , a popular framework to represent text data a vector which have be use in many machine learning and natural language processing task . We show that even word embeddings train on Google News article exhibit female/male gender stereotype to a disturbing extent . This raise concern because their widespread use , a we describe , often tend to amplify these bias . Geometrically , gender bias be first show to be capture by a direction in the word embedding . Second , gender neutral word be show to be linearly separable from gender definition word in the word embedding . Using these property , we provide a methodology for modify an embedding to remove gender stereotype , such a the association between between the word receptionist and female , while maintain desired association such a between the word queen and female . We define metric to quantify both direct and indirect gender bias in embeddings , and develop algorithm to `` debias '' the embedding . Using crowd-worker evaluation as well a standard benchmark , we empirically demonstrate that our algorithm significantly reduce gender bias in embeddings while preserve the it useful property such a the ability to cluster related concept and to solve analogy task . The resulting embeddings can be use in application without amplify gender bias . ","http://arxiv.org/pdf/1607.06520v1","cs.CL	cs.AI	cs.LG	stat.ML","Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word   Embeddings","tolgab@bu.edu,	kw@kwchang.net,	jamesyzou@gmail.com,	srv@bu.edu,	adam.kalai@microsoft.com"
"Adji B. Dieng	Chong Wang	Jianfeng Gao	John Paisley","5","11","2016","In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of $6.28\%$. This is comparable to the state-of-the-art $5.91\%$ resulting from a semi-supervised approach. Finally, TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.","In this paper , we propose TopicRNN , a recurrent neural network ( RNN ) -based language model design to directly capture the global semantic meaning relate word in a document via latent topic . Because of their sequential nature , RNNs be good at capture the local structure of a word sequence - both semantic and syntactic - but might face difficulty remember long-range dependency . Intuitively , these long-range dependency be of semantic nature . In contrast , latent topic model be able to capture the global underlying semantic structure of a document but do not account for word ordering . The propose TopicRNN model integrate the merit of RNNs and latent topic model : it capture local ( syntactic ) dependency use an RNN and global ( semantic ) dependency use latent topic . Unlike previous work on contextual RNN language modeling , our model be learn end-to-end . Empirical result on word prediction show that TopicRNN outperform exist contextual RNN baseline . In addition , TopicRNN can be use a an unsupervised feature extractor for document . We do this for sentiment analysis on the IMDB movie review dataset and report an error rate of $ 6.28\ % $ . This be comparable to the state-of-the-art $ 5.91\ % $ result from a semi-supervised approach . Finally , TopicRNN also yield sensible topic , make it a useful alternative to document model such a latent Dirichlet allocation . ","http://arxiv.org/pdf/1611.01702v2","cs.CL	cs.AI	cs.LG	stat.ML","TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency","abd2141@columbia.edu	chowang@microsoft.com	jpaisley@columbia.edu	jfgao@microsoft.com"
"Liwen Zhang	John Winn	Ryota Tomioka","7","11","2016","We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that performs question answering about the entities in the knowledge base. The proposed attention model can handle both the propagation of uncertainty when following a series of relations and also the conjunction of conditions in a natural way. On a dataset of soccer players who participated in the FIFA World Cup 2014, we demonstrate that our model can handle both path queries and conjunctive queries well.","We propose the Gaussian attention model for content-based neural memory access . With the propose attention model , a neural network have the additional degree of freedom to control the focus of it attention from a laser sharp attention to a broad attention . It be applicable whenever we can assume that the distance in the latent space reflect some notion of semantics . We use the propose attention model a a scoring function for the embedding of a knowledge base into a continuous vector space and then train a model that perform question answer about the entity in the knowledge base . The propose attention model can handle both the propagation of uncertainty when follow a series of relation and also the conjunction of condition in a natural way . On a dataset of soccer player who participate in the FIFA World Cup 2014 , we demonstrate that our model can handle both path query and conjunctive query well . ","http://arxiv.org/pdf/1611.02266v2","stat.ML	cs.AI	cs.CL	cs.LG","Gaussian Attention Model and Its Application to Knowledge Base Embedding   and Question Answering","jwinn@microsoft.com	ryoto@microsoft.com	liwenz@cs.uchicago.edu"
"Yacine Jernite	Edouard Grave	Armand Joulin	Tomas Mikolov","18","11","2016","Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information flow, most recurrent models still consume input features at a constant rate and perform a constant number of computations per time step, which can be detrimental to both speed and model capacity. In this paper, we explore a modification to existing recurrent units which allows them to learn to vary the amount of computation they perform at each step, without prior knowledge of the sequence's time structure. We show experimentally that not only do our models require fewer operations, they also lead to better performance overall on evaluation tasks.","Recurrent neural network ( RNNs ) have be use extensively and with increase success to model various type of sequential data . Much of this progress have be achieve through devise recurrent unit and architecture with the flexibility to capture complex statistic in the data , such a long range dependency or localize attention phenomenon . However , while many sequential data ( such a video , speech or language ) can have highly variable information flow , most recurrent model still consume input feature at a constant rate and perform a constant number of computation per time step , which can be detrimental to both speed and model capacity . In this paper , we explore a modification to exist recurrent unit which allow them to learn to vary the amount of computation they perform at each step , without prior knowledge of the sequence 's time structure . We show experimentally that not only do our model require few operation , they also lead to good performance overall on evaluation task . ","http://arxiv.org/pdf/1611.06188v2","stat.ML	cs.AI	cs.CL	cs.LG","Variable Computation in Recurrent Neural Networks","jernite@cs.nyu.edu	egrave@fb.com	ajoulin@fb.com	tmikolov@fb.com"
"Mostafa Dehghani	Aliaksei Severyn	Sascha Rothe	Jaap Kamps","30","11","2017","In this paper, we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. In our proposed model, we train two neural networks: a target network, the learner and a confidence network, the meta-learner. The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model.","In this paper , we propose a method for train neural network when we have a large set of data with weak label and a small amount of data with true label . In our propose model , we train two neural network : a target network , the learner and a confidence network , the meta-learner . The target network be optimize to perform a give task and be train use a large set of unlabeled data that be weakly annotate . We propose to control the magnitude of the gradient update to the target network use the score provide by the second confidence network , which be train on a small amount of supervise data . Thus we avoid that the weight update compute from noisy label harm the quality of the target network model . ","http://arxiv.org/pdf/1711.11383v1","stat.ML	cs.AI	cs.CL	cs.LG","Learning to Learn from Weak Supervision by Full Supervision","dehghani@uva.nl	severyn@google.com	rothe@google.com	kamps@uva.nl"
"Garrett B. Goh	Nathan O. Hodas	Charles Siegel	Abhinav Vishnu","6","12","2017","Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for additional explicit feature engineering. Using Bayesian optimization methods to tune the network architecture, we show that an optimized SMILES2vec model can serve as a general-purpose neural network for predicting distinct chemical properties including toxicity, activity, solubility and solvation energy, while also outperforming contemporary MLP neural networks that uses engineered features. Furthermore, we demonstrate proof-of-concept of interpretability by developing an explanation mask that localizes on the most important characters used in making a prediction. When tested on the solubility dataset, it identified specific parts of a chemical that is consistent with established first-principles knowledge with an accuracy of 88%. Our work demonstrates that neural networks can learn technically accurate chemical concept and provide state-of-the-art accuracy, making interpretable deep neural networks a useful tool of relevance to the chemical industry.","Chemical databases store information in text representation , and the SMILES format be a universal standard use in many cheminformatics software . Encoded in each SMILES string be structural information that can be use to predict complex chemical property . In this work , we develop SMILES2vec , a deep RNN that automatically learn feature from SMILES to predict chemical property , without the need for additional explicit feature engineering . Using Bayesian optimization method to tune the network architecture , we show that an optimized SMILES2vec model can serve a a general-purpose neural network for predict distinct chemical property include toxicity , activity , solubility and solvation energy , while also outperform contemporary MLP neural network that use engineered feature . Furthermore , we demonstrate proof-of-concept of interpretability by develop an explanation mask that localize on the most important character use in make a prediction . When test on the solubility dataset , it identify specific part of a chemical that be consistent with establish first-principles knowledge with an accuracy of 88 % . Our work demonstrate that neural network can learn technically accurate chemical concept and provide state-of-the-art accuracy , make interpretable deep neural network a useful tool of relevance to the chemical industry . ","http://arxiv.org/pdf/1712.02034v2","stat.ML	cs.AI	cs.CL	cs.LG","SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for   Predicting Chemical Properties","garrett.goh@pnnl.gov	nathan.hodas@pnnl.gov	charles.siegel@pnnl.gov	abhinav.vishnu@pnnl.gov"
"GellÃ©rt Weisz	PaweÅ Budzianowski	Pei-Hao Su	Milica GaÅ¡iÄ","11","2","2018","In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimisation task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement learning approaches to solve this problem. Particular attention is given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing the bias and variance of estimators. When combined, these methods result in the previously proposed ACER algorithm that gave competitive results in gaming environments. These environments however are fully observable and have a relatively small action set so in this paper we examine the application of ACER to dialogue policy optimisation. We show that this method beats the current state-of-the-art in deep learning approaches for spoken dialogue systems. This not only leads to a more sample efficient algorithm that can train faster, but also allows us to apply the algorithm in more difficult environments than before. We thus experiment with learning in a very large action space, which has two orders of magnitude more actions than previously considered. We find that ACER trains significantly faster than the current state-of-the-art.","In spoken dialogue system , we aim to deploy artificial intelligence to build automated dialogue agent that can converse with human . A part of this effort be the policy optimisation task , which attempt to find a policy describe how to respond to human , in the form of a function take the current state of the dialogue and return the response of the system . In this paper , we investigate deep reinforcement learn approach to solve this problem . Particular attention be give to actor-critic method , off-policy reinforcement learn with experience replay , and various method aim at reduce the bias and variance of estimator . When combine , these method result in the previously propose ACER algorithm that give competitive result in game environment . These environment however be fully observable and have a relatively small action set so in this paper we examine the application of ACER to dialogue policy optimisation . We show that this method beat the current state-of-the-art in deep learning approach for spoken dialogue system . This not only lead to a more sample efficient algorithm that can train faster , but also allow u to apply the algorithm in more difficult environment than before . We thus experiment with learning in a very large action space , which have two order of magnitude more action than previously consider . We find that ACER train significantly faster than the current state-of-the-art . ","http://arxiv.org/pdf/1802.03753v1","cs.CL	cs.AI	cs.LG	stat.ML","Sample Efficient Deep Reinforcement Learning for Dialogue Systems with   Large Action Spaces",""
"M. Andrecut","23","2","2018","In this paper we explore the "vector semantics" problem from the perspective of "almost orthogonal" property of high-dimensional random vectors. We show that this intriguing property can be used to "memorize" random vectors by simply adding them, and we provide an efficient probabilistic solution to the set membership problem. Also, we discuss several applications to word context vector embeddings, document sentences similarity, and spam filtering.","In this paper we explore the `` vector semantics '' problem from the perspective of `` almost orthogonal '' property of high-dimensional random vector . We show that this intriguing property can be use to `` memorize '' random vector by simply add them , and we provide an efficient probabilistic solution to the set membership problem . Also , we discuss several application to word context vector embeddings , document sentence similarity , and spam filtering . ","http://arxiv.org/pdf/1802.09914v1","cs.CL	cs.AI	cs.LG	stat.ML","High-Dimensional Vector Semantics","mircea.andrecut@gmail.com"
"Ashutosh Modi	Ivan Titov","18","12","2013","Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of the compositional process for computing the event representations and the ranking component of the model are jointly estimated from texts. We show that this approach results in a substantial boost in ordering performance with respect to previous methods.","Induction of common sense knowledge about prototypical sequence of event have recently receive much attention . Instead of induce this knowledge in the form of graph , a in much of the previous work , in our method , distribute representation of event realization be compute base on distributed representation of predicate and their argument , and then these representation be use to predict prototypical event ordering . The parameter of the compositional process for compute the event representation and the ranking component of the model be jointly estimate from text . We show that this approach result in a substantial boost in order performance with respect to previous method . ","http://arxiv.org/pdf/1312.5198v4","cs.LG	cs.AI	cs.CL	stat.ML	I.2.6; I.2.7","Learning Semantic Script Knowledge with Event Embeddings",""
"Andrew S. Lan	Divyanshu Vats	Andrew E. Waters	Richard G. Baraniuk","18","1","2015","While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mathematics) courses. Our data-driven framework for mathematical language processing (MLP) leverages solution data from a large number of learners to evaluate the correctness of their solutions, assign partial-credit scores, and provide feedback to each learner on the likely locations of any errors. MLP takes inspiration from the success of natural language processing for text data and comprises three main steps. First, we convert each solution to an open response mathematical question into a series of numerical features. Second, we cluster the features from several solutions to uncover the structures of correct, partially correct, and incorrect solutions. We develop two different clustering approaches, one that leverages generic clustering algorithms and one based on Bayesian nonparametrics. Third, we automatically grade the remaining (potentially large number of) solutions based on their assigned cluster and one instructor-provided grade per cluster. As a bonus, we can track the cluster assignment of each step of a multistep solution and determine when it departs from a cluster of correct solutions, which enables us to indicate the likely locations of errors to learners. We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort required in large-scale educational platforms.","While computer and communication technology have provide effective mean to scale up many aspect of education , the submission and grading of assessment such a homework assignment and test remain a weak link . In this paper , we study the problem of automatically grade the kind of open response mathematical question that figure prominently in STEM ( science , technology , engineering , and mathematics ) course . Our data-driven framework for mathematical language processing ( MLP ) leverage solution data from a large number of learner to evaluate the correctness of their solution , assign partial-credit score , and provide feedback to each learner on the likely location of any error . MLP take inspiration from the success of natural language processing for text data and comprises three main step . First , we convert each solution to an open response mathematical question into a series of numerical feature . Second , we cluster the feature from several solution to uncover the structure of correct , partially correct , and incorrect solution . We develop two different cluster approach , one that leverage generic cluster algorithm and one base on Bayesian nonparametrics . Third , we automatically grade the remain ( potentially large number of ) solution base on their assign cluster and one instructor-provided grade per cluster . As a bonus , we can track the cluster assignment of each step of a multistep solution and determine when it depart from a cluster of correct solution , which enable u to indicate the likely location of error to learner . We test and validate MLP on real-world MOOC data to demonstrate how it can substantially reduce the human effort require in large-scale educational platform . ","http://arxiv.org/pdf/1501.04346v1","stat.ML	cs.AI	cs.CL	cs.LG","Mathematical Language Processing: Automatic Grading and Feedback for   Open Response Mathematical Questions","mr.lan@sparfa.com	dvats@sparfa.com	waters@sparfa.com	richb@sparfa.com"
"Tadahiro Taniguchi	Ryo Nakashima	Shogo Nagasaka","22","6","2015","Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the "hierarchical Dirichlet process hidden language model" (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. The novel unsupervised double articulation analyzer is called NPB-DAA.   The NPB-DAA can automatically estimate double articulation structure embedded in speech signals. We also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing Japanese vowel sequences. In the word acquisition and phoneme categorization tasks, the NPB-DAA outperformed a conventional double articulation analyzer (DAA) and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner.","Human infant can discover word directly from unsegmented speech signal without any explicitly label data . In this paper , we develop a novel machine learn method call nonparametric Bayesian double articulation analyzer ( NPB-DAA ) that can directly acquire language and acoustic model from observe continuous speech signal . For this purpose , we propose an integrative generative model that combine a language model and an acoustic model into a single generative model call the `` hierarchical Dirichlet process hidden language model '' ( HDP-HLM ) . The HDP-HLM be obtain by extend the hierarchical Dirichlet process hide semi-Markov model ( HDP-HSMM ) propose by Johnson et al . An inference procedure for the HDP-HLM be derive use the blocked Gibbs sampler originally propose for the HDP-HSMM . This procedure enable the simultaneous and direct inference of language and acoustic model from continuous speech signal . Based on the HDP-HLM and it inference procedure , we develop a novel double articulation analyzer . By assume HDP-HLM a a generative model of observed time series data , and by infer latent variable of the model , the method can analyze latent double articulation structure , i.e. , hierarchically organize latent word and phoneme , of the data in an unsupervised manner . The novel unsupervised double articulation analyzer be call NPB-DAA . The NPB-DAA can automatically estimate double articulation structure embed in speech signal . We also carry out two evaluation experiment use synthetic data and actual human continuous speech signal represent Japanese vowel sequence . In the word acquisition and phoneme categorization task , the NPB-DAA outperform a conventional double articulation analyzer ( DAA ) and baseline automatic speech recognition system whose acoustic model be train in a supervised manner . ","http://arxiv.org/pdf/1506.06646v2","cs.AI	cs.CL	cs.LG	stat.ML","Nonparametric Bayesian Double Articulation Analyzer for Direct Language   Acquisition from Continuous Speech Signals",""
"Zhiting Hu	Xuezhe Ma	Zhengzhong Liu	Eduard Hovy	Eric Xing","21","3","2016","Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.","Combining deep neural network with structured logic rule be desirable to harness flexibility and reduce uninterpretability of the neural model . We propose a general framework capable of enhance various type of neural network ( e.g. , CNNs and RNNs ) with declarative first-order logic rule . Specifically , we develop an iterative distillation method that transfer the structured information of logic rule into the weight of neural network . We deploy the framework on a CNN for sentiment analysis , and an RNN for name entity recognition . With a few highly intuitive rule , we obtain substantial improvement and achieve state-of-the-art or comparable result to previous best-performing system . ","http://arxiv.org/pdf/1603.06318v4","cs.LG	cs.AI	cs.CL	stat.ML","Harnessing Deep Neural Networks with Logic Rules","zhitingh@cs.cmu.edu	xuezhem@cs.cmu.edu	liu@cs.cmu.edu	hovy@cmu.edu	epxing@cs.cmu.edu"
"Zhiting Hu	Zichao Yang	Xiaodan Liang	Ruslan Salakhutdinov	Eric P. Xing","2","3","2017","Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.","Generic generation and manipulation of text be challenge and have limit success compare to recent deep generative modeling in visual domain . This paper aim at generate plausible natural language sentence , whose attribute be dynamically control by learn disentangled latent representation with designated semantics . We propose a new neural generative model which combine variational auto-encoders and holistic attribute discriminator for effective imposition of semantic structure . With differentiable approximation to discrete text sample , explicit constraint on independent attribute control , and efficient collaborative learning of generator and discriminator , our model learn highly interpretable representation from even only word annotation , and produce realistic sentence with desired attribute . Quantitative evaluation validate the accuracy of sentence and attribute generation . ","http://arxiv.org/pdf/1703.00955v3","cs.LG	cs.AI	cs.CL	stat.ML","Toward Controlled Generation of Text",""
"Lianhui Qin	Zhisong Zhang	Hai Zhao	Zhiting Hu	Eric P. Xing","1","4","2017","Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient features for accurate classification. We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator. Our method effectively transfers discriminability of connectives to the implicit features, and achieves state-of-the-art performance on the PDTB benchmark.","Implicit discourse relation classification be of great challenge due to the lack of connective a strong linguistic cue , which motivate the use of annotated implicit connective to improve the recognition . We propose a feature imitation framework in which an implicit relation network be drive to learn from another neural network with access to connective , and thus encourage to extract similarly salient feature for accurate classification . We develop an adversarial model to enable an adaptive imitation scheme through competition between the implicit network and a rival feature discriminator . Our method effectively transfer discriminability of connective to the implicit feature , and achieve state-of-the-art performance on the PDTB benchmark . ","http://arxiv.org/pdf/1704.00217v1","cs.CL	cs.AI	cs.LG	stat.ML","Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification","qinlianhui	zzs2011}@sjtu.edu.cn	zhaohai@cs.sjtu.edu.cn,	zhitingh@cs.cmu.edu	epxing@cs.cmu.edu"
"Maxim Rabinovich	Mitchell Stern	Dan Klein","25","4","2017","Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.","Tasks like code generation and semantic parsing require mapping unstructured ( or partially structure ) input to well-formed , executable output . We introduce abstract syntax network , a modeling framework for these problem . The output be represent a abstract syntax tree ( ASTs ) and construct by a decoder with a dynamically-determined modular structure parallel the structure of the output tree . On the benchmark Hearthstone dataset for code generation , our model obtain 79.2 BLEU and 22.7 % exact match accuracy , compare to previous state-of-the-art value of 67.1 and 6.1 % . Furthermore , we perform competitively on the Atis , Jobs , and Geo semantic parse datasets with no task-specific engineering . ","http://arxiv.org/pdf/1704.07535v1","cs.CL	cs.AI	cs.LG	stat.ML","Abstract Syntax Networks for Code Generation and Semantic Parsing",""
"Ben Athiwaratkun	Andrew Gordon Wilson","27","4","2017","Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.","Word embeddings provide point representation of word contain useful semantic information . We introduce multimodal word distribution form from Gaussian mixture , for multiple word meaning , entailment , and rich uncertainty information . To learn these distribution , we propose an energy-based max-margin objective . We show that the result approach capture uniquely expressive semantic information , and outperform alternative , such a word2vec skip-grams , and Gaussian embeddings , on benchmark datasets such a word similarity and entailment . ","http://arxiv.org/pdf/1704.08424v1","stat.ML	cs.AI	cs.CL	cs.LG","Multimodal Word Distributions","pa338@cornell.edu	andrew@cornell.edu"
"Brent Harrison	Upol Ehsan	Mark O. Riedl","26","7","2017","In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effective at learning in unseen environments. We evaluate this technique using the popular arcade game, Frogger, under ideal and non-ideal conditions. This evaluation shows that our modified policy shaping algorithm improves over a Q-learning agent as well as a baseline version of policy shaping.","In this work we present a technique to use natural language to help reinforcement learn generalize to unseen environment . This technique use neural machine translation , specifically the use of encoder-decoder network , to learn association between natural language behavior description and state-action information . We then use this learned model to guide agent exploration use a modified version of policy shape to make it more effective at learn in unseen environment . We evaluate this technique use the popular arcade game , Frogger , under ideal and non-ideal condition . This evaluation show that our modified policy shape algorithm improves over a Q-learning agent as well a a baseline version of policy shaping . ","http://arxiv.org/pdf/1707.08616v2","cs.AI	cs.CL	cs.LG	stat.ML","Guiding Reinforcement Learning Exploration Using Natural Language","harrison@cs.uky.edu	ehsan@gatech.edu	riedl@cc.gatech.edu"
"Mo Yu	Xiaoxiao Guo	Jinfeng Yi	Shiyu Chang	Saloni Potdar	Gerald Tesauro	Haoyu Wang	Bowen Zhou","26","8","2017","We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. To overcome these limitations, we propose a novel task-clustering algorithm by using the matrix completion technique. The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs. We then use a matrix completion algorithm to complete the similarity matrix. Our theoretical analysis shows that under mild constraints, the proposed algorithm will perfectly recover the underlying "true" similarity matrix with a high probability. Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks. Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when the tasks are diverse.","We investigate task cluster for deep-learning base multi-task and few-shot learning in a many-task setting . We propose a new method to measure task similarity with cross-task transfer performance matrix for the deep learning scenario . Although this matrix provide u critical information regard similarity between task , it asymmetric property and unreliable performance score can affect conventional cluster method adversely . Additionally , the uncertain task-pairs , i.e. , the one with extremely asymmetric transfer score , may collectively mislead cluster algorithm to output an inaccurate task-partition . To overcome these limitation , we propose a novel task-clustering algorithm by use the matrix completion technique . The propose algorithm construct a partially-observed similarity matrix base on the certainty of cluster membership of the task-pairs . We then use a matrix completion algorithm to complete the similarity matrix . Our theoretical analysis show that under mild constraint , the propose algorithm will perfectly recover the underlie `` true '' similarity matrix with a high probability . Our result show that the new task cluster method can discover task cluster for train flexible and superior neural network model in a multi-task learning setup for sentiment classification and dialog intent classification task . Our task cluster approach also extend metric-based few-shot learn method to adapt multiple metric , which demonstrate empirical advantage when the task be diverse . ","http://arxiv.org/pdf/1708.07918v1","cs.LG	cs.AI	cs.CL	stat.ML","Robust Task Clustering for Deep Many-Task Learning",""
"Gino Brunner	Yuyi Wang	Roger Wattenhofer	Michael Weigelt","18","1","2018","We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. The representations change significantly when translation and part-of-speech decoders are added. The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. We explore the structure of the representation space by interpolating between sentences, which yields interesting pseudo-English sentences, many of which have recognizable syntactic structure. Lastly, we point out an interesting property of our models: The difference-vector between two sentences can be added to change a third sentence with similar features in a meaningful way.","We train multi-task autoencoders on linguistic task and analyze the learned hidden sentence representation . The representation change significantly when translation and part-of-speech decoder be add . The more decoder a model employ , the good it cluster sentence accord to their syntactic similarity , a the representation space become less entangled . We explore the structure of the representation space by interpolate between sentence , which yield interest pseudo-English sentence , many of which have recognizable syntactic structure . Lastly , we point out an interesting property of our model : The difference-vector between two sentence can be add to change a third sentence with similar feature in a meaningful way . ","http://arxiv.org/pdf/1801.06024v1","cs.CL	cs.AI	cs.LG	stat.ML","Natural Language Multitasking: Analyzing and Improving Syntactic   Saliency of Hidden Representations","brunnegi@ethz.ch	yuwang@ethz.ch	wattenhofer@ethz.ch	weigeltm@ethz.ch"
"Minghai Chen	Sen Wang	Paul Pu Liang	Tadas BaltruÅ¡aitis	Amir Zadeh	Louis-Philippe Morency","3","2","2018","With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.","With the increase popularity of video share website such a YouTube and Facebook , multimodal sentiment analysis have receive increase attention from the scientific community . Contrary to previous work in multimodal sentiment analysis which focus on holistic information in speech segment such a bag of word representation and average facial expression intensity , we develop a novel deep architecture for multimodal sentiment analysis that perform modality fusion at the word level . In this paper , we propose the Gated Multimodal Embedding LSTM with Temporal Attention ( GME-LSTM ( A ) ) model that be compose of 2 module . The Gated Multimodal Embedding alleviate the difficulty of fusion when there be noisy modality . The LSTM with Temporal Attention perform word level fusion at a finer fusion resolution between input modality and attends to the most important time step . As a result , the GME-LSTM ( A ) be able to good model the multimodal structure of speech through time and perform good sentiment comprehension . We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis ( CMU-MOSI ) dataset by achieve state-of-the-art sentiment classification and regression result . Qualitative analysis on our model emphasize the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modality be noisy . We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filter these noisy modality out . Our result and analysis open new area in the study of sentiment analysis in human communication and provide new model for multimodal fusion . ","http://arxiv.org/pdf/1802.00924v1","cs.LG	cs.AI	cs.CL	stat.ML","Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement   Learning","minghai1@cs.cmu.edu	senw1@cs.cmu.edu	pliang@cs.cmu.edu	tb346@cl.cam.ac.uk	abagherz@cs.cmu.edu	morency@cs.cmu.edu"
"Ed Collins	Isabelle Augenstein	Sebastian Riedel","13","6","2017","Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.","Automatic summarisation be a popular approach to reduce a document to it main argument . Recent research in the area have focus on neural approach to summarisation , which can be very data-hungry . However , few large datasets exist and none for the traditionally popular domain of scientific publication , which open up challenge research avenue center on encode large , complex document . In this paper , we introduce a new dataset for summarisation of computer science publication by exploit a large resource of author provide summary and show straightforward way of extend it further . We develop model on the dataset making use of both neural sentence encoding and traditionally use summarisation feature and show that model which encode sentence as well a their local and global context perform best , significantly outperform well-established baseline method . ","http://arxiv.org/pdf/1706.03946v1","cs.CL	cs.AI	cs.NE	stat.AP	stat.ML","A Supervised Approach to Extractive Summarisation of Scientific Papers","edward.collins.13@ucl.ac.uk	i.augenstein@ucl.ac.uk	s.riedel@ucl.ac.uk"
"Jacob Devlin	Hao Cheng	Hao Fang	Saurabh Gupta	Li Deng	Xiaodong He	Geoffrey Zweig	Margaret Mitchell","7","5","2015","Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.","Two recent approach have achieve state-of-the-art result in image captioning . The first use a pipelined process where a set of candidate word be generate by a convolutional neural network ( CNN ) train on image , and then a maximum entropy ( ME ) language model be use to arrange these word into a coherent sentence . The second use the penultimate activation layer of the CNN a input to a recurrent neural network ( RNN ) that then generate the caption sequence . In this paper , we compare the merit of these different language modeling approach for the first time by use the same state-of-the-art CNN a input . We examine issue in the different approach , include linguistic irregularity , caption repetition , and data set overlap . By combine key aspect of the ME and RNN method , we achieve a new record performance over previously publish result on the benchmark COCO dataset . However , the gain we see in BLEU do not translate to human judgment . ","http://arxiv.org/pdf/1505.01809v3","cs.CL	cs.AI	cs.CV	cs.LG","Language Models for Image Captioning: The Quirks and What Works","jdevlin@microsoft.com	xiaohe@microsoft.com	gzweig@microsoft.com	memitc@microsoft.com"
"Mengye Ren	Ryan Kiros	Richard Zemel","8","5","2015","This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.","This work aim to address the problem of image-based question-answering ( QA ) with new model and datasets . In our work , we propose to use neural network and visual semantic embeddings , without intermediate stage such a object detection and image segmentation , to predict answer to simple question about image . Our model perform 1.8 time well than the only publish result on an exist image QA dataset . We also present a question generation algorithm that convert image description , which be widely available , into QA form . We use this algorithm to produce an order-of-magnitude large dataset , with more evenly distributed answer . A suite of baseline result on this new dataset be also present . ","http://arxiv.org/pdf/1505.02074v4","cs.LG	cs.AI	cs.CL	cs.CV","Exploring Models and Data for Image Question Answering","mren@cs.toronto.edu	rkiros@cs.toronto.edu	zemel@cs.toronto.edu"
"Yash Goyal	Tejas Khot	Douglas Summers-Stay	Dhruv Batra	Devi Parikh","2","12","2016","Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.   We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0).   We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners.   Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.","Problems at the intersection of vision and language be of significant importance both a challenge research question and for the rich set of application they enable . However , inherent structure in our world and bias in our language tend to be a simple signal for learn than visual modality , result in model that ignore visual information , lead to an inflated sense of their capability . We propose to counter these language prior for the task of Visual Question Answering ( VQA ) and make vision ( the V in VQA ) matter ! Specifically , we balance the popular VQA dataset by collect complementary image such that every question in our balanced dataset be associate with not just a single image , but rather a pair of similar image that result in two different answer to the question . Our dataset be by construction more balanced than the original VQA dataset and have approximately twice the number of image-question pair . Our complete balance dataset be publicly available at www.visualqa.org a part of the 2nd iteration of the Visual Question Answering Dataset and Challenge ( VQA v2.0 ) . We further benchmark a number of state-of-art VQA model on our balanced dataset . All model perform significantly bad on our balanced dataset , suggest that these model have indeed learn to exploit language prior . This finding provide the first concrete empirical evidence for what seem to be a qualitative sense among practitioner . Finally , our data collection protocol for identify complementary image enable u to develop a novel interpretable model , which in addition to provide an answer to the give ( image , question ) pair , also provide a counter-example base explanation . Specifically , it identify an image that be similar to the original image , but it believe have a different answer to the same question . This can help in building trust for machine among their user . ","http://arxiv.org/pdf/1612.00837v3","cs.CV	cs.AI	cs.CL	cs.LG","Making the V in VQA Matter: Elevating the Role of Image Understanding in   Visual Question Answering","ygoyal@vt.edu	tjskhot@vt.edu	douglas.a.summers-stay.civ@mail.mil	dbatra@gatech.edu	parikh@gatech.edu"
"Mateusz Malinowski	Mario Fritz","1","10","2014","We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.","We propose a method for automatically answering question about image by bring together recent advance from natural language processing and computer vision . We combine discrete reason with uncertain prediction by a multi-world approach that represent uncertainty about the perceived world in a bayesian framework . Our approach can handle human question of high complexity about realistic scene and reply with range of answer like count , object class , instance and list of them . The system be directly train from question-answer pair . We establish a first benchmark for this task that can be see a a modern attempt at a visual turing test . ","http://arxiv.org/pdf/1410.0210v4","cs.AI	cs.CL	cs.CV	cs.LG","A Multi-World Approach to Question Answering about Real-World Scenes   based on Uncertain Input","mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Mateusz Malinowski	Mario Fritz","14","1","2015","Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for "question answering about images" as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.","Progress in language and image understanding by machine have sparkle the interest of the research community in more open-ended , holistic task , and refuel an old AI dream of building intelligent machine . We discuss a few prominent challenge that characterize such holistic task and argue for `` question answer about image '' a a particular appeal instance of such a holistic task . In particular , we point out that it be a version of a Turing Test that be likely to be more robust to over-interpretations and contrast it with task like ground and generation of description . Finally , we discuss tool to measure progress in this field . ","http://arxiv.org/pdf/1501.03302v2","cs.AI	cs.CL	cs.CV	cs.LG","Hard to Cheat: A Turing Test based on Answering Questions about Images","mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Aishwarya Agrawal	Dhruv Batra	Devi Parikh","23","6","2016","Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA models -- with-attention and without-attention and show the similarities and differences in the behavior of these models. We also analyze the winning entry of the VQA Challenge 2016.   Our behavior analysis reveals that despite recent progress, today's VQA models are "myopic" (tend to fail on sufficiently novel instances), often "jump to conclusions" (converge on a predicted answer after 'listening' to just half the question), and are "stubborn" (do not change their answers across images).","Recently , a number of deep-learning base model have be propose for the task of Visual Question Answering ( VQA ) . The performance of most model be cluster around 60-70 % . In this paper we propose systematic method to analyze the behavior of these model a a first step towards recognize their strength and weakness , and identify the most fruitful direction for progress . We analyze two model , one each from two major class of VQA model -- with-attention and without-attention and show the similarity and difference in the behavior of these model . We also analyze the win entry of the VQA Challenge 2016 . Our behavior analysis reveals that despite recent progress , today 's VQA model be `` myopic '' ( tend to fail on sufficiently novel instance ) , often `` jump to conclusion '' ( converge on a predict answer after 'listening ' to just half the question ) , and be `` stubborn '' ( do not change their answer across image ) . ","http://arxiv.org/pdf/1606.07356v2","cs.CL	cs.AI	cs.CV	cs.LG","Analyzing the Behavior of Visual Question Answering Models","aish@vt.edu	dbatra@vt.edu	parikh@vt.edu"
"Harsh Agrawal	Arjun Chandrasekaran	Dhruv Batra	Devi Parikh	Mohit Bansal","23","6","2016","Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.","Temporal common sense have application in AI task such a QA , multi-document summarization , and human-AI communication . We propose the task of sequence -- give a jumbled set of aligned image-caption pair that belong to a story , the task be to sort them such that the output sequence form a coherent story . We present multiple approach , via unary ( position ) and pairwise ( order ) prediction , and their ensemble-based combination , achieve strong result on this task . We use both text-based and image-based feature , which depict complementary improvement . Using qualitative example , we demonstrate that our model have learn interesting aspect of temporal common sense . ","http://arxiv.org/pdf/1606.07493v5","cs.CL	cs.AI	cs.CV	cs.LG","Sort Story: Sorting Jumbled Images and Captions into Stories","harsh92	carjun	dbatra	parikh}@vt.edu	mbansal@cs.unc.edu"
"Ashkan Mokarian	Mateusz Malinowski	Mario Fritz","9","8","2016","We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to train the network by directly maximizing the similarity between the internal representation of the deep learning architecture and candidate answers. Again, such approach achieves a significant improvement over the prior work that also uses CNN+LSTM approach on Visual Madlibs.","We present Mean Box Pooling , a novel visual representation that pool over CNN representation of a large number , highly overlap object proposal . We show that such representation together with nCCA , a successful multimodal embed technique , achieve state-of-the-art performance on the Visual Madlibs task . Moreover , inspire by the nCCA 's objective function , we extend classical CNN+LSTM approach to train the network by directly maximize the similarity between the internal representation of the deep learning architecture and candidate answer . Again , such approach achieve a significant improvement over the prior work that also use CNN+LSTM approach on Visual Madlibs . ","http://arxiv.org/pdf/1608.02717v1","cs.CV	cs.AI	cs.CL	cs.LG","Mean Box Pooling: A Rich Image Representation and Output Embedding for   the Visual Madlibs Task","ashkan@mpi-inf.mpg.de	mmalinow@mpi-inf.mpg.de	mfritz@mpi-inf.mpg.de"
"Yuval Atzmon	Jonathan Berant	Vahid Kezami	Amir Globerson	Gal Chechik","27","8","2016","Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.","Recurrent neural network have recently be use for learn to describe image use natural language . However , it have be observe that these model generalize poorly to scene that be not observe during training , possibly depend too strongly on the statistic of the text in the training data . Here we propose to describe image use short structured representation , aim to capture the crux of a description . These structured representation allow u to tease-out and evaluate separately two type of generalization : standard generalization to new image with similar scene , and generalization to new combination of known entity . We compare two learn approach on the MS-COCO dataset : a state-of-the-art recurrent network base on an LSTM ( Show , Attend and Tell ) , and a simple structured prediction model on top of a deep network . We find that the structured model generalizes to new composition substantially good than the LSTM , ~7 time the accuracy of predict structured representation . By provide a concrete method to quantify generalization for unseen combination , we argue that structured representation and compositional split be a useful benchmark for image captioning , and advocate compositional model that capture linguistic and visual structure . ","http://arxiv.org/pdf/1608.07639v1","cs.CV	cs.AI	cs.CL	cs.LG","Learning to generalize to new compositions in image understanding","yuval.atzmon@biu.ac.il"
"C. Lawrence Zitnick	Aishwarya Agrawal	Stanislaw Antol	Margaret Mitchell	Dhruv Batra	Devi Parikh","31","8","2016","As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.","As machine have become more intelligent , there have be a renew interest in method for measure their intelligence . A common approach be to propose task for which a human excels , but one which machine find difficult . However , an ideal task should also be easy to evaluate and not be easily gameable . We begin with a case study explore the recently popular task of image captioning and it limitation a a task for measure machine intelligence . An alternative and more promising task be Visual Question Answering that test a machine 's ability to reason about language and vision . We describe a dataset unprecedented in size create for the task that contain over 760,000 human generate question about image . Using around 10 million human generate answer , machine may be easily evaluate . ","http://arxiv.org/pdf/1608.08716v1","cs.AI	cs.CL	cs.CV	cs.LG","Measuring Machine Intelligence Through Visual Question Answering","zitnick@fb.com	aish@vt.edu	santol@vt.edu	memitc@microsoft.com	dbatra@vt.edu	parikh@vt.edu"
"Yash Goyal	Akrit Mohapatra	Devi Parikh	Dhruv Batra","31","8","2016","Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.","Deep neural network have show striking progress and obtain state-of-the-art result in many AI research field in the recent year . However , it be often unsatisfying to not know why they predict what they do . In this paper , we address the problem of interpret Visual Question Answering ( VQA ) model . Specifically , we be interested in find what part of the input ( pixel in image or word in question ) the VQA model focus on while answer the question . To tackle this problem , we use two visualization technique -- guide backpropagation and occlusion -- to find important word in the question and important region in the image . We then present qualitative and quantitative analysis of these importance map . We find that even without explicit attention mechanism , VQA model may sometimes be implicitly attend to relevant region in the image , and often to appropriate word in the question . ","http://arxiv.org/pdf/1608.08974v2","cs.CV	cs.AI	cs.CL	cs.LG","Towards Transparent AI Systems: Interpreting Visual Question Answering   Models","ygoyal@vt.edu	akrit@vt.edu	parikh@vt.edu	dbatra@vt.edu"
"Abhishek Das	Satwik Kottur	Khushi Gupta	Avi Singh	Deshraj Yadav	JosÃ© M. F. Moura	Devi Parikh	Dhruv Batra","26","11","2016","We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.   We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org","We introduce the task of Visual Dialog , which require an AI agent to hold a meaningful dialog with human in natural , conversational language about visual content . Specifically , give an image , a dialog history , and a question about the image , the agent have to grind the question in image , infer context from history , and answer the question accurately . Visual Dialog be disentangle enough from a specific downstream task so a to serve a a general test of machine intelligence , while be ground in vision enough to allow objective evaluation of individual response and benchmark progress . We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset ( VisDial ) . VisDial v0.9 have be release and contain 1 dialog with 10 question-answer pair on ~120k image from COCO , with a total of ~1.2M dialog question-answer pair . We introduce a family of neural encoder-decoder model for Visual Dialog with 3 encoders -- Late Fusion , Hierarchical Recurrent Encoder and Memory Network -- and 2 decoder ( generative and discriminative ) , which outperform a number of sophisticated baseline . We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent be ask to sort a set of candidate answer and evaluate on metric such a mean-reciprocal-rank of human response . We quantify gap between machine and human performance on the Visual Dialog task via human study . Putting it all together , we demonstrate the first 'visual chatbot ' ! Our dataset , code , trained model and visual chatbot be available on http : //visualdialog.org ","http://arxiv.org/pdf/1611.08669v5","cs.CV	cs.AI	cs.CL	cs.LG","Visual Dialog","abhshkdz@gatech.edu	parikh@gatech.edu	dbatra@gatech.edu	skottur@andrew.cmu.edu	khushig@andrew.cmu.edu	moura@andrew.cmu.edu	avisingh@cs.berkeley.edu	deshraj@vt.edu"
"Abhinav Thanda	Shankar M Venkatesan","10","1","2017","Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7\% relative improvement in WER is reported at -3 SNR dB","Multi-task learning ( MTL ) involve the simultaneous training of two or more related task over share representation . In this work , we apply MTL to audio-visual automatic speech recognition ( AV-ASR ) . Our primary task be to learn a mapping between audio-visual fused feature and frame label obtain from acoustic GMM/HMM model . This be combine with an auxiliary task which map visual feature to frame label obtain from a separate visual GMM/HMM model . The MTL model be test at various level of babble noise and the result be compare with a base-line hybrid DNN-HMM AV-ASR model . Our result indicate that MTL be especially useful at high level of noise . Compared to base-line , upto 7\ % relative improvement in WER be report at -3 SNR dB ","http://arxiv.org/pdf/1701.02477v1","cs.CL	cs.AI	cs.CV	cs.LG","Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic   Speech Recognition",""
"Abhishek Das	Satwik Kottur	JosÃ© M. F. Moura	Stefan Lee	Dhruv Batra","20","3","2017","We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dialog to game reward.   We demonstrate two experimental results.   First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision.   Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.","We introduce the first goal-driven training for visual question answering and dialog agent . Specifically , we pose a cooperative 'image guess ' game between two agent -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of image . We use deep reinforcement learning ( RL ) to learn the policy of these agent end-to-end -- from pixel to multi-agent multi-round dialog to game reward . We demonstrate two experimental result . First , a a 'sanity check ' demonstration of pure RL ( from scratch ) , we show result on a synthetic world , where the agent communicate in ungrounded vocabulary , i.e. , symbol with no pre-specified meaning ( X , Y , Z ) . We find that two bot invent their own communication protocol and start use certain symbol to ask/answer about certain visual attribute ( shape/color/style ) . Thus , we demonstrate the emergence of grounded language and communication among 'visual ' dialog agent with no human supervision . Second , we conduct large-scale real-image experiment on the VisDial dataset , where we pretrain with supervised dialog data and show that the RL 'fine-tuned ' agent significantly outperform SL agent . Interestingly , the RL Qbot learn to ask question that Abot be good at , ultimately result in more informative dialog and a good team . ","http://arxiv.org/pdf/1703.06585v2","cs.CV	cs.AI	cs.CL	cs.LG","Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning",""
"Wei-Lun Chao	Hexiang Hu	Fei Sha","24","4","2017","Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular visual QA datasets as well as to create a new visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via http://www.teds.usc.edu/website_vqa/.","Visual question answering ( QA ) have attract a lot of attention lately , see essentially a a form of ( visual ) Turing test that artificial intelligence should strive to achieve . In this paper , we study a crucial component of this task : how can we design good datasets for the task ? We focus on the design of multiple-choice base datasets where the learner have to select the right answer from a set of candidate one include the target ( i.e . the correct one ) and the decoy ( i.e . the incorrect one ) . Through careful analysis of the result attain by state-of-the-art learning model and human annotator on exist datasets , we show the design of the decoy answer have a significant impact on how and what the learning model learn from the datasets . In particular , the result learner can ignore the visual information , the question , or the both while still do well on the task . Inspired by this , we propose automatic procedure to remedy such design deficiency . We apply the procedure to re-construct decoy answer for two popular visual QA datasets as well a to create a new visual QA dataset from the Visual Genome project , result in the large dataset for this task . Extensive empirical study show that the design deficiency have be alleviate in the remedied datasets and the performance on them be likely a more faithful indicator of the difference among learning model . The datasets be release and publicly available via http : //www.teds.usc.edu/website_vqa/ . ","http://arxiv.org/pdf/1704.07121v1","cs.CL	cs.AI	cs.CV	cs.LG","Being Negative but Constructively: Lessons Learnt from Creating Better   Visual Question Answering Datasets","weilunc@usc.edu,	hexiang.frank.hu@gmail.com,	feisha@usc.edu"
"Aishwarya Agrawal	Aniruddha Kembhavi	Dhruv Batra	Devi Parikh","26","4","2017","Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.","Visual Question Answering ( VQA ) have receive a lot of attention over the past couple of year . A number of deep learning model have be propose for this task . However , it have be show that these model be heavily drive by superficial correlation in the training data and lack compositionality -- the ability to answer question about unseen composition of see concept . This compositionality be desirable and central to intelligence . In this paper , we propose a new setting for Visual Question Answering where the test question-answer pair be compositionally novel compare to train question-answer pair . To facilitate develop model under this setting , we present a new compositional split of the VQA v1.0 dataset , which we call Compositional VQA ( C-VQA ) . We analyze the distribution of question and answer in the C-VQA split . Finally , we evaluate several exist VQA model under this new setting and show that the performance of these model degrade by a significant amount compare to the original VQA setting . ","http://arxiv.org/pdf/1704.08243v1","cs.CV	cs.AI	cs.CL	cs.LG","C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0   Dataset","aish@vt.edu,	anik@allenai.org,	dbatra@gatech.edu	parikh@gatech.edu"
"Alexander Kuhnle	Ann Copestake","5","6","2017","We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detail.","We discuss problem with the standard approach to evaluation for task like visual question answering , and argue that artificial data can be use to address these a a complement to current practice . We demonstrate that with the help of exist 'deep ' linguistic processing technology we be able to create challenging abstract datasets , which enable u to investigate the language understanding ability of multimodal deep learning model in detail . ","http://arxiv.org/pdf/1706.01322v1","cs.CL	cs.AI	cs.CV	cs.LG","Deep learning evaluation using deep linguistic processing","aok25@cam.ac.uk	aac10@cam.ac.uk"
"Xu Sun	Xuancheng Ren	Shuming Ma	Houfeng Wang","19","6","2017","We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1--4\% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The code is available at https://github.com/jklj077/meProp","We propose a simple yet effective technique for neural network learning . The forward propagation be compute a usual . In back propagation , only a small subset of the full gradient be compute to update the model parameter . The gradient vector be sparsified in such a way that only the top- $ k $ element ( in term of magnitude ) be keep . As a result , only $ k $ row or column ( depend on the layout ) of the weight matrix be modify , lead to a linear reduction ( $ k $ divide by the vector dimension ) in the computational cost . Surprisingly , experimental result demonstrate that we can update only 1 -- 4\ % of the weight at each back propagation pas . This do not result in a large number of train iteration . More interestingly , the accuracy of the resulting model be actually improve rather than degrade , and a detailed analysis be give . The code be available at http : //github.com/jklj077/meProp ","http://arxiv.org/pdf/1706.06197v4","cs.LG	cs.AI	cs.CL	cs.CV","meProp: Sparsified Back Propagation for Accelerated Deep Learning with   Reduced Overfitting",""
"Suranjana Samanta	Sameep Mehta","10","7","2017","Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. An attacker introduces specially crafted adversarial samples to a deployed classifier, which are being mis-classified by the classifier. However, the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. Most of the prior works have been focused on synthesizing adversarial samples in the image domain. In this paper, we propose a new method of crafting adversarial text samples by modification of the original samples. Modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. Our algorithm works best for the datasets which have sub-categories within each of the classes of examples. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficiency of our proposed method.","Adversarial sample be strategically modify sample , which be craft with the purpose of fool a classifier at hand . An attacker introduces specially craft adversarial sample to a deploy classifier , which be be mis-classified by the classifier . However , the sample be perceive to be draw from entirely different class and thus it become hard to detect the adversarial sample . Most of the prior work have be focus on synthesize adversarial sample in the image domain . In this paper , we propose a new method of craft adversarial text sample by modification of the original sample . Modifications of the original text sample be do by delete or replace the important or salient word in the text or by introduce new word in the text sample . Our algorithm work best for the datasets which have sub-categories within each of the class of example . While craft adversarial sample , one of the key constraint be to generate meaningful sentence which can at pass off a legitimate from language ( English ) viewpoint . Experimental result on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficiency of our propose method . ","http://arxiv.org/pdf/1707.02812v1","cs.LG	cs.AI	cs.CL	cs.CV","Towards Crafting Text Adversarial Samples","suransam@in.ibm.com	sameepmehta@in.ibm.com"
"Ramakanth Pasunuru	Mohit Bansal","7","8","2017","Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.","Sequence-to-sequence model have show promising improvement on the temporal task of video captioning , but they optimize word-level cross-entropy loss during training . First , use policy gradient and mixed-loss method for reinforcement learning , we directly optimize sentence-level task-based metric ( a reward ) , achieve significant improvement over the baseline , base on both automatic metric and human evaluation on multiple datasets . Next , we propose a novel entailment-enhanced reward ( CIDEnt ) that correct phrase-matching base metric ( such a CIDEr ) to only allow for logically-implied partial match and avoid contradiction , achieve far significant improvement over the CIDEr-reward model . Overall , our CIDEnt-reward model achieve the new state-of-the-art on the MSR-VTT dataset . ","http://arxiv.org/pdf/1708.02300v1","cs.CL	cs.AI	cs.CV	cs.LG","Reinforced Video Captioning with Entailment Rewards","ram@cs.unc.edu	mbansal@cs.unc.edu"
"Licheng Yu	Mohit Bansal	Tamara L. Berg","9","8","2017","We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story. Automatic and human evaluations show our model achieves better performance on selection, generation, and retrieval than baselines.","We address the problem of end-to-end visual storytelling . Given a photo album , our model first select the most representative ( summary ) photo , and then compose a natural language story for the album . For this task , we make use of the Visual Storytelling dataset and a model compose of three hierarchically-attentive Recurrent Neural Nets ( RNNs ) to : encode the album photo , select representative ( summary ) photo , and compose the story . Automatic and human evaluation show our model achieve good performance on selection , generation , and retrieval than baseline . ","http://arxiv.org/pdf/1708.02977v1","cs.CL	cs.AI	cs.CV	cs.LG","Hierarchically-Attentive RNN for Album Summarization and Storytelling","licheng@cs.unc.edu	mbansal@cs.unc.edu	tlberg@cs.unc.edu"
